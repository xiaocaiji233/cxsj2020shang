def match(issues, time):
    # print("?")
    matched = {}
    time.sort()
    for i in time:
        in_time = []
        for j in issues:
            if i > j['created_at']:
                in_time.append(j)
                issues.pop(issues.index(j))
                # pprint.pprint(i)
        matched[i] = in_time
    return matched


if __name__ == "__main__":  # 测试代码
    import pprint
    time = ['2020-03-1', '2020-03-1', '2020-01-2', '2020-01-2', '2020-01-0']
    issue = [
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://www.tensorflow.org/api_docs/python/tf/keras/applications\r\n'
                 'https://github.com/tensorflow/models/tree/master/official\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 'There are the pre-trained models for Keras found in '
                 '`tf.keras.applications`. And there are those models found on GitHub '
                 'in the Model Garden (under `/models`, as linked above). Now, from '
                 'reading the docs (both for the applications as well as those in the '
                 "Model Garden) I don't get the difference. Why do we have those two "
                 'different model repos?',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10 x64\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: N/A\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: master 2.1.0\r\n'
                 '- Python version: 3.7.6\r\n'
                 '- Installed using virtualenv? pip? conda?: N/A\r\n'
                 '- Bazel version (if compiling from source): 1.2.1\r\n'
                 '- GCC/Compiler version (if compiling from source):  Visual Studio '
                 '2019 C++ compiler\r\n'
                 '- CUDA/cuDNN version: 10.2 / 7.6.5\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 'RTX2080Ti GDDR6 11GB\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'bazel build failed\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 'bazel build //tensorflwo/tools/pip_package:build_pip_package\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 '\r\n'
                 '```\r\n'
                 'ERROR: D:/repo/tensorflow/tensorflow/BUILD:867:1: Executing genrule '
                 '//tensorflow:tf_python_api_gen_v2 failed (Exit 1)\r\n'
                 'C:\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py:140: '
                 'UserWarning: mkl-service package failed to import, therefore '
                 'Intel(R) MKL initialization ensuring its correct out-of-the box '
                 'operation under condition when Gnu OpenMP had already been loaded '
                 'by Python process is not assured. Please install mkl-service '
                 'package, see http://github.com/IntelPython/mkl-service\r\n'
                 '  from . import _distributor_init\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"C:\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py", line '
                 '24, in <module>\r\n'
                 '    from . import multiarray\r\n'
                 '  File '
                 '"C:\\Anaconda3\\lib\\site-packages\\numpy\\core\\multiarray.py", '
                 'line 14, in <module>\r\n'
                 '    from . import overrides\r\n'
                 '  File '
                 '"C:\\Anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py", '
                 'line 7, in <module>\r\n'
                 '    from numpy.core._multiarray_umath import (\r\n'
                 'ImportError: DLL load failed: The specified module could not be '
                 'found.\r\n'
                 '\r\n'
                 'During handling of the above exception, another exception '
                 'occurred:\r\n'
                 '\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\ALAN-W~1\\AppData\\Local\\Temp\\Bazel.runfiles_t25kvwuy\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py", '
                 'line 27, in <module>\r\n'
                 '    from tensorflow.python.tools.api.generator import doc_srcs\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\ALAN-W~1\\AppData\\Local\\Temp\\Bazel.runfiles_t25kvwuy\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py", '
                 'line 48, in <module>\r\n'
                 '    import numpy as np\r\n'
                 '  File "C:\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py", '
                 'line 142, in <module>\r\n'
                 '    from . import core\r\n'
                 '  File '
                 '"C:\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py", line '
                 '54, in <module>\r\n'
                 '    raise ImportError(msg)\r\n'
                 'ImportError:\r\n'
                 '\r\n'
                 'IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS '
                 'ISSUE!\r\n'
                 '\r\n'
                 'Importing the numpy c-extensions failed.\r\n'
                 '- Try uninstalling and reinstalling numpy.\r\n'
                 '- If you have already done that, then:\r\n'
                 '  1. Check that you expected to use Python3.7 from '
                 '"C:\\Anaconda3\\python.exe",\r\n'
                 '     and that you have no directories in your PATH or PYTHONPATH '
                 'that can\r\n'
                 '     interfere with the Python and numpy version "1.18.1" you\'re '
                 'trying to use.\r\n'
                 '  2. If (1) looks fine, you can open a new issue at\r\n'
                 '     https://github.com/numpy/numpy/issues.  Please include details '
                 'on:\r\n'
                 '     - how you installed Python\r\n'
                 '     - how you installed numpy\r\n'
                 '     - your operating system\r\n'
                 '     - whether or not you have multiple versions of Python '
                 'installed\r\n'
                 '     - if you built from source, your compiler versions and ideally '
                 'a build log\r\n'
                 '\r\n'
                 "- If you're working with a numpy git repository, try `git clean "
                 '-xdf`\r\n'
                 '  (removes all files not under version control) and rebuild '
                 'numpy.\r\n'
                 '\r\n'
                 "Note: this error has many possible causes, so please don't comment "
                 'on\r\n'
                 'an existing issue about this - open a new one instead.\r\n'
                 '\r\n'
                 'Original error was: DLL load failed: The specified module could not '
                 'be found.\r\n'
                 '\r\n'
                 'Target //tensorflow/tools/pip_package:build_pip_package failed to '
                 'build\r\n'
                 'ERROR: D:/repo/tensorflow/tensorflow/tools/pip_package/BUILD:229:1 '
                 'Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit '
                 '1)\r\n'
                 '```\r\n',
         'created_at': '2020-01-1'},
        {'body': "For a given image, I'm extracting ResNet features i.e. after all "
                 'conv layers and global max pooling, which gives a 2048 length '
                 'vector per image. \r\n'
                 '\r\n'
                 'Earlier, I was using `keras==2.3.1` with backend '
                 "`tensorflow==1.13.1`. Now, I've shifted to `tensorflow==2.0.0` "
                 'since keras has been merged with tensorflow. I replaced my code '
                 'with `tf.keras` instead of `keras`.\r\n'
                 '\r\n'
                 'But now the features extracted are not the same as the features '
                 'extracted earlier. ResNet is a model which is independent of '
                 "tensorflow/keras or even pytorch for that matter. It's "
                 'functionality is predefined. Why is this difference occuring? Are '
                 'there any parameters that can be tweaked to get the same '
                 'functionality?\\\r\n'
                 '\r\n'
                 '```\r\n'
                 '    from tensorflow.keras.applications.resnet50 import ResNet50, '
                 'preprocess_input\r\n'
                 '    resnet_model = ResNet50(include_top=False)\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- TensorFlow version (you are using): 2.1.0\r\n'
                 '- Are you willing to contribute it (Yes/No): maybe\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 'Currently the [EarlyStopping '
                 'callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) '
                 '`min_delta` is an absolute value. Meaning that the metric being '
                 'monitored must improve by that fixed amount. The proposal is to add '
                 'the ability to use a relative threshold instead (e.g. a ratio or '
                 'percentage).\r\n'
                 '\r\n'
                 'An example use case is probably easier to understand. Lets say '
                 "we're doing a hyperparameter search, so we're running a bunch of "
                 'different models, and we want to stop early on the bad ones. On '
                 'good models our loss value is in the single digits. So we set the '
                 'parameters `baseline=50` (a little high so that we allow models to '
                 'start off somewhat bad), and `min_delta=0.1`. Now with this, if a '
                 "model gets under 50, but drops to just 48.9, that'll keep it from "
                 'triggering the abort threshold. If this were instead a ratio, such '
                 'that `min_delta=0.1` means `10%`, the loss would have to go from 50 '
                 'to 45 to keep it from triggering, and a good model at say 1.2 would '
                 'only have to drop to 1.08.\r\n'
                 '\r\n'
                 'Now we could try other approaches to address this problem, such as '
                 'lowering `baseline`, and increasing `patience` (allowing bad starts '
                 'a little longer to come down further), but by increasing patience, '
                 'this increases the amount of time we have to wait for good models '
                 'to stop as well.\r\n'
                 '\r\n'
                 '&nbsp;\r\n'
                 '\r\n'
                 'Instead of adding a flag for toggling whether `min_delta` is '
                 'absolute or relative, I think it might be better to allow the user '
                 'to provide their own "is this new value good enough" '
                 'function/lambda. Currently you can technically override '
                 "`EarlyStopping.monitor_op`, but it's clearly not designed to "
                 "operate that way as the arguments to `monitor_op` aren't simple "
                 '"best" and "current" values, and there\'s also code which directly '
                 'checks whether `monitor_op == np.less`, which overriding breaks.\r\n'
                 '\r\n'
                 'So ultimately the request is to either add a new argument to the '
                 'initializer accepting the signature `(current:float, best:float) -> '
                 'bool`, or adjust the code to allow overriding `monitor_op` with '
                 'that signature.\r\n'
                 '\r\n'
                 '**Will this change the current api? How?**\r\n'
                 'Maybe, if the direction is to add an additional parameter to the '
                 'initializer.\r\n'
                 '\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 'Those doing hyperparameter searches, or potentially other cases.\r\n'
                 '\r\n'
                 '**Any Other info.**\r\n',
         'created_at': '2020-01-1'},
        {'body': 'This PR enables the following kernels for complex64 and complex128 '
                 'on ROCm:\r\n'
                 'CSRSparseMatrixToDense\r\n'
                 'DenseToCSRSparseMatrix\r\n'
                 'SparseMatrixMul\r\n'
                 'CSRSparseMatrixComponents\r\n'
                 'DenseToCSRSparseMatrix\r\n'
                 'SparseTensorToCSRSparseMatrix\r\n'
                 '\r\n'
                 'In addition, it fixes a problem in one of the sparse kernel '
                 'implementations, and enables unit tests for all of the above.',
         'created_at': '2020-01-1'},
        {'body': 'This PR enables the kernel Relu for float16 on ROCm and removes '
                 "some obsolete #ifdef's from relu_op_gpu.cu.cc.",
         'created_at': '2020-01-1'},
        {'body': 'Add python dev in CI can efficiently avoid the problem in next '
                 'release. it it can make TensorFlow always support latest python '
                 'stable release.',
         'created_at': '2020-01-1'},
        {'body': '- adding example cwrapper which generates the projects with simple '
                 'api\r\n'
                 '- adding make library option to the makefile template\r\n'
                 '- make project have the compiler path defined setting\r\n'
                 '- adds a parser to pull out only the used ops from a tflite model',
         'created_at': '2020-01-1'},
        {'body': 'Hey guys,\r\n'
                 '\r\n'
                 "I'm having trouble with using a placement function inside "
                 '`tf.device(...)`. Basically I want that all network variables are '
                 'placed on a specific device (e.g. cpu). All other tensors and '
                 'operations should be placed at the current scope. Because a '
                 'placement function inside `tf.decive(...)` needs always to return a '
                 'device I implemented the function `pin_network_variables`, which '
                 'separates the variables, tensors and ops to two devices.\r\n'
                 'My problem with that is, that now nesting multiple device scopes '
                 "doesn't work any more. Now all variables, tensors and ops are "
                 'placed accordingly to `pin_network_variables`.\r\n'
                 'Here is a minimal example to illustrate the behavior:\r\n'
                 '\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 'from tensorflow.keras.layers import Dense\r\n'
                 '\r\n'
                 'def pin_network_variables(var_device, op_device):\r\n'
                 '    """\r\n'
                 '    Pins network variables to the specified device.\r\n'
                 '\r\n'
                 '    Args:\r\n'
                 '        var_device: Device to put network variables on.\r\n'
                 '        op_device: Device for everything except network '
                 'variables.\r\n'
                 '    """\r\n'
                 '\r\n'
                 "    VAR_NAME_TYPS = ['kernel', 'bias', 'gamma', 'beta', "
                 "'moving_mean', 'moving_variance', 'recurrent_kernel']\r\n"
                 '    def _is_network_var(op):\r\n'
                 '        for var_type in VAR_NAME_TYPS:\r\n'
                 "            if op.name.endswith('/' + var_type):\r\n"
                 '                return True\r\n'
                 '        return False\r\n'
                 '\r\n'
                 '    def _assign(op):\r\n'
                 '        if _is_network_var(op):\r\n'
                 '            return var_device\r\n'
                 '        else:\r\n'
                 '            return op_device\r\n'
                 '\r\n'
                 '    return _assign\r\n'
                 '\r\n'
                 'def main(unused_argv):\r\n'
                 '\r\n'
                 '    # normally all network varibles whould be placed on cpu and '
                 'everything else on gpu\r\n'
                 "    with tf.device(pin_network_variables('/cpu:0', '/gpu:0')):\r\n"
                 '\r\n'
                 '        input_tensor = tf.ones((1, 5), tf.float32, '
                 "'input_tensor')\r\n"
                 '\r\n'
                 '        # the kernel and bias tensors are placed on cpu '
                 'correctly\r\n'
                 '        dense_1 = Dense(units=3,\r\n'
                 "                        activation='softmax',\r\n"
                 '                        use_bias=True,\r\n'
                 "                        kernel_initializer='glorot_uniform',\r\n"
                 "                        bias_initializer='zeros',\r\n"
                 "                        name='dense_1')\r\n"
                 '        dense_2 = Dense(units=3,\r\n'
                 "                        activation='softmax',\r\n"
                 '                        use_bias=True,\r\n'
                 "                        kernel_initializer='glorot_uniform',\r\n"
                 "                        bias_initializer='zeros',\r\n"
                 "                        name='dense_2')\r\n"
                 '\r\n'
                 '        # the call ops of the layers are placed on gpu correctly\r\n'
                 '        dense_1_out = dense_1(input_tensor)\r\n'
                 '        dense_2_out = dense_2(input_tensor)\r\n'
                 '\r\n'
                 '        # in this scope are for example tensors and ops that need '
                 'to be put on cpu, for computational reason\r\n'
                 "        with tf.device('/cpu:0'):\r\n"
                 '\r\n'
                 '            # this op shoud be placed on cpu, but its put on gpu '
                 "(due to 'pin_network_variables')\r\n"
                 '            output_tensor = tf.add(dense_1_out, dense_2_out)\r\n'
                 '\r\n'
                 "    print('--------------------')\r\n"
                 "    print('input_tensor\\t', input_tensor.device)\r\n"
                 "    print('dense_1.kernel\\t', dense_1.kernel.device)\r\n"
                 "    print('dense_1.bias\\t', dense_1.bias.device)\r\n"
                 "    print('dense_2.kernel\\t', dense_2.kernel.device)\r\n"
                 "    print('dense_2.bias\\t', dense_2.bias.device)\r\n"
                 "    print('dense_1_out\\t', dense_1_out.device)\r\n"
                 "    print('dense_2_out\\t', dense_2_out.device)\r\n"
                 "    print('output_tensor\\t', output_tensor.device) # this should "
                 'be put on cpu\r\n'
                 "    print('--------------------')\r\n"
                 '\r\n'
                 '    # create, initialise and session\r\n'
                 '    sess = tf.compat.v1.Session()\r\n'
                 '    sess.run(tf.compat.v1.global_variables_initializer())\r\n'
                 '    output = sess.run(output_tensor)\r\n'
                 '    print(output)\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n"
                 '    tf.compat.v1.app.run()\r\n'
                 '```\r\n'
                 '\r\n'
                 'Does anybody now a fix to this?\r\n'
                 '\r\n'
                 'OS Platform and Distribution: Windows 10\r\n'
                 'TensorFlow installed from: pip\r\n'
                 'TensorFlow version: 1.14.0\r\n'
                 'Python version: 3.6',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X '
                 'Catalina (10.15.2)\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca '
                 '2.0.0\r\n'
                 '- Python version: 3.7.5\r\n'
                 '- GPU model and memory: Intel Iris Pro 1536 MB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'I get the errors\r\n'
                 '\r\n'
                 '> tensorflow.python.eager.core._FallbackException: This function '
                 'does not handle the case of the path where all inputs are not '
                 'already EagerTensors.\r\n'
                 '\r\n'
                 'then \r\n'
                 '\r\n'
                 "> AttributeError: 'Tensor' object has no attribute "
                 "'_datatype_enum'\r\n"
                 '\r\n'
                 'and then\r\n'
                 '\r\n'
                 "> AttributeError: 'ProgbarLogger' object has no attribute "
                 "'log_values'\r\n"
                 '\r\n'
                 'when I add the following callback to the list of callbacks of '
                 '`my_model.fit`\r\n'
                 '\r\n'
                 '```\r\n'
                 'my_callback = '
                 'tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch, '
                 'logs: tf.print(my_model.losses))\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'No error.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 '\r\n'
                 'def get_model():\r\n'
                 '    inp = tf.keras.layers.Input(shape=(1,))\r\n'
                 '    x = tf.keras.layers.Dense(8, '
                 'activity_regularizer=tf.keras.regularizers.l1(0.01))(inp)\r\n'
                 '    x = tf.keras.layers.Dense(16, '
                 'activity_regularizer=tf.keras.regularizers.l1(0.01))(x)\r\n'
                 '    out = tf.keras.layers.Dense(1)(x)\r\n'
                 '    model = tf.keras.Model(inputs=inp, outputs=out)\r\n'
                 '    return model\r\n'
                 '\r\n'
                 '\r\n'
                 'def train():\r\n'
                 '    my_model = get_model()\r\n'
                 '    my_model.compile(optimizer="adam", loss="mse")\r\n'
                 '    my_callback = '
                 'tf.keras.callbacks.LambdaCallback(on_batch_begin=lambda batch, '
                 'logs: tf.print(my_model.losses))\r\n'
                 '    my_model.fit([1, 2, 3, 4], [0.1, 0.2, 0.4, 0.2], '
                 'callbacks=[my_callback])\r\n'
                 '\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n"
                 '    train()\r\n'
                 '```\r\n'
                 '\r\n'
                 'This issue may be related to '
                 'https://github.com/tensorflow/tensorflow/issues/28924 and '
                 'https://github.com/tensorflow/tensorflow/issues/29931. Note that, '
                 "if I don't use any regulariser, `tf.print` prints an empty list and "
                 'no error occurs.',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a feature request. As per our '
                 '[GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. '
                 'tag:feature_template</em>\r\n'
                 '\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- TensorFlow version (you are using): 2.*\r\n'
                 '- Are you willing to contribute it (Yes/No): No\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 "TF_Compile doesn't accept saved_model\r\n"
                 '**Will this change the current api? How?**\r\n'
                 'it would add a saved_model entry under tf_library macro\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 'Anyone using tf2.* and tf-compile. Frozen graphs are deprecated in '
                 '2.0\r\n'
                 '**Any Other info.**\r\n'
                 '<3',
         'created_at': '2020-01-1'},
        {'body': 'This PR is one of steps to extend 8-bit quantization to support '
                 'symmetric 16-bit activations.\r\n'
                 '\r\n'
                 'Each activation is of type int16 and symmetric around zero. The '
                 'weight tensor precision remains at 8-bit signed values. The bias is '
                 'set to int64 precision.\r\n'
                 '\r\n'
                 'In this PR we introduce implementation and tests for CONV_2D kernel '
                 'reference function.\r\n'
                 'The specification of this operator:\r\n'
                 '\r\n'
                 'CONV_2D \r\n'
                 '\u202f Input 0: \r\n'
                 '\u202f \u202f data_type \u202f: int16 \r\n'
                 '\u202f \u202f range \u202f \u202f \u202f: [-32768, 32767] \r\n'
                 '\u202f \u202f granularity: per-tensor, zero_point=0 \r\n'
                 '\u202f Input 1 (Weight): \r\n'
                 '\u202f \u202f data_type \u202f: int8 \r\n'
                 '\u202f \u202f range \u202f \u202f \u202f: [-127, 127] \r\n'
                 '\u202f \u202f granularity: per-axis (dim = 0), zero_point=0 \r\n'
                 '\u202f Input 2 (Bias): \r\n'
                 '\u202f \u202f data_type \u202f: int64 \r\n'
                 '\u202f \u202f range \u202f \u202f \u202f: [-(1<<39), (1<<39)-1] \r\n'
                 '\u202f \u202f granularity: per-axis (dim = 0), zero_point=0 \r\n'
                 '\u202f \u202f restriction: (scale, zero_point) = (input0_scale * '
                 'input1_scale[...], 0) \r\n'
                 '\u202f Output 0: \r\n'
                 '\u202f \u202f data_type \u202f: int16 \r\n'
                 '\u202f \u202f range \u202f \u202f \u202f: [-32768,32767] \r\n'
                 '\u202f \u202f granularity: per-tensor, zero_point=0',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- custom code\r\n'
                 '- Ubuntu 18.04.1 LTS\r\n'
                 '- Thinkpad X240\r\n'
                 '- TensorFlow installed via pip3\r\n'
                 '- TensorFlow v2.0.0-rc2-26-g64c3d38 2.0.0\r\n'
                 '- Python 3.6.8\r\n'
                 '- no CUDA/cuDNN\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'The code below generates the output `(1, 0)` for the first print '
                 'when using the generator directly and the exception below when '
                 'wrapping the generator using `tf.data.Dataset.from_generator`.\r\n'
                 '\r\n'
                 '```\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'InvalidArgumentError                      Traceback (most recent '
                 'call last)\r\n'
                 '<ipython-input-35-36b15431cf1a> in <module>()\r\n'
                 '     17 \r\n'
                 '     18 print( next( movingWindow( data, window_size ) ) )\r\n'
                 '---> 19 print( next( iter( dataset ) ) )\r\n'
                 '\r\n'
                 '/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py '
                 'in __next__(self)\r\n'
                 '    620 \r\n'
                 '    621   def __next__(self):  # For Python 3 compatibility\r\n'
                 '--> 622     return self.next()\r\n'
                 '    623 \r\n'
                 '    624   def _next_internal(self):\r\n'
                 '\r\n'
                 '/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py '
                 'in next(self)\r\n'
                 '    664     """Returns a nested structure of `Tensor`s containing '
                 'the next element."""\r\n'
                 '    665     try:\r\n'
                 '--> 666       return self._next_internal()\r\n'
                 '    667     except errors.OutOfRangeError:\r\n'
                 '    668       raise StopIteration\r\n'
                 '\r\n'
                 '/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py '
                 'in _next_internal(self)\r\n'
                 '    649             self._iterator_resource,\r\n'
                 '    650             output_types=self._flat_output_types,\r\n'
                 '--> 651             output_shapes=self._flat_output_shapes)\r\n'
                 '    652 \r\n'
                 '    653       try:\r\n'
                 '\r\n'
                 '/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py '
                 'in iterator_get_next_sync(iterator, output_types, output_shapes, '
                 'name)\r\n'
                 '   2671       else:\r\n'
                 '   2672         message = e.message\r\n'
                 '-> 2673       _six.raise_from(_core._status_to_exception(e.code, '
                 'message), None)\r\n'
                 '   2674   # Add nodes to the TensorFlow graph.\r\n'
                 '   2675   if not isinstance(output_types, (list, tuple)):\r\n'
                 '\r\n'
                 '/usr/lib/python3/dist-packages/six.py in raise_from(value, '
                 'from_value)\r\n'
                 '\r\n'
                 'InvalidArgumentError: TypeError: an integer is required\r\n'
                 'Traceback (most recent call last):\r\n'
                 '\r\n'
                 '  File '
                 '"/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/script_ops.py", '
                 'line 221, in __call__\r\n'
                 '    ret = func(*args)\r\n'
                 '\r\n'
                 '  File '
                 '"/home/maximiliank/.local/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", '
                 'line 585, in generator_py_func\r\n'
                 '    values = next(generator_state.get_iterator(iterator_id))\r\n'
                 '\r\n'
                 '  File "<ipython-input-35-36b15431cf1a>", line 3, in '
                 'movingWindow\r\n'
                 '    buffer = collections.deque( data[:window_size-1], maxlen = '
                 'window_size )\r\n'
                 '\r\n'
                 'TypeError: an integer is required\r\n'
                 '\r\n'
                 '\r\n'
                 '\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]\r\n'
                 '```\r\n'
                 '\r\n'
                 'As it turns out this is because the `window_size` given in the '
                 '`args` argument is unexpectedly converted from `int` to `np.int32`, '
                 "which can't be used interchangeably for slicing or the deque maxlen "
                 'parameter.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'The example should work without exception. Which means, '
                 'from_generator should not change the types of any arguments given '
                 'via the `args` parameter.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 '\r\n'
                 '```Python3\r\n'
                 'import tensorflow as tf\r\n'
                 'import numpy as np\r\n'
                 'import collections\r\n'
                 '\r\n'
                 'def movingWindow( data, window_size ):\r\n'
                 '    #window_size = int( window_size )\r\n'
                 '    buffer = collections.deque( data[:window_size-1], maxlen = '
                 'window_size )\r\n'
                 '    for i, datum in enumerate( data[window_size-1:] ):\r\n'
                 '        buffer.append( datum )\r\n'
                 '        for b in buffer:\r\n'
                 '            yield datum, b\r\n'
                 '\r\n'
                 'window_size = 2\r\n'
                 'data = np.arange( 10 )\r\n'
                 '\r\n'
                 'dataset = tf.data.Dataset.from_generator( \r\n'
                 '    movingWindow,\r\n'
                 '    args = ( data, window_size ),\r\n'
                 '    output_types = ( np.int32, np.int32 )\r\n'
                 ')\r\n'
                 '\r\n'
                 'print( next( movingWindow( data, window_size ) ) )\r\n'
                 'print( next( iter( dataset ) ) )\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Custom code\r\n'
                 '- TensorFlow version 2.1.0\r\n'
                 '- Python version: 3.7\r\n'
                 '- GPU model: 4 V100 GPUs on Kubernetes Engine\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'On multi GPU loading the model from a h5 file is not working. \r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Saving and reloading the model from a h5 file using model.save and  '
                 'keras.models.load_model should work on both single and multi '
                 'GPU.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '``` python \r\n'
                 'import tensorflow as tf \r\n'
                 'import os\r\n'
                 'import contextlib\r\n'
                 'import numpy as np\r\n'
                 'import tensorflow.keras as keras  \r\n'
                 '\r\n'
                 'def get_model():\r\n'
                 '    model = keras.Sequential([\r\n'
                 '        keras.layers.Flatten(input_shape=(28, 28)),\r\n'
                 '        keras.layers.Dense(10, activation=tf.nn.softmax)\r\n'
                 '    ])\r\n'
                 '    model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n'
                 "                      loss='sparse_categorical_crossentropy')\r\n"
                 '    return model\r\n'
                 '\r\n'
                 'def get_model_path():\r\n'
                 "    model_dir = '/tmp/m' + str(np.random.randint(0, 1000000))\r\n"
                 '    os.makedirs(model_dir)\r\n'
                 "    model_path = os.path.join(model_dir, 'model')\r\n"
                 '    return model_path + ".h5"\r\n'
                 '\r\n'
                 'def attempt_save_and_reload(model_path, '
                 'distributed_training=False):\r\n'
                 '    fashion_mnist = keras.datasets.fashion_mnist\r\n'
                 '    (train_images, train_labels), (test_images, test_labels) = '
                 'fashion_mnist.load_data()\r\n'
                 '    train_images = train_images / 255.0\r\n'
                 '    test_images = test_images / 255.0\r\n'
                 '\r\n'
                 '    with strategy.scope() if distributed_training else '
                 'contextlib.nullcontext():\r\n'
                 '        model = get_model()\r\n'
                 '        model.fit(\r\n'
                 '            train_images,\r\n'
                 '            train_labels,\r\n'
                 '            epochs=1,\r\n'
                 '        )\r\n'
                 '        model.save(model_path)\r\n'
                 '        model = tf.keras.models.load_model(model_path)\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n"
                 '    strategy = tf.distribute.MirroredStrategy()\r\n'
                 '    for distributed_training in [False, True]:\r\n'
                 "        print('distributed training: ', distributed_training)\r\n"
                 '        model_path = get_model_path()\r\n'
                 '        try:\r\n'
                 '            attempt_save_and_reload(model_path, '
                 'distributed_training)\r\n'
                 '        except Exception as e:\r\n'
                 "            print('Exception raised: \\n', e)\r\n"
                 '        print()\r\n'
                 '```\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**other info/ logs**\r\n'
                 'I need to use h5 files since saving the optimizer state does not '
                 'work otherwise (see #33424).  The logs I get are: \r\n'
                 '```\r\n'
                 'INFO:tensorflow:Using MirroredStrategy with devices '
                 "('/job:localhost/replica:0/task:0/device:GPU:0', "
                 "'/job:localhost/replica:0/task:0/device:GPU:1', "
                 "'/job:localhost/replica:0/task:0/device:GPU:2', "
                 "'/job:localhost/replica:0/task:0/device:GPU:3')\r\n"
                 'distributed training:  False\r\n'
                 'Train on 60000 samples\r\n'
                 '60000/60000 [==============================] - 3s 52us/sample - '
                 'loss: 0.5991\r\n'
                 '\r\n'
                 'distributed training:  True\r\n'
                 'Train on 60000 samples\r\n'
                 'INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = '
                 'nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and '
                 'agg_small_grads_max_group = 10\r\n'
                 'INFO:tensorflow:Reduce to '
                 '/job:localhost/replica:0/task:0/device:CPU:0 then broadcast to '
                 "('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n"
                 'INFO:tensorflow:Reduce to '
                 '/job:localhost/replica:0/task:0/device:CPU:0 then broadcast to '
                 "('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n"
                 'INFO:tensorflow:batch_all_reduce: 2 all-reduces with algorithm = '
                 'nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and '
                 'agg_small_grads_max_group = 10\r\n'
                 'INFO:tensorflow:Reduce to '
                 '/job:localhost/replica:0/task:0/device:CPU:0 then broadcast to '
                 "('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n"
                 'INFO:tensorflow:Reduce to '
                 '/job:localhost/replica:0/task:0/device:CPU:0 then broadcast to '
                 "('/job:localhost/replica:0/task:0/device:CPU:0',).\r\n"
                 '60000/60000 [==============================] - 9s 152us/sample - '
                 'loss: 0.6016\r\n'
                 'Exception raised: \r\n'
                 ' `handle` is not available outside the replica context or a '
                 '`tf.distribute.Strategy.update()` call.\r\n'
                 '```\r\n'
                 '\r\n',
         'created_at': '2020-01-1'},
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c05_exercise_rock_paper_scissors.ipynb\r\n'
                 '---------------------------------------------------------------\r\n'
                 '![Screenshot from 2020-01-16 '
                 '18-37-30](https://user-images.githubusercontent.com/29497701/72527695-5d004900-388f-11ea-84f8-57ed0c12c915.png)\r\n'
                 '----------------------------------------------------------------\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 "I think this exercise doesn't make use of cats_vs_dogs dataset, "
                 'right??\r\n'
                 '\r\n'
                 '### Clear description\r\n'
                 '\r\n'
                 'In place of `cats_vs_dogs` dataset, `rock_paper_scissors` dataset '
                 'should be mentioned.\r\n'
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 'Yes, shortly',
         'created_at': '2020-01-1'},
        {'body': '@tensorflow/micro\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): '
                 'Linux Ubuntu 19.10\r\n'
                 '- TensorFlow installed from (source or binary): source (for '
                 'inference), python (for training)\r\n'
                 '- Tensorflow version (commit SHA if source): '
                 '1768c8f2fa155d4c6406e8ff7addf374c83de7ad for inference, release 2.0 '
                 'or 2.1 for training.\r\n'
                 '- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): '
                 '[RIOT](https://github.com/RIOT-OS/RIOT), ARM Cortex-M3/4/7\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 'I recently ported TensorFlow Lite to RIOT, an operating system for '
                 'microcontrollers, using the package mechanism provided by the RIOT '
                 'build system: this allows to build TensorFlow-Lite on the fly when '
                 "generating a RIOT firmware and to use it's API from a RIOT "
                 'application.\r\n'
                 'See https://github.com/RIOT-OS/RIOT/pull/12847 for details.\r\n'
                 '\r\n'
                 'RIOT provides support for many boards including many ARM based '
                 'boards. It also supports both the GCC and the LLVM (Clang) '
                 'toolchains.\r\n'
                 '\r\n'
                 'Our CI showed problems when building with Clang, see below for '
                 'details and [this '
                 'issue](https://github.com/RIOT-OS/RIOT/issues/13133). In short, the '
                 'firmware is crashing when evaluating the FullyConnected operator on '
                 'ARM Cortex-M but the same thing works fine with GCC.\r\n'
                 '\r\n'
                 'The [example application in '
                 'RIOT](https://github.com/RIOT-OS/RIOT/tree/master/tests/pkg_tensorflow-lite) '
                 'is just running a very basic MLP model (with Dense and Softmax '
                 'layers) on an image taken from the MNIST dataset. For details of '
                 'the application, you can have a look at the [main_functions.cc '
                 'file](https://github.com/RIOT-OS/RIOT/blob/master/tests/pkg_tensorflow-lite/mnist/main_functions.cc) '
                 'and [the '
                 'script](https://github.com/RIOT-OS/RIOT/blob/master/tests/pkg_tensorflow-lite/mnist/mnist_mlp.py) '
                 'used to generate the flatbuffers file containing the model.\r\n'
                 '\r\n'
                 'Note that `hello_world` example is running fine, even when built '
                 "with LLVM so I don't know what's wrong here: is the Python script "
                 'or is it the way the `MicroMutableOpResolver` is built ?\r\n'
                 '\r\n'
                 'Sorry for the long description but I wanted to make as complete as '
                 'possible.\r\n'
                 '\r\n'
                 '**Please provide the exact sequence of commands/steps when you ran '
                 'into the problem**\r\n'
                 '\r\n'
                 '- Software required on the host:\r\n'
                 '  - **clang**: on Ubuntu, can be installed with `apt install '
                 'clang`\r\n'
                 '  - **ARM Cortex-M gdb**, from the [GNU ARM '
                 'toolchain](https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-rm) '
                 '(this is for the debug command below).\r\n'
                 '  - pyocd, can be installed with `pip3 install --user pyocd`\r\n'
                 '  - socat, on Ubuntu, can be installed with `apt install socat`\r\n'
                 '- Build/flash/run the default tensorflow-lite example of RIOT on an '
                 '[nrf52832-mdk](https://wiki.makerdiary.com/nrf52832-mdk/):\r\n'
                 '```\r\n'
                 'DEVELHELP=1 RIOT_TERMINAL=socat TOOLCHAIN=llvm make '
                 'BOARD=nrf52832-mdk -C tests/pkg_tensorflow-lite flash term\r\n'
                 '```\r\n'
                 '- You can check where the program crashed using the `debug` target '
                 '(just follow the instructions reported by the RIOT crash):\r\n'
                 '```\r\n'
                 'make BOARD=nrf52832-mdk -C tests/pkg_tensorflow-lite debug\r\n'
                 '```\r\n'
                 'Then in gdb:\r\n'
                 '```gdb\r\n'
                 'set $pc=0x8006b04\r\n'
                 'frame 0\r\n'
                 'bt\r\n'
                 '```\r\n'
                 'You get the following output:\r\n'
                 '```gdb\r\n'
                 'Reading symbols from '
                 '/work/riot/RIOT/tests/pkg_tensorflow-lite/bin/stm32f723e-disco/tests_pkg_tensorflow-lite.elf...\r\n'
                 'Remote debugging using :3333\r\n'
                 'hard_fault_handler (sp=0x20001bb8 <setup()::static_interpreter+16>, '
                 'corrupted=1132396544, \r\n'
                 '    exc_return=536882079, r4_to_r11_stack=0x310) at '
                 'vectors_cortexm.c:393\r\n'
                 '393\t    __BKPT(1);\r\n'
                 '(gdb) set $pc=0x8006bbe\r\n'
                 '(gdb) frame 0\r\n'
                 '#0  tflite::GetOptionalInputTensor (context=0x20001bb8 '
                 '<setup()::static_interpreter+16>, node=0xc46c, \r\n'
                 '    index=2)\r\n'
                 '    at '
                 '/work/riot/RIOT/tests/pkg_tensorflow-lite/bin/pkg/stm32f723e-disco/tensorflow-lite/tensorflow/lite/kernels/kernel_util.h:80\r\n'
                 '80\t  const bool use_tensor = index < node->inputs->size &&\r\n'
                 '(gdb) bt\r\n'
                 '#0  tflite::GetOptionalInputTensor (context=0x20001bb8 '
                 '<setup()::static_interpreter+16>, node=0xc46c, \r\n'
                 '    index=2)\r\n'
                 '    at '
                 '/work/riot/RIOT/tests/pkg_tensorflow-lite/bin/pkg/stm32f723e-disco/tensorflow-lite/tensorflow/lite/kernels/kernel_util.h:80\r\n'
                 '#1  tflite::ops::micro::fully_connected::Eval (context=0x20001bb8 '
                 '<setup()::static_interpreter+16>, \r\n'
                 '    node=0xc46c) at fully_connected.cc:172\r\n'
                 '#2  0x08002276 in tflite::MicroInterpreter::Invoke (this=0x20001ba8 '
                 '<setup()::static_interpreter>)\r\n'
                 '    at micro_interpreter.cc:201\r\n'
                 '#3  0x080018b0 in setup () at main_functions.cc:100\r\n'
                 '#4  0x0800164e in main (argc=5, argv=0xc46c) at main.cpp:28\r\n'
                 '(gdb) quit\r\n'
                 '```\r\n'
                 '\r\n'
                 'The board is not very important, this command can be adapted for a '
                 'lot of other ARM based boards supported by RIOT: STM32 nucleo, '
                 "kinetis, etc. Note that you'll have to install the right tool for "
                 'flashing the boards (OpenOCD, JLink, etc) depending on the board '
                 'configuration.\r\n'
                 '\r\n',
         'created_at': '2020-01-1'},
        {'body': '(base) C:\\Users\\vishwasnarayan>conda install -c anaconda '
                 'tensorflow\r\n'
                 'Collecting package metadata (current_repodata.json): done\r\n'
                 'Solving environment: done\r\n'
                 '\r\n'
                 '## Package Plan ##\r\n'
                 '\r\n'
                 '  environment location: '
                 'C:\\Users\\vishwasnarayan\\AppData\\Local\\Continuum\\anaconda3\r\n'
                 '\r\n'
                 '  added / updated specs:\r\n'
                 '    - tensorflow\r\n'
                 '\r\n'
                 '\r\n'
                 'The following packages will be downloaded:\r\n'
                 '\r\n'
                 '    package                    |            build\r\n'
                 '    ---------------------------|-----------------\r\n'
                 '    _tflow_select-2.3.0        |              mkl           3 KB  '
                 'anaconda\r\n'
                 '    absl-py-0.8.1              |           py37_0         162 KB  '
                 'anaconda\r\n'
                 '    astor-0.8.0                |           py37_0          45 KB  '
                 'anaconda\r\n'
                 '    gast-0.2.2                 |           py37_0         138 KB  '
                 'anaconda\r\n'
                 '    google-pasta-0.1.8         |             py_0          43 KB  '
                 'anaconda\r\n'
                 '    keras-applications-1.0.8   |             py_0          33 KB  '
                 'anaconda\r\n'
                 '    keras-preprocessing-1.1.0  |             py_1          36 KB  '
                 'anaconda\r\n'
                 '    libmklml-2019.0.5          |                0        21.4 MB  '
                 'anaconda\r\n'
                 '    libprotobuf-3.11.2         |       h7bd577a_0         2.3 MB  '
                 'anaconda\r\n'
                 '    markdown-3.1.1             |           py37_0         132 KB  '
                 'anaconda\r\n'
                 '    opt_einsum-3.1.0           |             py_0          54 KB  '
                 'anaconda\r\n'
                 '    protobuf-3.11.2            |   py37h33f27b4_0         597 KB  '
                 'anaconda\r\n'
                 '    tensorboard-2.0.0          |     pyhb38c66f_1         3.3 MB  '
                 'anaconda\r\n'
                 '    tensorflow-2.0.0           |mkl_py37he1bbcac_0           4 KB  '
                 'anaconda\r\n'
                 '    tensorflow-base-2.0.0      |mkl_py37hd1d5974_0        41.9 MB  '
                 'anaconda\r\n'
                 '    tensorflow-estimator-2.0.0 |     pyh2649769_0         272 KB  '
                 'anaconda\r\n'
                 '    termcolor-1.1.0            |           py37_1           7 KB  '
                 'anaconda\r\n'
                 '    ------------------------------------------------------------\r\n'
                 '                                           Total:        70.4 MB\r\n'
                 '\r\n'
                 'The following NEW packages will be INSTALLED:\r\n'
                 '\r\n'
                 '  _tflow_select      anaconda/win-64::_tflow_select-2.3.0-mkl\r\n'
                 '  absl-py            anaconda/win-64::absl-py-0.8.1-py37_0\r\n'
                 '  astor              anaconda/win-64::astor-0.8.0-py37_0\r\n'
                 '  gast               anaconda/win-64::gast-0.2.2-py37_0\r\n'
                 '  google-pasta       anaconda/noarch::google-pasta-0.1.8-py_0\r\n'
                 '  grpcio             '
                 'pkgs/main/win-64::grpcio-1.16.1-py37h351948d_1\r\n'
                 '  keras-applications '
                 'anaconda/noarch::keras-applications-1.0.8-py_0\r\n'
                 '  keras-preprocessi~ '
                 'anaconda/noarch::keras-preprocessing-1.1.0-py_1\r\n'
                 '  libmklml           anaconda/win-64::libmklml-2019.0.5-0\r\n'
                 '  libprotobuf        '
                 'anaconda/win-64::libprotobuf-3.11.2-h7bd577a_0\r\n'
                 '  markdown           anaconda/win-64::markdown-3.1.1-py37_0\r\n'
                 '  opt_einsum         anaconda/noarch::opt_einsum-3.1.0-py_0\r\n'
                 '  protobuf           '
                 'anaconda/win-64::protobuf-3.11.2-py37h33f27b4_0\r\n'
                 '  tensorboard        '
                 'anaconda/noarch::tensorboard-2.0.0-pyhb38c66f_1\r\n'
                 '  tensorflow         '
                 'anaconda/win-64::tensorflow-2.0.0-mkl_py37he1bbcac_0\r\n'
                 '  tensorflow-base    '
                 'anaconda/win-64::tensorflow-base-2.0.0-mkl_py37hd1d5974_0\r\n'
                 '  tensorflow-estima~ '
                 'anaconda/noarch::tensorflow-estimator-2.0.0-pyh2649769_0\r\n'
                 '  termcolor          anaconda/win-64::termcolor-1.1.0-py37_1\r\n'
                 '\r\n'
                 '\r\n'
                 'Proceed ([y]/n)? y\r\n'
                 '\r\n'
                 '\r\n'
                 'Downloading and Extracting Packages\r\n'
                 'gast-0.2.2           | 138 KB    | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'tensorflow-estimator | 272 KB    | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'tensorflow-base-2.0. | 41.9 MB   | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'tensorflow-2.0.0     | 4 KB      | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'astor-0.8.0          | 45 KB     | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'absl-py-0.8.1        | 162 KB    | '
                 '############################################################################ '
                 '| 100%\r\n'
                 '_tflow_select-2.3.0  | 3 KB      | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'libmklml-2019.0.5    | 21.4 MB   | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'tensorboard-2.0.0    | 3.3 MB    | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'libprotobuf-3.11.2   | 2.3 MB    | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'opt_einsum-3.1.0     | 54 KB     | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'markdown-3.1.1       | 132 KB    | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'google-pasta-0.1.8   | 43 KB     | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'keras-applications-1 | 33 KB     | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'termcolor-1.1.0      | 7 KB      | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'keras-preprocessing- | 36 KB     | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'protobuf-3.11.2      | 597 KB    | '
                 '############################################################################ '
                 '| 100%\r\n'
                 'Preparing transaction: done\r\n'
                 'Verifying transaction: failed\r\n'
                 '\r\n'
                 'CondaVerificationError: The package for tensorflow-base located at '
                 'C:\\Users\\vishwasnarayan\\AppData\\Local\\Continuum\\anaconda3\\pkgs\\tensorflow-base-2.0.0-mkl_py37hd1d5974_0\r\n'
                 'appears to be corrupted. The path '
                 "'Lib/site-packages/tensorflow-2.0.0.data/purelib/tensorflow_core/include/tensorflow_core/core/grappler/optimizers/generic_layout_optimizer_transposer_factory.h'\r\n"
                 'specified in the package manifest cannot be found.',
         'created_at': '2020-01-1'},
        {'body': 'My code works in `GPU` based tensorflow environment without any '
                 'fuss but fails in `CPU` based environments. Some other people also '
                 'are facing the same issue. `Training` works without any issues but '
                 "it's the `predict` method that's failing.\r\n"
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- TensorFlow installed from (source or binary): Source\r\n'
                 '- Other system related information below:\r\n'
                 '\r\n'
                 '```\r\n'
                 'Collecting system information...\r\n'
                 '/tmp/check_os.py:18: DeprecationWarning: dist() and '
                 'linux_distribution() functions are deprecated in Python 3.5\r\n'
                 '  platform.linux_distribution(),\r\n'
                 '/tmp/check_os.py:19: DeprecationWarning: dist() and '
                 'linux_distribution() functions are deprecated in Python 3.5\r\n'
                 '  platform.dist(),\r\n'
                 'cat: /proc/1/cgroup: No such file or directory\r\n'
                 '2020-01-16 15:58:53.661018: I '
                 'tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow '
                 'binary is optimized with Intel(R) MKL-DNN to use the following CPU '
                 'instructions in performance critical operations:  SSE4.1 SSE4.2 AVX '
                 'AVX2 FMA\r\n'
                 'To enable them in non-MKL-DNN operations, rebuild TensorFlow with '
                 'the appropriate compiler flags.\r\n'
                 '2020-01-16 15:58:53.661360: I '
                 'tensorflow/core/common_runtime/process_util.cc:115] Creating new '
                 'thread pool with default inter op setting: 12. Tune using '
                 'inter_op_parallelism_threads for best performance.\r\n'
                 "')\r\n"
                 "architecture: ('64bit', '')\r\n"
                 'machine: x86_64\r\n'
                 '\r\n'
                 '\r\n'
                 '== are we in docker '
                 '=============================================\r\n'
                 'No\r\n'
                 '\r\n'
                 '== compiler '
                 '=====================================================\r\n'
                 'Apple LLVM version 10.0.1 (clang-1001.0.46.4)\r\n'
                 'Target: x86_64-apple-darwin18.7.0\r\n'
                 'Thread model: posix\r\n'
                 'InstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n'
                 '\r\n'
                 '== check pips '
                 '===================================================\r\n'
                 'numpy                         1.17.4             \r\n'
                 'protobuf                      3.11.2             \r\n'
                 'tensorflow                    2.1.0              \r\n'
                 'tensorflow-estimator          2.1.0              \r\n'
                 'tensorflow-hub                0.7.0              \r\n'
                 '\r\n'
                 '== check for virtualenv '
                 '=========================================\r\n'
                 'False\r\n'
                 '\r\n'
                 '== tensorflow import '
                 '============================================\r\n'
                 'tf.version.VERSION = 2.0.0\r\n'
                 'tf.version.GIT_VERSION = unknown\r\n'
                 'tf.version.COMPILER_VERSION = 4.2.1 Compatible Clang 4.0.1 '
                 '(tags/RELEASE_401/final)\r\n'
                 'Sanity check: array([1], dtype=int32)\r\n'
                 '\r\n'
                 '== env '
                 '==========================================================\r\n'
                 'LD_LIBRARY_PATH is unset\r\n'
                 'DYLD_LIBRARY_PATH is unset\r\n'
                 '\r\n'
                 '== nvidia-smi '
                 '===================================================\r\n'
                 'tf_env_cololect.sh: line 147: nvidia-smi: command not found\r\n'
                 '\r\n'
                 '== cuda libs  '
                 '===================================================\r\n'
                 '\r\n'
                 '== tensorflow installed from info ==================\r\n'
                 'Name: tensorflow\r\n'
                 'Version: 2.1.0\r\n'
                 'Summary: TensorFlow is an open source machine learning framework '
                 'for everyone.\r\n'
                 'Home-page: https://www.tensorflow.org/\r\n'
                 'Author-email: packages@tensorflow.org\r\n'
                 'License: Apache 2.0\r\n'
                 'Location: '
                 '/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages\r\n'
                 'Required-by: \r\n'
                 '\r\n'
                 '== python version  '
                 '==============================================\r\n'
                 '(major, minor, micro, releaselevel, serial)\r\n'
                 "(3, 7, 6, 'final', 0)\r\n"
                 '\r\n'
                 '== bazel version  '
                 '===============================================\r\n'
                 'Build label: 1.2.1\r\n'
                 'Build time: Tue Nov 26 15:27:31 2019 (1574782051)\r\n'
                 'Build timestamp: 1574782051\r\n'
                 'Build timestamp as int: 1574782051\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'The code is running successfully in `GPU` based environment and '
                 'failing in `CPU` based environments.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'It should run/fail in the same way in both `GPU` and `CPU` based '
                 'environments.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Code in the following link has the same behavior:\r\n'
                 'https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\r\n'
                 '\r\n'
                 'And also people are talking about this issue in the comments.\r\n'
                 '\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '```\r\n'
                 'Making predictions\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"/Users/sardarmrinal/Egnyte/Private/sardar.mrinal/workspace/competitions/kaggle_nlp_disaster/working/NLP_disaster_bert.py", '
                 'line 218, in <module>\r\n'
                 '    df_sub = model_bert.predict(df_eval)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/Egnyte/Private/sardar.mrinal/workspace/competitions/kaggle_nlp_disaster/working/NLP_disaster_bert.py", '
                 'line 186, in predict\r\n'
                 '    prediction = self.model.predict(x=X)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py", '
                 'line 909, in predict\r\n'
                 '    use_multiprocessing=use_multiprocessing)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", '
                 'line 462, in predict\r\n'
                 '    steps=steps, callbacks=callbacks, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", '
                 'line 444, in _model_iteration\r\n'
                 '    total_epochs=1)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py", '
                 'line 123, in run_one_epoch\r\n'
                 '    batch_outs = execution_function(iterator)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", '
                 'line 86, in execution_function\r\n'
                 '    distributed_function(input_fn))\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 457, in __call__\r\n'
                 '    result = self._call(*args, **kwds)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 503, in _call\r\n'
                 '    self._initialize(args, kwds, '
                 'add_initializers_to=initializer_map)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 408, in _initialize\r\n'
                 '    *args, **kwds))\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 1848, in _get_concrete_function_internal_garbage_collected\r\n'
                 '    graph_function, _, _ = self._maybe_define_function(args, '
                 'kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 2150, in _maybe_define_function\r\n'
                 '    graph_function = self._create_graph_function(args, kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 2041, in _create_graph_function\r\n'
                 '    capture_by_value=self._capture_by_value),\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", '
                 'line 915, in func_graph_from_py_func\r\n'
                 '    func_outputs = python_func(*func_args, **func_kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 358, in wrapped_fn\r\n'
                 '    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", '
                 'line 73, in distributed_function\r\n'
                 '    per_replica_function, args=(model, x, y, sample_weights))\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", '
                 'line 760, in experimental_run_v2\r\n'
                 '    return self._extended.call_for_each_replica(fn, args=args, '
                 'kwargs=kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", '
                 'line 1787, in call_for_each_replica\r\n'
                 '    return self._call_for_each_replica(fn, args, kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py", '
                 'line 2132, in _call_for_each_replica\r\n'
                 '    return fn(*args, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py", '
                 'line 292, in wrapper\r\n'
                 '    return func(*args, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", '
                 'line 162, in _predict_on_batch\r\n'
                 '    return predict_on_batch(model, x)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", '
                 'line 370, in predict_on_batch\r\n'
                 '    return model(inputs)  # pylint: disable=not-callable\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py", '
                 'line 847, in __call__\r\n'
                 '    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", '
                 'line 708, in call\r\n'
                 '    '
                 'convert_kwargs_to_constants=base_layer_utils.call_context().saving)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py", '
                 'line 860, in _run_internal_graph\r\n'
                 '    output_tensors = layer(computed_tensors, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py", '
                 'line 847, in __call__\r\n'
                 '    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py", '
                 'line 292, in wrapper\r\n'
                 '    return func(*args, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py", '
                 'line 218, in call\r\n'
                 '    lambda: f(training=False))\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py", '
                 'line 56, in smart_cond\r\n'
                 '    return false_fn()\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_hub/keras_layer.py", '
                 'line 218, in <lambda>\r\n'
                 '    lambda: f(training=False))\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/load.py", '
                 'line 436, in _call_attribute\r\n'
                 '    return instance.__call__(*args, **kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 457, in __call__\r\n'
                 '    result = self._call(*args, **kwds)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 494, in _call\r\n'
                 '    results = self._stateful_fn(*args, **kwds)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 1822, in __call__\r\n'
                 '    graph_function, args, kwargs = '
                 'self._maybe_define_function(args, kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 2150, in _maybe_define_function\r\n'
                 '    graph_function = self._create_graph_function(args, kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 2041, in _create_graph_function\r\n'
                 '    capture_by_value=self._capture_by_value),\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", '
                 'line 915, in func_graph_from_py_func\r\n'
                 '    func_outputs = python_func(*func_args, **func_kwargs)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 358, in wrapped_fn\r\n'
                 '    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n'
                 '  File '
                 '"/Users/sardarmrinal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py", '
                 'line 262, in restored_function_body\r\n'
                 '    "\\n\\n".join(signature_descriptions)))\r\n'
                 'ValueError: Could not find matching function to call loaded from '
                 'the SavedModel. Got:\r\n'
                 '  Positional arguments (3 total):\r\n'
                 "    * [<tf.Tensor 'inputs:0' shape=(None, 3) dtype=int64>, "
                 "<tf.Tensor 'inputs_1:0' shape=(None, 3) dtype=int64>, <tf.Tensor "
                 "'inputs_2:0' shape=(None, 3) dtype=int64>]\r\n"
                 '    * False\r\n'
                 '    * None\r\n'
                 '  Keyword arguments: {}\r\n'
                 '\r\n'
                 'Expected these arguments to match one of the following 4 '
                 'option(s):\r\n'
                 '\r\n'
                 'Option 1:\r\n'
                 '  Positional arguments (3 total):\r\n'
                 '    * [TensorSpec(shape=(None, None), dtype=tf.int32, '
                 "name='input_word_ids'), TensorSpec(shape=(None, None), "
                 "dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), "
                 "dtype=tf.int32, name='input_type_ids')]\r\n"
                 '    * True\r\n'
                 '    * None\r\n'
                 '  Keyword arguments: {}\r\n'
                 '\r\n'
                 'Option 2:\r\n'
                 '  Positional arguments (3 total):\r\n'
                 '    * [TensorSpec(shape=(None, None), dtype=tf.int32, '
                 "name='input_word_ids'), TensorSpec(shape=(None, None), "
                 "dtype=tf.int32, name='input_mask'), TensorSpec(shape=(None, None), "
                 "dtype=tf.int32, name='input_type_ids')]\r\n"
                 '    * False\r\n'
                 '    * None\r\n'
                 '  Keyword arguments: {}\r\n'
                 '\r\n'
                 'Option 3:\r\n'
                 '  Positional arguments (3 total):\r\n'
                 '    * [TensorSpec(shape=(None, None), dtype=tf.int32, '
                 "name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, "
                 "name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, "
                 "name='inputs/2')]\r\n"
                 '    * True\r\n'
                 '    * None\r\n'
                 '  Keyword arguments: {}\r\n'
                 '\r\n'
                 'Option 4:\r\n'
                 '  Positional arguments (3 total):\r\n'
                 '    * [TensorSpec(shape=(None, None), dtype=tf.int32, '
                 "name='inputs/0'), TensorSpec(shape=(None, None), dtype=tf.int32, "
                 "name='inputs/1'), TensorSpec(shape=(None, None), dtype=tf.int32, "
                 "name='inputs/2')]\r\n"
                 '    * False\r\n'
                 '    * None\r\n'
                 '  Keyword arguments: {}\r\n'
                 '```\r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (use command below): 2.1\r\n'
                 '- Python version: 3.7\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 "model can't restore weights from h5 file\r\n"
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n'
                 'class Model(keras.Model):\r\n'
                 '    def __init__(self, inp1, inp2):\r\n'
                 '        super(Model, self).__init__()\r\n'
                 "        self.x1 = self.add_weight('w1',[inp1])\r\n"
                 "        self.x2 = self.add_weight('w2',[inp2])\r\n"
                 '    def call(self,x):\r\n'
                 '        return x\r\n'
                 "# load_weights method works when save format is 'tf\r\n"
                 'x = Model(100,200)\r\n'
                 "x.save_weights('temp.tmp',save_format='tf')\r\n"
                 'old = x.weights[0][0].numpy()\r\n'
                 'print(old)\r\n'
                 'x = Model(100,200)\r\n'
                 "x.load_weights('temp.tmp')\r\n"
                 'new = x.weights[0][0].numpy()\r\n'
                 'print(new)\r\n'
                 'print(old==new)\r\n'
                 '\r\n'
                 "# load_weights method does not work when save format is 'h5'\r\n"
                 'x = Model(100,200)\r\n'
                 "x.save_weights('temp.h5',save_format='h5')\r\n"
                 'old = x.weights[0][0].numpy()\r\n'
                 'print(old)\r\n'
                 'x = Model(100,200)\r\n'
                 "x.load_weights('temp.h5')\r\n"
                 'new = x.weights[0][0].numpy()\r\n'
                 'print(new)\r\n'
                 'print(old==new)\r\n'
                 '```\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code: Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '16.04\r\n'
                 '- TensorFlow installed from (source or binary): Conda\r\n'
                 '- TensorFlow version (use command below): 2.0.0 and 2.1.0\r\n'
                 '- Python version: 3.7.5\r\n'
                 '- CUDA/cuDNN version: 10.0.130 / 7.6.4\r\n'
                 '- GPU model and memory: GTX 980 Ti, GTX 2080 Ti\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'The performance was very low when using persistent mode '
                 'tf.GradientTape or create multi-GradientTape objects in one '
                 '```with``` block.\r\n'
                 'This phenomenon only happens when the model includes a LSTM '
                 'layer.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Here is a sample code to reproduce the problem.\r\n'
                 '```python\r\n'
                 'import timeit\r\n'
                 'import numpy as np\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'model0 = tf.keras.models.Sequential(\r\n'
                 '    tf.keras.layers.LSTM(128, input_shape=(300, 40))\r\n'
                 ')\r\n'
                 'model1 = tf.keras.models.Sequential(\r\n'
                 "    tf.keras.layers.Dense(1, activation='sigmoid', "
                 'input_shape=(128,))\r\n'
                 ')\r\n'
                 'loss_object = '
                 'tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n'
                 '\r\n'
                 '@tf.function\r\n'
                 'def train_step_0():\r\n'
                 '  with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\r\n'
                 '    f = model0(tf.random.normal([256, 300, 40]))\r\n'
                 '    y_p0 = model1(f)\r\n'
                 '    y_p1 = model1(tf.random.normal((256, 128)))\r\n'
                 '    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n'
                 '    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n'
                 '  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n'
                 '  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n'
                 '\r\n'
                 'train_step_0()\r\n'
                 "print(timeit.timeit('train_step_0()', globals=globals(), "
                 'number=100))\r\n'
                 '```\r\n'
                 'In this case, the GPU-Util was only about 30 % and the time was 37 '
                 'seconds.\r\n'
                 'The following message was shown in screen.\r\n'
                 '```\r\n'
                 '2020-01-16 17:16:22.519978: E '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] '
                 'function_optimizer failed: Invalid argument: Input 0 of node '
                 'sequential/lstm/zeros_like was passed int32 from '
                 'sequential/lstm/StatefulPartitionedCall:9 incompatible with '
                 'expected variant.\r\n'
                 '2020-01-16 17:16:22.667364: E '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] '
                 'function_optimizer failed: Invalid argument: Input 0 of node '
                 'sequential/lstm/zeros_like was passed int32 from '
                 'sequential/lstm/StatefulPartitionedCall:9 incompatible with '
                 'expected variant.\r\n'
                 '2020-01-16 17:16:22.739393: W '
                 'tensorflow/core/common_runtime/process_function_library_runtime.cc:675] '
                 'Ignoring multi-device function optimization failure: Invalid '
                 'argument: Input 0 of node sequential/lstm/zeros_like was passed '
                 'int32 from sequential/lstm/StatefulPartitionedCall:9 incompatible '
                 'with expected variant.\r\n'
                 '```\r\n'
                 '\r\n'
                 '```python\r\n'
                 '@tf.function\r\n'
                 'def train_step_1():\r\n'
                 '  with tf.GradientTape(persistent=True) as tape:\r\n'
                 '    f = model0(tf.random.normal([256, 300, 40]))\r\n'
                 '    y_p0 = model1(f)\r\n'
                 '    y_p1 = model1(tf.random.normal((256, 128)))\r\n'
                 '    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n'
                 '    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n'
                 '  grad0 = tape.gradient(loss0, model0.trainable_variables)\r\n'
                 '  grad1 = tape.gradient(loss1, model1.trainable_variables)\r\n'
                 '\r\n'
                 'train_step_1()\r\n'
                 "print(timeit.timeit('train_step_1()', globals=globals(), "
                 'number=100))\r\n'
                 '```\r\n'
                 'In this case, the performance was similar to the previous one.\r\n'
                 '\r\n'
                 '```python\r\n'
                 '@tf.function\r\n'
                 'def train_step_2():\r\n'
                 '  with tf.GradientTape() as tape0:\r\n'
                 '    f = model0(tf.random.normal([256, 300, 40]))\r\n'
                 '    y_p0 = model1(f)\r\n'
                 '    loss0 = loss_object(tf.zeros_like(y_p0), y_p0)\r\n'
                 '  with tf.GradientTape() as tape1:\r\n'
                 '    y_p1 = model1(tf.random.normal((256, 128)))\r\n'
                 '    loss1 = loss_object(tf.ones_like(y_p1), y_p1)\r\n'
                 '  grad0 = tape0.gradient(loss0, model0.trainable_variables)\r\n'
                 '  grad1 = tape1.gradient(loss1, model1.trainable_variables)\r\n'
                 '\r\n'
                 'train_step_2()\r\n'
                 "print(timeit.timeit('train_step_2()', globals=globals(), "
                 'number=100))\r\n'
                 '```\r\n'
                 'In this case, the GPU-Util was almost 100 % and the time was only 4 '
                 'seconds.\r\n'
                 '\r\n'
                 'In my opinion, ```train_step_0```, ```train_step_1``` and '
                 '```train_step_2``` should have similar performance.\r\n'
                 'I am wandering why the GPU-Util and process time were so that '
                 'different.\r\n'
                 'The strangest thing is that this phenomenon only happens when the '
                 'model includes a LSTM layer.\r\n'
                 'If we exchange the LSTM layer to Conv or Dense layer, the process '
                 'time will be all same.\r\n'
                 'Here is a colaboratory page to reproduce this problem.\r\n'
                 'https://colab.research.google.com/drive/1sluVFuW1yYtH0Ye4reoOEmLUHYHDSGn7\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 'Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu, Newest\r\n'
                 '- TensorFlow installed from (source or binary): **Pip (Binary)**\r\n'
                 '- TensorFlow version (use command below): **2.1**\r\n'
                 '- Python version: Python **3.7.6**\r\n'
                 '- CUDA/cuDNN version: **10.2**\r\n'
                 '- GPU model and memory: **GeForce 1070 8Gb**\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'Passing in a dataset to model.fit from a model generated with '
                 'tf.Keras layers results in **IndexError: tuple index out of '
                 'range.** Error both with custom TFRecord dataset and datasets '
                 'derived from tensorflow-datasets installed via pip. Looks like it '
                 'is in the standardize_input_data function but since it is an '
                 'instance of DatasetV2 it should not be hitting that if '
                 'statement...\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Keras models should accept tf DataSets.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```Python\r\n'
                 'from tensorflow.keras.layers import Dense, Embedding, Flatten, '
                 'Lambda, Subtract, Input, Concatenate, Average, Reshape, '
                 'GlobalAveragePooling1D, Dot, Dropout\r\n'
                 'from tensorflow.keras.models import Model, Sequential\r\n'
                 'from tensorflow.keras.utils import Sequence\r\n'
                 'from tensorflow.keras import initializers\r\n'
                 '\r\n'
                 'import tensorflow_datasets as tfds\r\n'
                 'tfds.list_builders()\r\n'
                 'dataset, info = tfds.load("mnist", with_info=True)\r\n'
                 'inputs = Input((28, 28, 1), name="image")\r\n'
                 'First = Dense(128, activation="relu")\r\n'
                 'Second = Dropout(0.2)\r\n'
                 'Third = Dense(10, activation="softmax", name="label")\r\n'
                 '\r\n'
                 'first = First(inputs)\r\n'
                 'second = Second(first)\r\n'
                 'third = Third(second)\r\n'
                 'model = Model(inputs=[inputs], outputs=[third])\r\n'
                 "model.compile(optimizer='adam',\r\n"
                 "              loss='sparse_categorical_crossentropy',\r\n"
                 "              metrics=['accuracy'])\r\n"
                 "model.fit(dataset['train'].batch(4096))\r\n"
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '```\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py '
                 'in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, '
                 'validation_split, validation_data, shuffle, class_weight, '
                 'sample_weight, initial_epoch, steps_per_epoch, validation_steps, '
                 'validation_freq, max_queue_size, workers, use_multiprocessing,\r\n'
                 '**kwargs)\r\n'
                 '    340                 mode=ModeKeys.TRAIN,\r\n'
                 '    341                 training_context=training_context,\r\n'
                 '--> 342                 total_epochs=epochs)\r\n'
                 '    343             cbks.make_logs(model, epoch_logs, '
                 'training_result, ModeKeys.TRAIN)\r\n'
                 '    344 \r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py '
                 'in run_one_epoch(model, iterator, execution_function, dataset_size, '
                 'batch_size, strategy, steps_per_epoch, num_samples, mode, '
                 'training_context, total_epochs)\r\n'
                 '    126         step=step, mode=mode, size=current_batch_size) as '
                 'batch_logs:\r\n'
                 '    127       try:\r\n'
                 '--> 128         batch_outs = execution_function(iterator)\r\n'
                 '    129       except (StopIteration, errors.OutOfRangeError):\r\n'
                 '    130         # TODO(kaftan): File bug about tf function and '
                 'errors.OutOfRangeError?\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py '
                 'in execution_function(input_fn)\r\n'
                 '     96     # `numpy` translates Tensors to values in Eager '
                 'mode.\r\n'
                 '     97     return nest.map_structure(_non_none_constant_value,\r\n'
                 '---> 98                               '
                 'distributed_function(input_fn))\r\n'
                 '     99 \r\n'
                 '    100   return execution_function\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py '
                 'in __call__(self, *args, **kwds)\r\n'
                 '    566         xla_context.Exit()\r\n'
                 '    567     else:\r\n'
                 '--> 568       result = self._call(*args, **kwds)\r\n'
                 '    569 \r\n'
                 '    570     if tracing_count == self._get_tracing_count():\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py '
                 'in _call(self, *args, **kwds)\r\n'
                 '    613       # This is the first call of __call__, so we have to '
                 'initialize.\r\n'
                 '    614       initializers = []\r\n'
                 '--> 615       self._initialize(args, kwds, '
                 'add_initializers_to=initializers)\r\n'
                 '    616     finally:\r\n'
                 '    617       # At this point we know that the initialization is '
                 'complete (or less\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py '
                 'in _initialize(self, args, kwds, add_initializers_to)\r\n'
                 '    495     self._concrete_stateful_fn = (\r\n'
                 '    496         '
                 'self._stateful_fn._get_concrete_function_internal_garbage_collected( \r\n'
                 '# pylint: disable=protected-access\r\n'
                 '--> 497             *args, **kwds))\r\n'
                 '    498 \r\n'
                 '    499     def invalid_creator_scope(*unused_args, '
                 '**unused_kwds):\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py '
                 'in _get_concrete_function_internal_garbage_collected(self, '
                 '*args,\r\n'
                 '**kwargs)    2387       args, kwargs = None, None    2388     with '
                 'self._lock:\r\n'
                 '-> 2389       graph_function, _, _ = '
                 'self._maybe_define_function(args, kwargs)    2390     return '
                 'graph_function    2391 \r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py '
                 'in _maybe_define_function(self, args, kwargs)    2701     '
                 '2702       self._function_cache.missed.add(call_context_key)\r\n'
                 '-> 2703       graph_function = self._create_graph_function(args, '
                 'kwargs)    2704       self._function_cache.primary[cache_key] = '
                 'graph_function    2705       return graph_function, args, kwargs\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py '
                 'in _create_graph_function(self, args, kwargs, '
                 'override_flat_arg_shapes)    2591             '
                 'arg_names=arg_names,    2592             '
                 'override_flat_arg_shapes=override_flat_arg_shapes,\r\n'
                 '-> 2593             capture_by_value=self._capture_by_value),    '
                 '2594         self._function_attributes,    2595         # Tell the '
                 'ConcreteFunction to clean up its graph once it goes out of\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py '
                 'in func_graph_from_py_func(name, python_func, args, kwargs, '
                 'signature, func_graph, autograph, autograph_options, '
                 'add_control_dependencies, arg_names, op_return_value, collections, '
                 'capture_by_value, override_flat_arg_shapes)\r\n'
                 '    976                                           '
                 'converted_func)\r\n'
                 '    977 \r\n'
                 '--> 978       func_outputs = python_func(*func_args, '
                 '**func_kwargs)\r\n'
                 '    979 \r\n'
                 '    980       # invariant: `func_outputs` contains only Tensors, '
                 'CompositeTensors,\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py '
                 'in wrapped_fn(*args, **kwds)\r\n'
                 '    437         # __wrapped__ allows AutoGraph to swap in a '
                 'converted function. We give\r\n'
                 '    438         # the function a weak reference to itself to avoid '
                 'a reference cycle.\r\n'
                 '--> 439         return weak_wrapped_fn().__wrapped__(*args, '
                 '**kwds)\r\n'
                 '    440     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n'
                 '    441 \r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py '
                 'in distributed_function(input_iterator)\r\n'
                 '     83     args = _prepare_feed_values(model, input_iterator, '
                 'mode, strategy)\r\n'
                 '     84     outputs = strategy.experimental_run_v2(\r\n'
                 '---> 85         per_replica_function, args=args)\r\n'
                 '     86     # Out of PerReplica outputs reduce or pick values to '
                 'return.\r\n'
                 '     87     all_outputs = dist_utils.unwrap_output_dict(\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py '
                 'in experimental_run_v2(self, fn, args, kwargs)\r\n'
                 '    761       fn = autograph.tf_convert(fn, '
                 'ag_ctx.control_status_ctx(),\r\n'
                 '    762                                 '
                 'convert_by_default=False)\r\n'
                 '--> 763       return self._extended.call_for_each_replica(fn, '
                 'args=args, kwargs=kwargs)\r\n'
                 '    764 \r\n'
                 '    765   def reduce(self, reduce_op, value, axis):\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py '
                 'in call_for_each_replica(self, fn, args, kwargs)    1817       '
                 'kwargs\r\n'
                 '= {}    1818     with self._container_strategy().scope():\r\n'
                 '-> 1819       return self._call_for_each_replica(fn, args, '
                 'kwargs)    1820     1821   def _call_for_each_replica(self, fn, '
                 'args, kwargs):\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py '
                 'in _call_for_each_replica(self, fn, args, kwargs)    2162         '
                 'self._container_strategy(),    2163         '
                 'replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\r\n'
                 '-> 2164       return fn(*args, **kwargs)    2165     2166   def '
                 '_reduce_to(self, reduce_op, value, destinations):\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py '
                 'in wrapper(*args, **kwargs)\r\n'
                 '    290   def wrapper(*args, **kwargs):\r\n'
                 '    291     with '
                 'ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n'
                 '--> 292       return func(*args, **kwargs)\r\n'
                 '    293 \r\n'
                 '    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py '
                 'in train_on_batch(model, x, y, sample_weight, class_weight, '
                 'reset_metrics, standalone)\r\n'
                 '    414   x, y, sample_weights = model._standardize_user_data(\r\n'
                 '    415       x, y, sample_weight=sample_weight, '
                 'class_weight=class_weight,\r\n'
                 '--> 416       extract_tensors_from_dataset=True)\r\n'
                 '    417   batch_size = array_ops.shape(nest.flatten(x, '
                 'expand_composites=True)[0])[0]\r\n'
                 '    418   # If `model._distribution_strategy` is True, then we are '
                 'in a replica context\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py '
                 'in _standardize_user_data(self, x, y, sample_weight, class_weight, '
                 'batch_size, check_steps, steps_name, steps, validation_split, '
                 'shuffle, extract_tensors_from_dataset)    2381         '
                 'is_dataset=is_dataset,   2382         class_weight=class_weight,\r\n'
                 '-> 2383         batch_size=batch_size)    2384     2385   def '
                 '_standardize_tensors(self, x, y, sample_weight, run_eagerly, '
                 'dict_inputs,\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py '
                 'in _standardize_tensors(self, x, y, sample_weight, run_eagerly, '
                 'dict_inputs, is_dataset, class_weight, batch_size)    '
                 '2467           shapes=None,    2468           '
                 "check_batch_axis=False,  # Don't enforce the batch size.\r\n"
                 "-> 2469           exception_prefix='target')    2470     2471       "
                 '# Generate sample-wise weight values given the `sample_weight` '
                 'and\r\n'
                 '\r\n'
                 '~/miniconda3/envs/keras/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py '
                 'in standardize_input_data(data, names, shapes, check_batch_axis, '
                 'exception_prefix)\r\n'
                 "    510                        'for each key in: ' + str(names))\r\n"
                 '    511   elif isinstance(data, (list, tuple)):\r\n'
                 '--> 512     if isinstance(data[0], (list, tuple)):\r\n'
                 '    513       data = [np.asarray(d) for d in data]\r\n'
                 '    514     elif len(names) == 1 and isinstance(data[0], (float, '
                 'int)):\r\n'
                 '\r\n'
                 'IndexError: tuple index out of range\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': 'This PR implements GPU kernels ApplyAdagrad, ApplyAdagradV2, '
                 'ApplyAdadelta, ApplyRMSProp, and ApplyCenteredRMSProp for ROCm, and '
                 'enables corresponding unit tests.',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04\r\n'
                 '- TensorFlow installed from (source or binary): from pip '
                 '(binary?)\r\n'
                 '- TensorFlow version (use command below): 2.1\r\n'
                 '- Python version: 3.7\r\n'
                 '- CUDA/cuDNN version: no\r\n'
                 '- GPU model and memory: no\r\n'
                 '\r\n'
                 '**Describe the current behavior and code to reproduce**\r\n'
                 '\r\n'
                 'I am running this code:\r\n'
                 '\r\n'
                 'https://gist.github.com/cossio/dc761b2731b1428267b65333dbd3f321\r\n'
                 '\r\n'
                 'to train an RBM by contrastive divergence. Briefly I am using the '
                 'Keras sub-classing API and I view the multiple Monte Carlo samples '
                 'as layers.\r\n'
                 '\r\n'
                 'However it produces the following error:\r\n'
                 '\r\n'
                 '```\r\n'
                 "TypeError: object of type 'NoneType' has no len()\r\n"
                 '```\r\n'
                 '\r\n'
                 'Please find the full error message and stack-trace as a comment on '
                 'the above gist link.',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): No\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: No\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version (use command below): 2.1.0\r\n'
                 '- Python version: 3.6.8\r\n'
                 '- Bazel version (if compiling from source): 0.29.1\r\n'
                 '- GCC/Compiler version (if compiling from source): GCC 7.4.0\r\n'
                 '- CUDA/cuDNN version: 10.1,10.2 CuDNN 7\r\n'
                 '- GPU model and memory: NVidia GTX 1080 Ti\r\n'
                 '- TensorRT: version 6 or 7.\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'Compiling from source, as indicated in the official tensorflow '
                 'manual, compilation fails, with the error listed in the log '
                 'below.\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Compilation should successfully complete\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'No code needed. After a checkout of the current v.2.1.0, '
                 'configuration is carried out with no changes from default (other '
                 'than enabling CUDA). \r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Log: see attached log.txt\r\n'
                 '[log.txt](https://github.com/tensorflow/tensorflow/files/4068010/log.txt)\r\n'
                 '\r\n'
                 '                                                                                                                                                   \r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>I trained a model in Tensorflow and used 36 convolution layers, '
                 'each holding a batch normalization layer. What I do not understand '
                 'is why the numbering of batch normalization layer is global instead '
                 'of starting from 0 for every name scope? \r\n'
                 'I am not going to create the layers with the same name_scope in the '
                 'same order in every model, sometimes I will insert a few layers '
                 'before a particular layer and would like to restore the batch norm '
                 "parameters from other model checkpoint, but it's not possible "
                 'because batch norm layer is numbered globally, and because I '
                 'created this layer at a step numerically bigger than the step at '
                 'which I created the same layer in other model, the names will be '
                 "different and I won't be able to restore parameters in the "
                 'traditional way. </em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '-\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Mint\r\n'
                 '- TensorFlow installed from (source or binary): tf-gpu anaconda \r\n'
                 '- TensorFlow version : tf.2.0.0\r\n'
                 '- Python version: 3.7.4\r\n'
                 '\r\n'
                 '\r\n'
                 'Here is the list of batch norm parameters in my original model, '
                 'mind the layer: batch_normalization_18 , this name is '
                 'batch_normalization_18 because this is the 19th (18 is in numbering '
                 'because index starts from 0) batchnorm layer created.\r\n'
                 '\r\n'
                 "['Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block1_conv1/batch_normalization/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block1_conv2/batch_normalization_1/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv1/batch_normalization_2/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block2_conv2/batch_normalization_3/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv1/batch_normalization_4/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv2/batch_normalization_5/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv3/batch_normalization_6/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block3_conv4/batch_normalization_7/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv1/batch_normalization_8/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv2/batch_normalization_9/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv3/batch_normalization_10/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block4_conv4/batch_normalization_11/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv1/batch_normalization_12/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv2/batch_normalization_13/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv3/batch_normalization_14/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block5_conv4/batch_normalization_15/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv1/batch_normalization_16/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Block6_conv2/batch_normalization_17/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_18/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_19/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv1/batch_normalization_20/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv2/batch_normalization_21/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock5_conv3/batch_normalization_22/moving_variance:0']\r\n"
                 '\r\n'
                 '\r\n'
                 'Now, I created another model, where I am creating the same layers, '
                 'but I am inserting two layers before this 19th layer, and hence, '
                 'this will be given a different name just because of this '
                 'reason. \r\n'
                 '\r\n'
                 ' '
                 "['Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv1/batch_normalization_20/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/Dlock6_conv2/batch_normalization_21/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv1_first/batch_normalization_18/moving_variance:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/beta:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/gamma:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/moving_mean:0',\r\n"
                 ' '
                 "'Train/vgg_deconv_RGB_LSTM/extra_conv2_second/batch_normalization_19/moving_variance:0']\r\n"
                 '\r\n'
                 '\r\n'
                 'You can see, now batch_normalization_18 name is given to another '
                 'layer, and my original layer, which was given '
                 'batch_normalization_18 in previous model, it is given the name '
                 'of \r\n'
                 'batch_normalization_20, even though all the variable scope and name '
                 'scope is same. \r\n'
                 '\r\n'
                 "Should'nt it be like local numbering for every name_scope? What's "
                 'the point behind global numbering? \r\n'
                 '\r\n'
                 'Please let me know if any more information is needed. \r\n',
         'created_at': '2020-01-1'},
        {'body': 'Minor fixes for StructuredTensor pydocs that had incorrect example '
                 'code and Python 2 style print statements.',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): docker '
                 'image `nvcr.io/nvidia/tensorflow:19.09-py3`\r\n'
                 '- ~Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:~\r\n'
                 '- TensorFlow installed from (source or binary): see docker image\r\n'
                 '- TensorFlow version (use command below): 1.14.0+nv\r\n'
                 '- Python version: 3.6.8\r\n'
                 '- ~Bazel version (if compiling from source):~\r\n'
                 '- ~GCC/Compiler version (if compiling from source):~\r\n'
                 '- CUDA/cuDNN version: 10.1\r\n'
                 '- GPU model and memory: **Tesla V100-PCIE-16GB**\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'with the following code added in `model_fn`, program crashes with '
                 '`CUDA_ERROR_ILLEGAL_ADDRESS`:\r\n'
                 '```python\r\n'
                 '           def create_mean_metrics(t_logits, t_label, v_label):\r\n'
                 '                t_filtered = tf.boolean_mask(t_logits, '
                 'tf.equal(t_map_label, v_label))\r\n'
                 '                return '
                 'tf.metrics.mean_tensor(tf.reduce_mean(t_filtered), '
                 'tf.size(t_filtered))\r\n'
                 '\r\n'
                 '            eval_metrics = {\r\n'
                 "                'mean_perfect': create_mean_metrics(t_logits, "
                 't_map_label, 0),\r\n'
                 "                'mean_excellent': create_mean_metrics(t_logits, "
                 't_map_label, 1),\r\n'
                 "                'mean_good': create_mean_metrics(t_logits, "
                 't_map_label, 2),\r\n'
                 "                'mean_fair': create_mean_metrics(t_logits, "
                 't_map_label, 3),\r\n'
                 "                'mean_bad': create_mean_metrics(t_logits, "
                 't_map_label, 5),\r\n'
                 '            }\r\n'
                 '\r\n'
                 '            return tf.estimator.EstimatorSpec(\r\n'
                 '                mode=mode,\r\n'
                 '                loss=t_eval_loss,\r\n'
                 '                eval_metric_ops=eval_metrics,\r\n'
                 '            )\r\n'
                 '```\r\n'
                 'if the `eval_metrics` part removed, the program will run without '
                 'crashes.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'program run without crashing.\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '```\r\n'
                 '2020-01-15 11:07:08.705510: I '
                 'tensorflow/stream_executor/cuda/ptxas_utils.cc:202] \r\n'
                 '2020-01-15 11:07:11.712068: E '
                 'tensorflow/stream_executor/cuda/cuda_driver.cc:1048] Internal: '
                 'could not synchronize on CUDA stream: CUDA_ERROR_ILLEGAL_ADDRESS: '
                 'an illegal memory access was encountered :: *** Begin stack trace '
                 '***\r\n'
                 '        tensorflow::CurrentStackTrace()\r\n'
                 '        '
                 'stream_executor::gpu::GpuDriver::SynchronizeStream(stream_executor::gpu::GpuContext*, '
                 'CUstream_st*)\r\n'
                 '        '
                 'stream_executor::gpu::GpuExecutor::BlockHostUntilDone(stream_executor::Stream*)\r\n'
                 '        '
                 'stream_executor::StreamExecutor::BlockHostUntilDone(stream_executor::Stream*)\r\n'
                 '        stream_executor::Stream::BlockHostUntilDone()\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        '
                 'tensorflow::XlaRunOp::Compute(tensorflow::OpKernelContext*)\r\n'
                 '        '
                 'tensorflow::BaseGPUDevice::ComputeHelper(tensorflow::OpKernel*, '
                 'tensorflow::OpKernelContext*)\r\n'
                 '        tensorflow::BaseGPUDevice::Compute(tensorflow::OpKernel*, '
                 'tensorflow::OpKernelContext*)\r\n'
                 '\r\n'
                 '\r\n'
                 '        '
                 'Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int)\r\n'
                 '        std::_Function_handler<void (), '
                 'tensorflow::thread::EigenEnvironment::CreateThread(std::function<void '
                 '()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\r\n'
                 '\r\n'
                 '\r\n'
                 '        clone\r\n'
                 '*** End stack trace ***\r\n'
                 '\r\n'
                 '2020-01-15 11:07:11.765522: E '
                 'tensorflow/stream_executor/cuda/cuda_driver.cc:1032] could not '
                 'synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal '
                 'memory access was encountered :: *** Begin stack trace ***\r\n'
                 '        tensorflow::CurrentStackTrace()\r\n'
                 '        '
                 'stream_executor::gpu::GpuDriver::SynchronizeContext(stream_executor::gpu::GpuContext*)\r\n'
                 '        '
                 'stream_executor::StreamExecutor::SynchronizeAllActivity()\r\n'
                 '        tensorflow::XlaCompilationCache::~XlaCompilationCache()\r\n'
                 '        tensorflow::XlaCompilationCache::~XlaCompilationCache()\r\n'
                 '        tensorflow::ResourceMgr::Clear()\r\n'
                 '        tensorflow::DirectSession::~DirectSession()\r\n'
                 '        tensorflow::DirectSession::~DirectSession()\r\n'
                 '        tensorflow::SessionRef::Close()\r\n'
                 '        TF_CloseSession\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '        _PyFunction_FastCallDict\r\n'
                 '\r\n'
                 '        _PyObject_FastCallDict\r\n'
                 '        PyObject_CallFunctionObjArgs\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '        _PyFunction_FastCallDict\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyObject_FastCallKeywords\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '        _PyFunction_FastCallDict\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyObject_FastCallKeywords\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '\r\n'
                 '        _PyEval_EvalFrameDefault\r\n'
                 '\r\n'
                 '        PyEval_EvalCode\r\n'
                 '\r\n'
                 '        PyRun_FileExFlags\r\n'
                 '        PyRun_SimpleFileExFlags\r\n'
                 '        Py_Main\r\n'
                 '        main\r\n'
                 '        __libc_start_main\r\n'
                 '        _start\r\n'
                 '*** End stack trace ***\r\n'
                 '\r\n'
                 '2020-01-15 11:07:11.765871: E '
                 'tensorflow/compiler/jit/xla_compilation_cache.cc:53] Error '
                 'synchronizing activity while waiting for all programs to '
                 'complete\r\n'
                 '2020-01-15 11:07:11.768797: E '
                 'tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to '
                 'unload module 0x7f66709c3ec0; leaking: CUDA_ERROR_ILLEGAL_ADDRESS: '
                 'an illegal memory access was encountered\r\n'
                 '2020-01-15 11:07:11.776140: E '
                 'tensorflow/stream_executor/cuda/cuda_driver.cc:763] failed to '
                 'unload module 0x7f66748a71f0; leaking: CUDA_ERROR_ILLEGAL_ADDRESS: '
                 'an illegal memory access was encountered\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", '
                 'line 1356, in _do_call\r\n'
                 '    return fn(*args)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", '
                 'line 1341, in _run_fn\r\n'
                 '    options, feed_dict, fetch_list, target_list, run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", '
                 'line 1429, in _call_tf_sessionrun\r\n'
                 '    run_metadata)\r\n'
                 'tensorflow.python.framework.errors_impl.InternalError: 2 root '
                 'error(s) found.\r\n'
                 '  (0) Internal: Failed to complete all kernels launched on stream '
                 '0x1f50d730: could not synchronize on CUDA stream: '
                 'CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was '
                 'encountered\r\n'
                 '         [[{{node cluster_0_1/xla_run}}]]\r\n'
                 '         [[Identity/_941]]\r\n'
                 '  (1) Internal: Failed to complete all kernels launched on stream '
                 '0x1f50d730: could not synchronize on CUDA stream: '
                 'CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was '
                 'encountered\r\n'
                 '         [[{{node cluster_0_1/xla_run}}]]\r\n'
                 '0 successful operations.\r\n'
                 '0 derived errors ignored.\r\n'
                 '\r\n'
                 'During handling of the above exception, another exception '
                 'occurred:\r\n'
                 '\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "hvd_combine_model_train_fidelity.py", line 684, in '
                 '<module>\r\n'
                 '    tf.app.run()\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py", '
                 'line 40, in run\r\n'
                 '    _run(main=main, argv=argv, '
                 'flags_parser=_parse_flags_tolerate_undef)\r\n'
                 '  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line '
                 '299, in run\r\n'
                 '    _run_main(main, args)\r\n'
                 '  File "/usr/local/lib/python3.6/dist-packages/absl/app.py", line '
                 '250, in _run_main\r\n'
                 '    sys.exit(main(argv))\r\n'
                 '  File "hvd_combine_model_train_fidelity.py", line 464, in main\r\n'
                 '    max_steps=num_train_steps, hooks=training_hooks)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", '
                 'line 367, in train\r\n'
                 '    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", '
                 'line 1158, in _train_model\r\n'
                 '    return self._train_model_default(input_fn, hooks, '
                 'saving_listeners)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", '
                 'line 1192, in _train_model_default\r\n'
                 '    saving_listeners)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", '
                 'line 1480, in _train_with_estimator_spec\r\n'
                 '    log_step_count_steps=log_step_count_steps) as mon_sess:\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 584, in MonitoredTrainingSession\r\n'
                 '    stop_grace_period_secs=stop_grace_period_secs)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1007, in __init__\r\n'
                 '    stop_grace_period_secs=stop_grace_period_secs)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 725, in __init__\r\n'
                 '    self._sess = _RecoverableSession(self._coordinated_creator)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1200, in __init__\r\n'
                 '    _WrappedSession.__init__(self, self._create_session())\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1205, in _create_session\r\n'
                 '    return self._sess_creator.create_session()\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 878, in create_session\r\n'
                 '    hook.after_create_session(self.tf_sess, self.coord)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/hooks/hooks.py", '
                 'line 180, in after_create_session\r\n'
                 '    self._evaluate(session)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/hooks/hooks.py", '
                 'line 203, in _evaluate\r\n'
                 '    output_dir=self._eval_dir)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py", '
                 'line 1609, in _evaluate_run\r\n'
                 '    config=self._session_config)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/evaluation.py", '
                 'line 272, in _evaluate_once\r\n'
                 '    session.run(eval_ops, feed_dict)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 754, in run\r\n'
                 '    run_metadata=run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1252, in run\r\n'
                 '    run_metadata=run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1353, in run\r\n'
                 '    raise six.reraise(*original_exc_info)\r\n'
                 '  File "/usr/local/lib/python3.6/dist-packages/six.py", line 693, '
                 'in reraise\r\n'
                 '    raise value\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1338, in run\r\n'
                 '    return self._sess.run(*args, **kwargs)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1411, in run\r\n'
                 '    run_metadata=run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1169, in run\r\n'
                 '    return self._sess.run(*args, **kwargs)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", '
                 'line 950, in run\r\n'
                 '    run_metadata_ptr)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", '
                 'line 1173, in _run\r\n'
                 '    feed_dict_tensor, options, run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", '
                 'line 1350, in _do_run\r\n'
                 '    run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py", '
                 'line 1370, in _do_call\r\n'
                 '    raise type(e)(node_def, op, message)\r\n'
                 'tensorflow.python.framework.errors_impl.InternalError: 2 root '
                 'error(s) found.\r\n'
                 '  (0) Internal: Failed to complete all kernels launched on stream '
                 '0x1f50d730: could not synchronize on CUDA stream: '
                 'CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was '
                 'encountered\r\n'
                 '         [[{{node cluster_0_1/xla_run}}]]\r\n'
                 '         [[Identity/_941]]\r\n'
                 '  (1) Internal: Failed to complete all kernels launched on stream '
                 '0x1f50d730: could not synchronize on CUDA stream: '
                 'CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was '
                 'encountered\r\n'
                 '         [[{{node cluster_0_1/xla_run}}]]\r\n'
                 '0 successful operations.\r\n'
                 '0 derived errors ignored.\r\n'
                 '```\r\n',
         'created_at': '2020-01-1'},
        {'body': 'The test //tensorflow/python/kernel_tests/signal:mel_ops_test fails '
                 'on ROCm in eager mode by slightly exceeding the error tolerance.\r\n'
                 'This patch replaces the calls to exp and log with more accurate '
                 'expm1 and log1p (which is something that occurs automatically in '
                 'graph mode but not in eager mode), bringing the error back below '
                 'threshold.',
         'created_at': '2020-01-1'},
        {'body': '@tensorflow/micro\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): '
                 '"Ubuntu"\r\n'
                 'VERSION="18.04.3 LTS (Bionic Beaver)"\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- Tensorflow version (commit SHA if source): current\r\n'
                 '- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm '
                 'Mbed OS - STM32F7\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'When running make for "generate_hello_world_mbed_project", a '
                 'failure is seen and it stops building. \r\n'
                 'Ref: '
                 '[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world](url)\r\n'
                 'Ex:\r\n'
                 '`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed '
                 'TAGS="CMSIS disco_f746ng" generate_hello_world_mbed_project`\r\n'
                 '\r\n'
                 '**Please provide the exact sequence of commands/steps when you ran '
                 'into the problem**\r\n'
                 '```\r\n'
                 '$ git clone --recursive '
                 'https://github.com/tensorflow/tensorflow.git\r\n'
                 '\r\n'
                 '$ cd tensorflow/\r\n'
                 '\r\n'
                 '$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed '
                 'TAGS="CMSIS disco_f746ng" generate_hello_world_mbed_project\r\n'
                 '```\r\n'
                 '\r\n'
                 'Error seen:\r\n'
                 '\r\n'
                 '```\r\n'
                 '$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed '
                 'TAGS="CMSIS disco_f746ng" generate_hello_world_mbed_project\r\n'
                 'tensorflow/lite/micro/tools/make/download_and_extract.sh '
                 '"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip" '
                 '"7e8191b24853d75de2af87622ad293ba" '
                 'tensorflow/lite/micro/tools/make/downloads/gemmlowp  \r\n'
                 'downloading '
                 'https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\n'
                 'tensorflow/lite/micro/tools/make/download_and_extract.sh '
                 '"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz" '
                 '"02c64880acb89dbd57eebacfd67200d8" '
                 'tensorflow/lite/micro/tools/make/downloads/flatbuffers  \r\n'
                 'downloading '
                 'https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\n'
                 'tensorflow/lite/micro/tools/make/download_and_extract.sh '
                 '"https://github.com/ARM-software/CMSIS_5/archive/d76d5e3acb87cf089daf50b31f991026149ecb6c.zip" '
                 '"866f79cfb86f7aee29a320aeda530aca" '
                 'tensorflow/lite/micro/tools/make/downloads/cmsis  \r\n'
                 'downloading '
                 'https://github.com/ARM-software/CMSIS_5/archive/d76d5e3acb87cf089daf50b31f991026149ecb6c.zip\r\n'
                 'tensorflow/lite/micro/tools/make/download_and_extract.sh '
                 '"https://developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2" '
                 '"299ebd3f1c2c90930d28ab82e5d8d6c0" '
                 'tensorflow/lite/micro/tools/make/downloads/gcc_embedded  \r\n'
                 'downloading '
                 'https://developer.arm.com/-/media/Files/downloads/gnu-rm/7-2018q2/gcc-arm-none-eabi-7-2018-q2-update-linux.tar.bz2\r\n'
                 'tensorflow/lite/micro/tools/make/download_and_extract.sh '
                 '"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip" '
                 '"8a7d2c70325f53136faea6dde517b8cc" '
                 'tensorflow/lite/micro/tools/make/downloads/person_model_int8  \r\n'
                 'downloading '
                 'https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip\r\n'
                 'tensorflow/lite/micro/tools/make/download_and_extract.sh '
                 '"https://github.com/mborgerding/kissfft/archive/v130.zip" '
                 '"438ba1fef5783cc5f5f201395cc477ca" '
                 'tensorflow/lite/micro/tools/make/downloads/kissfft '
                 'patch_kissfft \r\n'
                 'downloading '
                 'https://github.com/mborgerding/kissfft/archive/v130.zip\r\n'
                 'Finished patching kissfft\r\n'
                 'tensorflow/lite/micro/tools/make/download_and_extract.sh '
                 '"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip" '
                 '"fe2934bd0788f1dcc7af3f0a954542ab" '
                 'tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  \r\n'
                 'downloading '
                 'https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip\r\n'
                 'make: *** No rule to make target '
                 "'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/tensorflow/lite/micro/tools/make/downloads/CMSIS_ext/arm_cmplx_mag_squared_q10p6.c', "
                 "needed by 'generate_hello_world_mbed_project'.  Stop.\r\n"
                 '\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu '
                 '16.04):Windows\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary):binary\r\n'
                 '- TensorFlow version (use command below):tf-nightly\r\n'
                 '- Python version:3.7.5\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 'You can collect some of this information using our environment '
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n'
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'File '
                 '"\\\\?\\C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\Bazel.runfiles_e847arzc\\runfiles\\__main__\\tensorflow_addons\\activations\\rrelu_test.py", '
                 'line 88, in benchmarkRreluOp\r\n'
                 '    self.run_op_benchmark(sess, result.op, min_iters=25)\r\n'
                 '  File '
                 '"C:\\hostedtoolcache\\windows\\Python\\3.7.5\\x64\\lib\\site-packages\\tensorflow_core\\python\\platform\\benchmark.py", '
                 'line 389, in run_op_benchmark\r\n'
                 '    "throughput": mbs / median_delta\r\n'
                 'ZeroDivisionError: float division by zero\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Same behavior as ubuntu\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 '```python \r\n'
                 'self.run_op_benchmark(sess, result.op, min_iters=25)\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/addons/issues/839\r\n'
                 'It seems '
                 '[```time.time()```](https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/platform/benchmark.py#L294) '
                 'cause the problem and ```timeit.default_timer()``` could fix the '
                 'problem',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yee\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 19.10\r\n'
                 '- TensorFlow installed from (source or binary): PyPI wheel\r\n'
                 '- TensorFlow version: v2.1.0-rc2-17-ge5bf8de\r\n'
                 '- Python version: 3.7.5\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'The `predict` method of a Keras model with a sigmoid activiation '
                 'function for the output returns probabilities.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 '`predict` should return class indices or class labels, as in the '
                 'case of softmax activation. The current behavior also seems to be '
                 "redundant with the method `predict_proba`, which in turn doesn't "
                 'seem to be documented, at least on '
                 'https://www.tensorflow.org/api_docs/python/tf/keras/Model\r\n'
                 '\r\n'
                 '#7287 is perhaps related.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 '```python\r\n'
                 'from tensorflow.keras.models import Sequential\r\n'
                 'from tensorflow.keras.layers import Dense, Activation\r\n'
                 '\r\n'
                 'model = Sequential([\r\n'
                 '    Dense(10, activation = "relu"),\r\n'
                 '    Dense(1, activation = "sigmoid")])\r\n'
                 'model.compile(\r\n'
                 '    optimizer = "rmsprop",\r\n'
                 '    loss = "binary_crossentropy")\r\n'
                 'model.fit(\r\n'
                 '    [[1, 2], [1, 3], [1, 1], [2, 2], [2, 3]],\r\n'
                 '    [True, False, False, True, True])\r\n'
                 '\r\n'
                 'print(model.predict([[1, 2], [1, 3], [1, 1]]))\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': 'tcmalloc: large alloc 6645350400 bytes == 0x695ff4000 @ '
                 '0x7f6d5f840b6b 0x7f6d5f860379 0x7f6d2b49ec27 0x7f6d2b291a7f '
                 '0x7f6d2b15d3cb 0x7f6d2b123526 0x7f6d2b1243b3 0x7f6d2b124583 '
                 '0x7f6d2f9ad7b5 0x7f6d3012d949 0x7f6d3012e3fe 0x7f6d3010ab82 '
                 '0x7f6d3010b5bf 0x7f6d3010f44c 0x7f6d301110aa 0x7f6d2df5db25 '
                 '0x7f6d2def4e1b 0x50a8af 0x50c5b9 0x508245 0x50a080 0x50aa7d '
                 '0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 '
                 '0x5a067e 0x50d966\r\n'
                 '\r\n'
                 'This perhaps happens only I load High-quality images....and ofc it '
                 'needs more RAM but not more than the RAM offered by colab.Any '
                 'solutions?\r\n'
                 '\r\n'
                 'WARNING: tensorflow/core/framework/cpu_allocator_impl.cc:81] '
                 'Allocation of 6645350400 exceeds 10% of system memory.\r\n'
                 '\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 'Centos 7 (Docker image)\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 'Source\r\n'
                 '- TensorFlow version:\r\n'
                 'r2.1\r\n'
                 '- Python version:\r\n'
                 '3.6.8\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '0.29.1 (installed using bazelisk)\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '7.3.1 (installed via devtoolset-7)\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'When building Tensorflow 2.1, the following linking error '
                 'appears:\r\n'
                 '\r\n'
                 '```\r\n'
                 'ERROR: /tmp/bazel/external/swig/BUILD.bazel:5:1: Linking of '
                 'rule \r\n'
                 "'@swig//:swig' failed (Exit 1)\r\n"
                 'bazel-out/host/bin/external/swig/_objs/swig/allocate.o:allocate.cxx:\r\n'
                 'function Allocate::~Allocate(): \r\n'
                 "error: undefined reference to 'operator delete(void*, unsigned "
                 "long)'\r\n"
                 '```\r\n'
                 '\r\n'
                 'Seems like an incompatibility issue when building some third party '
                 'components caused when looking for that symbol.\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '\r\n'
                 '`$ docker run centos:7`\r\n'
                 '\r\n'
                 'Then inside the container:\r\n'
                 '```\r\n'
                 '# yum-config-manager --enable rhel-server-rhscl-7-rpms -y && yum '
                 'install devtoolset-7 -y\r\n'
                 '# export PATH=/opt/rh/devtoolset-7/root/usr/bin${PATH:+:${PATH}}\r\n'
                 '# export PCP_DIR=/opt/rh/devtoolset-7/root\r\n'
                 '# export '
                 'PERL5LIB=/opt/rh/devtoolset-7/root//usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/lib/perl5:/opt/rh/devtoolset-7/root//usr/share/perl5/vendor_perl${PERL5LIB:+:${PERL5LIB}}\r\n'
                 '# rpmlibdir=$(rpm --eval "%{_libdir}")\r\n'
                 '# if [ "$rpmlibdir" != "${rpmlibdir/lib64/}" ]; then\r\n'
                 '#       '
                 'rpmlibdir32=":/opt/rh/devtoolset-7/root${rpmlibdir/lib64/lib}"\r\n'
                 '# fi\r\n'
                 '# export '
                 'LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root$rpmlibdir$rpmlibdir32${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n'
                 '# export '
                 'LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root$rpmlibdir$rpmlibdir32:/opt/rh/devtoolset-7/root$rpmlibdir/dyninst$rpmlibdir32/dyninst${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n'
                 '# pythonvers=2.7\r\n'
                 '# export '
                 'PYTHONPATH=/opt/rh/devtoolset-7/root/usr/lib64/python$pythonvers/site-packages:/opt/rh/devtoolset-7/root/usr/lib/python$pythonvers/site-packages${PYTHONPATH:+:${PYTHONPATH}}\r\n'
                 '# export '
                 'JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b10-1.el7_7.x86_64\r\n'
                 '# git clone https://github.com/tensorflow/ && cd tensorflow && git '
                 'checkout r2.1\r\n'
                 '# export OPTM=3\r\n'
                 '# export PYTHON_BIN_PATH=/usr/bin/python\r\n'
                 '# export USE_DEFAULT_PYTHON_LIB_PATH=1\r\n'
                 '# export CC_OPT_FLAGS="-march=${ARCH} -mtune=$TUNE"\r\n'
                 '# export TF_NEED_JEMALLOC=1\r\n'
                 '# export TF_NEED_KAFKA=0\r\n'
                 '# export TF_NEED_OPENCL_SYCL=0\r\n'
                 '# export TF_NEED_GCP=0\r\n'
                 '# export TF_NEED_HDFS=0\r\n'
                 '# export TF_NEED_S3=0\r\n'
                 '# export TF_ENABLE_XLA=1\r\n'
                 '# export TF_NEED_GDR=0\r\n'
                 '# export TF_NEED_VERBS=0\r\n'
                 '# export TF_NEED_OPENCL=0\r\n'
                 '# export TF_NEED_MPI=0\r\n'
                 '# export TF_NEED_TENSORRT=0\r\n'
                 '# export TF_SET_ANDROID_WORKSPACE=0\r\n'
                 '# export TF_DOWNLOAD_CLANG=0\r\n'
                 '# export TF_NEED_CUDA=0\r\n'
                 '# ./configure && bazel build --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 '
                 '//tensorflow/tools/pip_package:build_pip_package\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Note this error message appears when building different components. '
                 'Here are some examples:\r\n'
                 '\r\n'
                 '```\r\n'
                 'ERROR: /tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:713:1: '
                 'Linking of rule '
                 "'//tensorflow/compiler/mlir/tensorflow:derived_attr_populator_gen' "
                 'failed (Exit 1)\r\n'
                 'bazel-out/host/bin/external/llvm/_objs/tablegen/Main.o:Main.cpp:function '
                 'llvm::cl::list<std::string, bool, llvm::cl::parser<std::string> '
                 ">::~list(): error: undefined reference to 'operator delete(void*, "
                 "unsigned long)'\r\n"
                 '```\r\n'
                 '\r\n'
                 '```\r\n'
                 'ERROR: /tmp/bazel/external/com_google_protobuf/BUILD:406:1: Linking '
                 "of rule '@com_google_protobuf//:protoc' failed (Exit 1)\r\n"
                 'bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.o:code_generator.cc:function '
                 'google::protobuf::compiler::ParseGeneratorParameter(std::string '
                 'const&, std::vector<std::pair<std::string, std::string>, '
                 'std::allocator<std::pair<std::string, std::string> > >*): error: '
                 "undefined reference to 'std::__throw_out_of_range_fmt(char const*, "
                 "...)'\r\n"
                 'bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function '
                 'google::protobuf::io::StringOutputStream::~StringOutputStream(): '
                 "error: undefined reference to 'operator delete(void*, unsigned "
                 "long)'\r\n"
                 '```\r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Stateless LSTM from Keras tutorial '
                 'using tf backend\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (use command below): 2.1.0\r\n'
                 '- Python version: 3.7.4\r\n'
                 '- CUDA/cuDNN version: 10.1\r\n'
                 '- GPU model and memory: MX150 10GB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'When using tf.keras.callbacks.TensorBoard() without the '
                 'profile_batch setting, it gives out errors of '
                 'CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and '
                 'CUPTI_ERROR_INVALID_PARAMETER from '
                 'tensorflow/core/profiler/internal/gpu/cupti_tracer.cc.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'With profile_batch = 0, these two errors are gone. \r\n'
                 'But comes back when profile_batch = 1, or other non-zero values.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n'
                 '\r\n'
                 'from __future__ import print_function\r\n'
                 'import numpy as np\r\n'
                 'import matplotlib.pyplot as plt\r\n'
                 'import pandas as pd\r\n'
                 'import tensorflow as tf\r\n'
                 'from tensorflow.keras.models import Sequential\r\n'
                 'from tensorflow.keras.layers import Dense, LSTM\r\n'
                 '\r\n'
                 '\r\n'
                 'input_len = 1000\r\n'
                 'tsteps = 2\r\n'
                 'lahead = 1\r\n'
                 'batch_size = 1\r\n'
                 'epochs = 5\r\n'
                 '\r\n'
                 'print("*" * 33)\r\n'
                 'if lahead >= tsteps:\r\n'
                 '    print("STATELESS LSTM WILL ALSO CONVERGE")\r\n'
                 'else:\r\n'
                 '    print("STATELESS LSTM WILL NOT CONVERGE")\r\n'
                 'print("*" * 33)\r\n'
                 '\r\n'
                 'np.random.seed(1986)\r\n'
                 '\r\n'
                 "print('Generating Data...')\r\n"
                 '\r\n'
                 '\r\n'
                 'def gen_uniform_amp(amp=1, xn=10000):\r\n'
                 '\r\n'
                 '    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)\r\n'
                 '    data_input = pd.DataFrame(data_input)\r\n'
                 '    return data_input\r\n'
                 '\r\n'
                 '\r\n'
                 'to_drop = max(tsteps - 1, lahead - 1)\r\n'
                 'data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)\r\n'
                 '\r\n'
                 'expected_output = data_input.rolling(window=tsteps, '
                 'center=False).mean()\r\n'
                 '\r\n'
                 'if lahead > 1:\r\n'
                 '    data_input = np.repeat(data_input.values, repeats=lahead, '
                 'axis=1)\r\n'
                 '    data_input = pd.DataFrame(data_input)\r\n'
                 '    for i, c in enumerate(data_input.columns):\r\n'
                 '        data_input[c] = data_input[c].shift(i)\r\n'
                 '\r\n'
                 'expected_output = expected_output[to_drop:]\r\n'
                 'data_input = data_input[to_drop:]\r\n'
                 '\r\n'
                 '\r\n'
                 'def create_model(stateful):\r\n'
                 '    model = Sequential()\r\n'
                 '    model.add(LSTM(20,\r\n'
                 '              input_shape=(lahead, 1),\r\n'
                 '              batch_size=batch_size,\r\n'
                 '              stateful=stateful))\r\n'
                 '    model.add(Dense(1))\r\n'
                 "    model.compile(loss='mse', optimizer='adam')\r\n"
                 '    return model\r\n'
                 '\r\n'
                 "print('Creating Stateful Model...')\r\n"
                 'model_stateful = create_model(stateful=True)\r\n'
                 '\r\n'
                 '\r\n'
                 'def split_data(x, y, ratio=0.8):\r\n'
                 '    to_train = int(input_len * ratio)\r\n'
                 '    to_train -= to_train % batch_size\r\n'
                 '    x_train = x[:to_train]\r\n'
                 '    y_train = y[:to_train]\r\n'
                 '    x_test = x[to_train:]\r\n'
                 '    y_test = y[to_train:]\r\n'
                 '\r\n'
                 '    # tweak to match with batch_size\r\n'
                 '    to_drop = x.shape[0] % batch_size\r\n'
                 '    if to_drop > 0:\r\n'
                 '        x_test = x_test[:-1 * to_drop]\r\n'
                 '        y_test = y_test[:-1 * to_drop]\r\n'
                 '\r\n'
                 '    # some reshaping\r\n'
                 '    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], '
                 '1))\r\n'
                 '    x_train = reshape_3(x_train)\r\n'
                 '    x_test = reshape_3(x_test)\r\n'
                 '\r\n'
                 '    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))\r\n'
                 '    y_train = reshape_2(y_train)\r\n'
                 '    y_test = reshape_2(y_test)\r\n'
                 '\r\n'
                 '    return (x_train, y_train), (x_test, y_test)\r\n'
                 '\r\n'
                 '\r\n'
                 '(x_train, y_train), (x_test, y_test) = split_data(data_input, '
                 'expected_output)\r\n'
                 "print('x_train.shape: ', x_train.shape)\r\n"
                 "print('y_train.shape: ', y_train.shape)\r\n"
                 "print('x_test.shape: ', x_test.shape)\r\n"
                 "print('y_test.shape: ', y_test.shape)\r\n"
                 '\r\n'
                 "print('Creating Stateless Model...')\r\n"
                 'model_stateless = create_model(stateful=False)\r\n'
                 '\r\n'
                 'import os\r\n'
                 'import datetime\r\n'
                 'ROOT_DIR = os.getcwd()\r\n'
                 "log_dir = os.path.join('callback_tests')\r\n"
                 'if not os.path.exists(log_dir):\r\n'
                 '    os.makedirs(log_dir)\r\n'
                 'print(log_dir)\r\n'
                 'tensorboard_callback = '
                 'tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n'
                 '                                       \r\n'
                 "print('Training')\r\n"
                 'history = model_stateless.fit(x_train,\r\n'
                 '                    y_train,\r\n'
                 '                    batch_size=batch_size,\r\n'
                 '                    epochs=epochs,\r\n'
                 '                    verbose=1,\r\n'
                 '                    validation_data=(x_test, y_test),\r\n'
                 '                    shuffle=False,\r\n'
                 '                    callbacks=[tensorboard_callback]\r\n'
                 '                    )\r\n'
                 '\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Train on 800 samples, validate on 200 samples\r\n'
                 '2020-01-14 21:30:27.591905: I '
                 'tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler '
                 'session started.\r\n'
                 '2020-01-14 21:30:27.594743: I '
                 'tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] '
                 'Profiler found 1 GPUs\r\n'
                 '2020-01-14 21:30:27.599172: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library cupti64_101.dll\r\n'
                 '2020-01-14 21:30:27.704083: E '
                 'tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] '
                 'function cupti_interface_->Subscribe( &subscriber_, '
                 '(CUpti_CallbackFunc)ApiCallback, this)failed with error '
                 'CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n'
                 '2020-01-14 21:30:27.716790: E '
                 'tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] '
                 'function cupti_interface_->ActivityRegisterCallbacks( '
                 'AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error '
                 'CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n'
                 'Epoch 1/5\r\n'
                 '2020-01-14 21:30:28.370429: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library cublas64_10.dll\r\n'
                 '2020-01-14 21:30:28.651767: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library cudnn64_7.dll\r\n'
                 '2020-01-14 21:30:29.662864: E '
                 'tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] '
                 'function cupti_interface_->EnableCallback( 0 , subscriber_, '
                 'CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error '
                 'CUPTI_ERROR_INVALID_PARAMETER\r\n'
                 '2020-01-14 21:30:29.670282: I '
                 'tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  '
                 'GpuTracer has collected 0 callback api events and 0 activity '
                 'events.\r\n'
                 '800/800 [==============================] - 5s 6ms/sample - loss: '
                 '0.0011 - val_loss: 0.0011\r\n'
                 'Epoch 2/5\r\n'
                 '800/800 [==============================] - 3s 4ms/sample - loss: '
                 '8.5921e-04 - val_loss: 0.0010\r\n'
                 'Epoch 3/5\r\n'
                 '800/800 [==============================] - 3s 3ms/sample - loss: '
                 '8.5613e-04 - val_loss: 0.0010\r\n'
                 'Epoch 4/5\r\n'
                 '800/800 [==============================] - 3s 4ms/sample - loss: '
                 '8.5458e-04 - val_loss: 9.9713e-04\r\n'
                 'Epoch 5/5\r\n'
                 '800/800 [==============================] - 3s 4ms/sample - loss: '
                 '8.5345e-04 - val_loss: 9.8825e-04\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '\r\n'
                 'TensorFlow version (you are using): 2.0\r\n'
                 'Are you willing to contribute it (Yes/No): Yes\r\n'
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 '\r\n'
                 'I am looking to add one new optimizer called "Adamod" . Adamod was  '
                 'proposed by Jianbang Ding in the paper "An Adaptive and Momental '
                 'Bound Method for Stochastic Learning"\r\n'
                 'AdaMod uses a new parameter called “Beta3”. The results are '
                 'improved convergence, no need for warmup, and less sensitivity to '
                 'the actual learning rate chosen. Beta3 parameter controls the '
                 'degree of memory.\r\n'
                 '\r\n'
                 '**Will this change the current api? How?**\r\n'
                 "I 've edited `optimizers.py` and created a new file adamod.py which "
                 'is compatible with Tensorflow 2.x.\r\n'
                 'Usage :\r\n'
                 '\r\n'
                 '```\r\n'
                 "model.compile(optimizer='adamod',\r\n"
                 "              loss='sparse_categorical_crossentropy',\r\n"
                 "              metrics=['accuracy'])\r\n"
                 '```\r\n',
         'created_at': '2020-01-1'},
        {'body': 'Hi,\r\n'
                 '\r\n'
                 'is there a way (documented procedure) for preparing "Python wheel" '
                 'to just install the interpreter to run inferences with TensorFlow '
                 'Lite.  On the platform I use (arm64) I have option to run an '
                 'inference with CPU or GPU. How this will be evaluated via '
                 'Interpreter, is there an option to have more possible IP blocks to '
                 'run inference on?\r\n'
                 "I'm using the 1.13.2 version.",
         'created_at': '2020-01-1'},
        {'body': 'Recently,  I ran AI benchmark v4 model(Mobilenet-v3-quant.tflite) '
                 'by label_image. But I got the different result by tflite and NNAPI '
                 'implementation quantization. The detailed difference is just 1 per '
                 'quant HardSwish layer. And HardSwish is splited by NNAPI delegate '
                 'as below: \r\n'
                 '\r\n'
                 '> // Lower hardswish according to the following equation:\r\n'
                 '> // hard_swish[x] = x (ReLU6(x + 3)) / 6 == x * (Relu_N1_to_1(x/3) '
                 '* 3 + 3) / 6\r\n'
                 '> // = 0.5x * Relu_N1_to_1(x/3) + 0.5x\r\n'
                 '\r\n'
                 'in nnpai_delegate.cc at the line around 510.\r\n'
                 'BTW, NNAPI uses int32 quantization but tflite uses int16.',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 '- TensorFlow version (use command below): 2.1.0-rc1\r\n'
                 '- Python version: 3.6.9\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source): \r\n'
                 '- CUDA/cuDNN version: V10.1.243\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'I use tf.image.random_brightness api for augmentation. With dataset '
                 'api I put those augmentation functions in parser method and use '
                 'Dataset.map api for applying the method to the dataset and '
                 '_Transformed dataset_ is not reproducible when I put non negative '
                 'integer > 0 in _num_parallel_calls_ argument of Dataset.map api. It '
                 'is also the same if I use tf.data.experimental.AUTOTUNE for '
                 'num_parallel_calls argument.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '[here is the colab link to reproduce the '
                 'issue](https://colab.research.google.com/drive/1uNpn1Rf1_WvG2lnAS41g36IDWOB2IW7-)\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'It looks like tf.random.uniform method is the reason of happening '
                 'this. The implementation of tf.image.random_brightness use '
                 'tensorflow random uniform method and the method is not reproducible '
                 'with multiprocessing. I have tested tf.image.random_contrast and '
                 'tf.image.random_saturation. They are not reproducible also.\r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS: Windows 10 Pro x64 18362\r\n'
                 '- TensorFlow installed from (source or binary): source (master, '
                 '8782b7679c12780fd914827abf4e79ceb51d6b41)\r\n'
                 '- TensorFlow version: 2.1.0\r\n'
                 '- Python version: 3.5.3/3.6.8/3.7.6/3.8.1\r\n'
                 '- Installed using virtualenv? pip? conda?: virtualenv\r\n'
                 '- Bazel version (if compiling from source): 1.2.1\r\n'
                 '- GCC/Compiler version (if compiling from source): VS2017 '
                 '14.16.27023\r\n'
                 '- CUDA/cuDNN/tensorrt version: 10.2.89, 7.6.5.32, 7.0.0.11\r\n'
                 '- GPU model and memory: RTX 2080Ti, 11GB\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 '```\r\n'
                 'build --action_env '
                 'PYTHON_BIN_PATH="C:/tensorflow/venv36/Scripts/python.exe"\r\n'
                 'build --action_env '
                 'PYTHON_LIB_PATH="C:/tensorflow/venv36/lib/site-packages"\r\n'
                 'build --python_path="C:/tensorflow/venv36/Scripts/python.exe"\r\n'
                 'build:xla --define with_xla_support=true\r\n'
                 'build --action_env CUDA_TOOLKIT_PATH="C:/Program Files/NVIDIA GPU '
                 'Computing Toolkit/CUDA/v10.2"\r\n'
                 'build --action_env TF_CUDA_COMPUTE_CAPABILITIES="6.1,7.5"\r\n'
                 'build --config=cuda\r\n'
                 '# build --config=tensorrt\r\n'
                 'build:opt --copt=/arch:AVX\r\n'
                 'build:opt --define with_default_optimizations=true\r\n'
                 'build --define=override_eigen_strong_inline=true\r\n'
                 'test --flaky_test_attempts=3\r\n'
                 'test --test_size_filters=small,medium\r\n'
                 'test:v1 '
                 '--test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial\r\n'
                 'test:v1 '
                 '--build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu\r\n'
                 'test:v2 '
                 '--test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial,-v1only\r\n'
                 'test:v2 '
                 '--build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-v1only\r\n'
                 'build --action_env TF_CONFIGURE_IOS="0"\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 'If I enable tensorrt and compile using the method described in '
                 '[this '
                 'article](https://tensorflow.google.cn/install/source_windows#gpu_support), '
                 'I get the following error:\r\n'
                 '\r\n'
                 '\r\n'
                 '```\r\n'
                 'ERROR: C:/tensorflow/tensorflow/python/eager/BUILD:14:1: in '
                 'cc_library rule //tensorflow/python/eager:pywrap_tfe_lib: cycle in '
                 'dependency graph:\r\n'
                 '    //tensorflow/tools/pip_package:build_pip_package\r\n'
                 '    '
                 '//tensorflow/python/keras/distribute:distribute_strategy_test_lib\r\n'
                 '    //tensorflow/python/distribute:tpu_strategy\r\n'
                 '    //tensorflow/python/distribute:values\r\n'
                 '    //tensorflow/python:framework_ops\r\n'
                 '    //tensorflow/python:tensor_conversion_registry\r\n'
                 '    //tensorflow/python/eager:context\r\n'
                 '    //tensorflow/python/eager:executor\r\n'
                 '    //tensorflow/python:pywrap_tfe\r\n'
                 '    //tensorflow/python:pywrap_tensorflow\r\n'
                 '    //tensorflow/python:pywrap_tensorflow_internal\r\n'
                 '    //tensorflow/python:pywrap_tensorflow_internal.py\r\n'
                 '    //tensorflow/python:pywrap_tensorflow_internal_py_wrap\r\n'
                 '.-> //tensorflow/python/eager:pywrap_tfe_lib\r\n'
                 '|   //tensorflow/python:ndarray_tensor_bridge\r\n'
                 '|   //tensorflow/python:bfloat16_lib\r\n'
                 '|   //tensorflow/python:safe_ptr\r\n'
                 '|   //tensorflow/c/eager:c_api\r\n'
                 '|   //tensorflow/core/distributed_runtime/rpc:grpc_server_lib\r\n'
                 '|   '
                 '//tensorflow/core/distributed_runtime/rpc:grpc_master_service\r\n'
                 '|   //tensorflow/core/distributed_runtime:master\r\n'
                 '|   //tensorflow/core/distributed_runtime:master_session\r\n'
                 '|   //tensorflow/core/distributed_runtime:scheduler\r\n'
                 '|   //tensorflow/core:tensorflow_opensource\r\n'
                 '|   //tensorflow/core:all_kernels\r\n'
                 '|   //tensorflow/core:all_kernels_impl\r\n'
                 '|   //tensorflow/compiler/tf2tensorrt:trt_op_kernels\r\n'
                 '|   //tensorflow/python:pywrap_tensorflow_import_lib\r\n'
                 '|   //tensorflow/python:pywrap_tensorflow_import_lib_file\r\n'
                 '|   //tensorflow/python:get_pywrap_tensorflow_import_lib_file\r\n'
                 '|   //tensorflow/python:_pywrap_tensorflow_internal.so\r\n'
                 '`-- //tensorflow/python/eager:pywrap_tfe_lib\r\n'
                 'This cycle occurred because of a configuration option\r\n'
                 'ERROR: Analysis of target '
                 "'//tensorflow/tools/pip_package:build_pip_package' failed; build "
                 'aborted\r\n'
                 '```\r\n'
                 '\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code: Yes\r\n'
                 '- OS Platform and Distribution: Windows 10\r\n'
                 '- TensorFlow installed from: binary\r\n'
                 '- TensorFlow version: Same issue using tf2.0.0-beta1-cpu\r\n'
                 ' and tf1.14.0-gpu\r\n'
                 '- Python version: 3.6.9\r\n'
                 '- CUDA/cuDNN version: CUDA 10.1.168/cuDNN 7.6.2\r\n'
                 '- GPU model and memory: NVIDIA GeForce RTX 2060, 6GB dedicated '
                 'memory\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'A convolutional reggression (last layer has linear activation and '
                 'one neuron) network built with tf.keras is shown to fit the MNIST '
                 'dataset (I know that MNIST is a classification task; this is an '
                 'example) when converted to a dataset.\r\n'
                 '\r\n'
                 'When the same model is packaged into an estimator using  '
                 '`tf.keras.estimator.model_to_estimator` no error messages occur, '
                 'however the model no longer fits. The loss does not decrease.\r\n'
                 '\r\n'
                 'I had made a Stackoverflow question about this '
                 '(https://stackoverflow.com/q/59631744/9988487) with no traction '
                 'whatsoever. After some more trying to get it to work, I believe it '
                 'is a bug.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'The keras estimator should have the same behaviour as the '
                 'underlying model. Change the USE_ESTIMATOR variable to see that the '
                 'underlying model works.\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n'
                 '# python 3.6. Tested with tensorflow-gpu-1.14 and '
                 'tensorflow-cpu-2.0\r\n'
                 'import tensorflow as tf\r\n'
                 'import numpy as np\r\n'
                 '\r\n'
                 '\r\n'
                 'def get_model(IM_WIDTH=28, num_color_channels=1):\r\n'
                 '    """Create a very simple convolutional neural network using a '
                 'tf.keras Functional Model."""\r\n'
                 '    input = tf.keras.Input(shape=(IM_WIDTH, IM_WIDTH, '
                 'num_color_channels))\r\n'
                 "    x = tf.keras.layers.Conv2D(32, 3, activation='relu')(input)\r\n"
                 '    x = tf.keras.layers.MaxPooling2D(3)(x)\r\n'
                 "    x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\r\n"
                 '    x = tf.keras.layers.MaxPooling2D(3)(x)\r\n'
                 '    x = tf.keras.layers.Flatten()(x)\r\n'
                 "    x = tf.keras.layers.Dense(64, activation='relu')(x)\r\n"
                 "    output = tf.keras.layers.Dense(1, activation='linear')(x)\r\n"
                 '    model = tf.keras.Model(inputs=[input], outputs=[output])\r\n'
                 '    model.compile(optimizer=\'adam\', loss="mae",\r\n'
                 "                  metrics=['mae'])\r\n"
                 '    model.summary()\r\n'
                 '    return model\r\n'
                 '\r\n'
                 '\r\n'
                 'def input_fun(train=True):\r\n'
                 '    """Load MNIST and return the training or test set as a '
                 'tf.data.Dataset; Valid input function for tf.estimator"""\r\n'
                 '    (train_images, train_labels), (eval_images, eval_labels) = '
                 'tf.keras.datasets.mnist.load_data()\r\n'
                 '    train_images = train_images.reshape((60_000, 28, 28, '
                 '1)).astype(np.float32) / 255.\r\n'
                 '    eval_images = eval_images.reshape((10_000, 28, 28, '
                 '1)).astype(np.float32) / 255.\r\n'
                 '    # train_labels = train_labels.astype(np.float32)  # these two '
                 "lines don't affect behaviour.\r\n"
                 '    # eval_labels = eval_labels.astype(np.float32)\r\n'
                 '    # For a neural network with one neuron in the final layer, it '
                 "doesn't seem to matter if target data is float or int.\r\n"
                 '\r\n'
                 '    if train:\r\n'
                 '        dataset = tf.data.Dataset.from_tensor_slices((train_images, '
                 'train_labels))\r\n'
                 '        dataset = '
                 'dataset.shuffle(buffer_size=100).repeat(None).batch(32).prefetch(1)\r\n'
                 '    else:\r\n'
                 '        dataset = tf.data.Dataset.from_tensor_slices((eval_images, '
                 'eval_labels))\r\n'
                 '        dataset = dataset.batch(32).prefetch(1)  # note: '
                 'prefetching does not affect behaviour\r\n'
                 '\r\n'
                 '    return dataset\r\n'
                 '\r\n'
                 '\r\n'
                 'model = get_model()\r\n'
                 'train_input_fn = lambda: input_fun(train=True)\r\n'
                 'eval_input_fn = lambda: input_fun(train=False)\r\n'
                 '\r\n'
                 'NUM_EPOCHS, STEPS_PER_EPOCH = 4, 1875  # 1875 = '
                 'number_of_train_images(=60.000)  /  batch_size(=32)\r\n'
                 'USE_ESTIMATOR = False  # change this to compare model/estimator. '
                 'Estimator performs much worse for no apparent reason\r\n'
                 'if USE_ESTIMATOR:\r\n'
                 '    estimator = tf.keras.estimator.model_to_estimator(\r\n'
                 '        keras_model=model, model_dir="model_directory",\r\n'
                 '        config=tf.estimator.RunConfig(save_checkpoints_steps=200, '
                 'save_summary_steps=200))\r\n'
                 '\r\n'
                 '    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, '
                 'max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\r\n'
                 '    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, '
                 'throttle_secs=0)\r\n'
                 '\r\n'
                 '    tf.estimator.train_and_evaluate(estimator, train_spec, '
                 'eval_spec)\r\n'
                 '    print("Training complete. Evaluating Estimator:")\r\n'
                 '    print(estimator.evaluate(eval_input_fn))\r\n'
                 '    # final train loss with estimator: ~2.5 (mean abs. error).\r\n'
                 'else:\r\n'
                 '    dataset = train_input_fn()\r\n'
                 '    model.fit(dataset, steps_per_epoch=STEPS_PER_EPOCH, '
                 'epochs=NUM_EPOCHS)\r\n'
                 '    print("Training complete. Evaluating Keras model:")\r\n'
                 '    print(model.evaluate(eval_input_fn()))\r\n'
                 '    # final train loss with Keras model: ~0.4 (mean abs. error).\r\n'
                 '```\r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 'Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 'Google Colab\r\n'
                 '- TensorFlow version (use command below):\r\n'
                 '1.15.0\r\n'
                 '- Python version:\r\n'
                 '3.6.9\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 'Cuda compilation tools, release 10.0, V10.0.130\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'I am trying to convert my CPU GRU code to a CudnnGRU '
                 'implementation. When I run the code '
                 '[here](https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9), '
                 'my script runs for a seemingly random of number of training epochs '
                 '(normally between 0 and 20) before the session crashes, often with '
                 'a CUDA_ERROR_ILLEGAL_ADDRESS error. I see similar behaviour when I '
                 "run the script on my institution's hardware (TF version 1.14, CUDA "
                 'Version: 10.1).\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'See Google colab sheet '
                 '[here](https://colab.research.google.com/drive/1c64kUiCs8K17I5YygWaf14DKWhObdKA9). '
                 'I select the GPU as the hardware accelerator in the notebook '
                 'settings.\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '```\r\n'
                 'Learning Rate: 0.001 \r\n'
                 'Total number of portions: 10 \r\n'
                 'batch_length: 500 \r\n'
                 'n_mini_batches 25 \r\n'
                 'mini_batch_length 20 \r\n'
                 'number of MEG channels/PCs: 5 \r\n'
                 '\r\n'
                 'WARNING:tensorflow:\r\n'
                 'The TensorFlow contrib module will not be included in TensorFlow '
                 '2.0.\r\n'
                 'For more information, please see:\r\n'
                 '  * '
                 'https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\r\n'
                 '  * https://github.com/tensorflow/addons\r\n'
                 '  * https://github.com/tensorflow/io (for I/O related ops)\r\n'
                 'If you depend on functionality not listed there, please file an '
                 'issue.\r\n'
                 '\r\n'
                 'WARNING:tensorflow:From '
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:342: '
                 'calling GlorotUniform.__init__ (from '
                 'tensorflow.python.ops.init_ops) with dtype is deprecated and will '
                 'be removed in a future version.\r\n'
                 'Instructions for updating:\r\n'
                 'Call initializer instance with the dtype argument instead of '
                 'passing it to the constructor\r\n'
                 'WARNING:tensorflow:From '
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py:345: '
                 'calling Constant.__init__ (from tensorflow.python.ops.init_ops) '
                 'with dtype is deprecated and will be removed in a future '
                 'version.\r\n'
                 'Instructions for updating:\r\n'
                 'Call initializer instance with the dtype argument instead of '
                 'passing it to the constructor\r\n'
                 'Alpha coefficients will be soft-plus transformed\r\n'
                 'WARNING:tensorflow:From '
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/slot_creator.py:193: '
                 'Variable.initialized_value (from tensorflow.python.ops.variables) '
                 'is deprecated and will be removed in a future version.\r\n'
                 'Instructions for updating:\r\n'
                 'Use Variable.read_value. Variables in 2.X are initialized '
                 'automatically both in eager and graph (inside tf.defun) '
                 'contexts.\r\n'
                 'Device mapping:\r\n'
                 '/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU '
                 'device\r\n'
                 '/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU '
                 'device\r\n'
                 '/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: '
                 'Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\r\n'
                 '\r\n'
                 'Beginning training...\r\n'
                 '0\r\n'
                 '1\r\n'
                 '2\r\n'
                 '3\r\n'
                 '4\r\n'
                 '```\r\n'
                 '\r\n'
                 'Please see the colab-jupyter.log file attached.\r\n'
                 '\r\n'
                 'Apologies for my ignorance and thank you in advance for any kind '
                 'help.\r\n'
                 '[colab-jupyter.log](https://github.com/tensorflow/tensorflow/files/4055241/colab-jupyter.log)\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- TensorFlow version (you are using): \r\n'
                 'TF 2.0.0\r\n'
                 '- Are you willing to contribute it (Yes/No):\r\n'
                 'Yes\r\n'
                 '\r\n'
                 '**Describe the feature and the current behaviour/state.**\r\n'
                 'Currently `tf.keras.models.Model.fit` method allows the user to '
                 "pass either 'sample_weight' and 'class_weight' parameters. These "
                 'are used to compute at [some '
                 'point](https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/engine/training.py#L2527 '
                 ") a standardised 'sample_weights' and used later on while "
                 'calculating the loss.\r\n'
                 '\r\n'
                 'This feature request is about extending the '
                 "'tf.keras.Model.evaluate' API so that is permits using "
                 '`class_weight` directly. The `evaluate` function already permits '
                 'for `sample_weight`.\r\n'
                 '\r\n'
                 '\r\n'
                 '**Will this change the current api? How?**\r\n'
                 'current API\r\n'
                 '```\r\n'
                 'evaluate(\r\n'
                 '    x=None,\r\n'
                 '    y=None,\r\n'
                 '    batch_size=None,\r\n'
                 '    verbose=1,\r\n'
                 '    sample_weight=None,\r\n'
                 '    steps=None,\r\n'
                 '    callbacks=None,\r\n'
                 '    max_queue_size=10,\r\n'
                 '    workers=1,\r\n'
                 '    use_multiprocessing=False\r\n'
                 ')\r\n'
                 '```\r\n'
                 'new API\r\n'
                 '```\r\n'
                 'evaluate(\r\n'
                 '    x=None,\r\n'
                 '    y=None,\r\n'
                 '    batch_size=None,\r\n'
                 '    verbose=1,\r\n'
                 '    sample_weight=None,\r\n'
                 '>>>    class_weight=None,\r\n'
                 '    steps=None,\r\n'
                 '    callbacks=None,\r\n'
                 '    max_queue_size=10,\r\n'
                 '    workers=1,\r\n'
                 '    use_multiprocessing=False\r\n'
                 ')\r\n'
                 '```\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 '\r\n'
                 'Those users of the API who would like to perform the evaluation of '
                 'a model that was trained with bespoke class weights.\r\n'
                 '\r\n'
                 '**Any Other info.**\r\n',
         'created_at': '2020-01-1'},
        {'body': 'Python Version: 3.76\r\n'
                 'TensorFlow Version: 2.1\r\n'
                 'OS: Windows 10\r\n'
                 '\r\n'
                 'Issue:\r\n'
                 'It does not seem the TimeDistributed layer supports multiple '
                 'inputs. See example code:\r\n'
                 '\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 'from tensorflow.keras.layers import Dense, Concatenate, '
                 'RepeatVector, Activation, Dot, Bidirectional, Embedding, Input, '
                 'SpatialDropout1D, LSTM, Dropout, Lambda, Conv1D, Attention, '
                 'AdditiveAttention, GlobalAveragePooling1D, TimeDistributed, '
                 'AveragePooling1D\r\n'
                 'from tensorflow.keras.models import Model\r\n'
                 'import numpy as np\r\n'
                 '\r\n'
                 'def example_2():\r\n'
                 '    # Encode each timestep\r\n'
                 "    input_1 = Input(shape=(None,), dtype='int64', "
                 'name="Input1")\r\n'
                 "    input_2 = Input(shape=(None,), dtype='int64', "
                 'name="Input2")\r\n'
                 '\r\n'
                 '    output = Concatenate([input_1, input_2])\r\n'
                 '    output = TimeDistributed(output)([input_1, input_2])\r\n'
                 '\r\n'
                 '    model = Model([input_1, input_2], output)\r\n'
                 "    model.compile(loss='categorical_crossentropy',\r\n"
                 "                  optimizer='rmsprop',\r\n"
                 "                  metrics=['accuracy'])\r\n"
                 '\r\n'
                 '    return model\r\n'
                 '\r\n'
                 '\r\n'
                 'input_1 = np.array([[1, 2, 3, 4, 5, 6, 7]])\r\n'
                 'input_2 = np.array([[1, 1, 1, 1, 1, 1, 1]])\r\n'
                 'y = np.array([1, 0, 1, 1, 0, 0, 1])\r\n'
                 '\r\n'
                 'example_2().fit(input=[input_1, input_2], output=y)\r\n'
                 '```\r\n'
                 'I get the following issue:\r\n'
                 '\r\n'
                 '```\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/TimeDistributed_Multiple_Inputs.py", '
                 'line 26, in <module>\r\n'
                 '    example_2().fit(input=[input_1, input_2], output=y)\r\n'
                 '  File '
                 '"C:/Development/Projects/TensorFlow_2.0_Hierarchical_Attention/TimeDistributed_Multiple_Inputs.py", '
                 'line 12, in example_2\r\n'
                 '    output = TimeDistributed(output)([input_1, input_2])\r\n'
                 '  File '
                 '"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py", '
                 'line 748, in __call__\r\n'
                 '    self._maybe_build(inputs)\r\n'
                 '  File '
                 '"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py", '
                 'line 2116, in _maybe_build\r\n'
                 '    self.build(input_shapes)\r\n'
                 '  File '
                 '"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\wrappers.py", '
                 'line 197, in build\r\n'
                 '    input_shape = '
                 'tensor_shape.TensorShape(input_shape).as_list()\r\n'
                 '  File '
                 '"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py", '
                 'line 771, in __init__\r\n'
                 '    self._dims = [as_dimension(d) for d in dims_iter]\r\n'
                 '  File '
                 '"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py", '
                 'line 771, in <listcomp>\r\n'
                 '    self._dims = [as_dimension(d) for d in dims_iter]\r\n'
                 '  File '
                 '"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py", '
                 'line 716, in as_dimension\r\n'
                 '    return Dimension(value)\r\n'
                 '  File '
                 '"C:\\Development\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py", '
                 'line 200, in __init__\r\n'
                 '    None)\r\n'
                 '  File "<string>", line 3, in raise_from\r\n'
                 'TypeError: Dimension value must be integer or None or have an '
                 '__index__ method, got TensorShape([None, None])\r\n'
                 '```\r\n'
                 '\r\n'
                 '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 '- TensorFlow version (use command below):\r\n'
                 '- Python version:\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 'You can collect some of this information using our environment '
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n'
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): No\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): Not applicable\r\n'
                 '- TensorFlow version (use command below): 2.1.0\r\n'
                 '- Python version: Not applicable\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: Not applicable\r\n'
                 '- GPU model and memory: Not applicable\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'When applying a layer multiple times to the same input, it seems TF '
                 "is computing it for every call, even if it's unnecessary, see "
                 'colab:\r\n'
                 'https://colab.research.google.com/drive/1PTvolVt1xQJvgp4MP0ZvgbemKBrqWy0U\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'The duplicate parts of the graph should be squashed\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'https://colab.research.google.com/drive/1PTvolVt1xQJvgp4MP0ZvgbemKBrqWy0U\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS '
                 '7\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (or github SHA if from source): r1.14\r\n'
                 '\r\n'
                 '\r\n'
                 '**Command used to run the converter or code if you’re using the '
                 'Python API**\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'converter = '
                 "tf.lite.TFLiteConverter.from_saved_model('./savedmodel')\r\n"
                 'tflite_model = converter.convert()\r\n'
                 'open("lstm.tflite", "wb").write(tflite_model)\r\n'
                 '\r\n'
                 '**The output from the converter invocation**\r\n'
                 '\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "saved2lite.py", line 4, in <module>\r\n'
                 '    tflite_model = converter.convert()\r\n'
                 '  File '
                 '"/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/lite.py", '
                 'line 898, in convert\r\n'
                 '    **converter_kwargs)\r\n'
                 '  File '
                 '"/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/convert.py", '
                 'line 404, in toco_convert_impl\r\n'
                 '    input_data.SerializeToString())\r\n'
                 '  File '
                 '"/workdir/env-cpu/lib/python2.7/site-packages/tensorflow/lite/python/convert.py", '
                 'line 172, in toco_convert_protos\r\n'
                 '    "TOCO failed. See console for info.\\n%s\\n%s\\n" % (stdout, '
                 'stderr))\r\n'
                 'tensorflow.lite.python.convert.ConverterError: TOCO failed. See '
                 'console for info.\r\n'
                 '2020-01-13 14:56:27.199876: F '
                 'tensorflow/lite/toco/tooling_util.cc:918] Check failed: '
                 'GetOpWithOutput(model, output_array) Specified output array '
                 '"lstm/body/encoder/rnn/while/Identity_4" is not produced by any op '
                 'in this graph. Is it a typo? This should not happen. If you trigger '
                 'this error please send a bug report (with code to reporduce this '
                 'error), to the TensorFlow Lite team.\r\n'
                 '\r\n'
                 '\r\n'
                 '**Failure details**\r\n'
                 '\r\n'
                 'Model conversion fails when final_state is returned as an output\r\n'
                 '\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 '\r\n'
                 'relevant code in model definition:\r\n'
                 '\r\n'
                 'output, next_state = '
                 'tf.compat.v1.lite.experimental.nn.dynamic_rnn(...)\r\n'
                 'c_out = next_state[0].c\r\n'
                 'h_out = next_state[0].h\r\n'
                 '\r\n'
                 'relevant code in conversion to SavedModel (which does work):\r\n'
                 '\r\n'
                 'tensor_info_c_out = '
                 'tf.saved_model.utils.build_tensor_info(c_out)\r\n'
                 'tensor_info_h_out = '
                 'tf.saved_model.utils.build_tensor_info(h_out)\r\n'
                 '\r\n'
                 'prediction_signature = '
                 'tf.saved_model.signature_def_utils.build_signature_def(inputs = '
                 "{'x': x}, outputs = {'y': output, 'c_out': c_out, 'h_out':h_out), "
                 'method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\r\n'
                 '\r\n'
                 '\r\n'
                 "The TFLite conversion works if I don't return the next state "
                 'variables, c_out and h_out, but I need those to run the next step '
                 'of the prediction.\r\n'
                 '\r\n'
                 'If I run the conversion with tfnightly (using the same SavedModel '
                 'file saved using 1.14) I get a different, seemingly unrelated, '
                 'error:\r\n'
                 '\r\n'
                 'Some of the operators in the model are not supported by the '
                 'standard TensorFlow Lite runtime. If those are native TensorFlow '
                 'operators, you might be able to use the extended runtime by passing '
                 '--enable_select_tf_ops, or by setting '
                 'target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling '
                 'tf.lite.TFLiteConverter(). Otherwise, if you have a custom '
                 'implementation for them you can disable this error with '
                 '--allow_custom_ops, or by setting allow_custom_ops=True when '
                 'calling tf.lite.TFLiteConverter(). Here is a list of builtin '
                 'operators you are using: CAST, FULLY_CONNECTED, GATHER, MUL, '
                 'NOT_EQUAL, RESHAPE, SOFTMAX, SQUEEZE, TOPK_V2. Here is a list of '
                 'operators for which you will need custom implementations: '
                 'TensorListFromTensor, TensorListReserve, TensorListStack, While.',
         'created_at': '2020-01-1'},
        {
            'body': 'https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c04_exercise_convert_model_to_tflite_solution.ipynb\r\n'
                    '\r\n'
                    'Under "Create a Dataset from Images and Labels", we have this '
                    'code\r\n'
                    '\r\n'
                    '`train_batches=train_examples.cache().shuffle(num_examples//4).batch(BATCH_SIZE).map(format_example).prefetch(1)`\r\n'
                    '\r\n'
                    'and similar for the validation and test examples...\r\n'
                    '\r\n'
                    'Is this a right practise?? \r\n'
                    "Shouldn't we first format the raw images and then batch them "
                    'together rather than opposite way?',
            'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- No custom code\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04.3 LTS\r\n'
                 '- TensorFlow installed from (source or binary): from pip\r\n'
                 '- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de '
                 '2.1.0\r\n'
                 '- Python version: Python 3.7.6 | packaged by conda-forge | '
                 '(default, Jan  7 2020, 22:33:48) \r\n'
                 '[GCC 7.3.0] on linux\r\n'
                 '\r\n'
                 '- CUDA/cuDNN version: N/A\r\n'
                 '- GPU model and memory: N/A\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'When I build a `tf.keras` model with the functional API in a '
                 '`threading.Thread`, I often (not always) see an error in a '
                 'thread:\r\n'
                 '```python\r\n'
                 "TypeError: 'NoneType' object is not iterable\r\n"
                 '```\r\n'
                 'After the exception has appeared once, it does not appear again, '
                 'until the python kernel is restarted.\r\n'
                 '\r\n'
                 'In fact, it is not even necessary to build a whole model. Just '
                 'instantiating a `tf.keras.layers.Input` layer is sufficient to get '
                 'the error. I tested a few other layers (Dense, Conv1D) and they do '
                 'not lead to the error, so am suspecting that there is something '
                 'happening with Input)\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'I expect that no such error should appear in any thread when '
                 'building a `tf.keras` model.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```python\r\n'
                 'from threading import Thread\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'def make_model():\r\n'
                 '    tf.keras.layers.Input(10)\r\n'
                 '    \r\n'
                 '[Thread(target=make_model).start() for _ in range(10)]\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'The error does not appear if i instantiate a '
                 '`tf.keras.models.Model` subclass.\r\n'
                 'for example:\r\n'
                 '\r\n'
                 '```python\r\n'
                 'from threading import Thread\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'class Model(tf.keras.models.Model):\r\n'
                 '    def __init__(self):\r\n'
                 '        super().__init__()\r\n'
                 '        self.dense = tf.keras.layers.Dense(10)\r\n'
                 '    \r\n'
                 '    def call(self, inputs):\r\n'
                 '        return self.dense(inputs)\r\n'
                 '    \r\n'
                 'def make_model():\r\n'
                 '    Model()\r\n'
                 '    \r\n'
                 '[Thread(target=make_model).start() for _ in range(10)]\r\n'
                 '```\r\n'
                 '\r\n'
                 'Full traceback:\r\n'
                 '```python\r\n'
                 'Exception in thread Thread-3:\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/threading.py", '
                 'line 926, in _bootstrap_inner\r\n'
                 '    self.run()\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/threading.py", '
                 'line 870, in run\r\n'
                 '    self._target(*self._args, **self._kwargs)\r\n'
                 '  File "<ipython-input-1-3fe088eac4c0>", line 5, in make_model\r\n'
                 '    tf.keras.layers.Input(10)\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py", '
                 'line 274, in Input\r\n'
                 '    input_layer = InputLayer(**input_layer_config)\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py", '
                 'line 127, in __init__\r\n'
                 '    ragged=ragged)\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py", '
                 'line 1054, in placeholder\r\n'
                 '    x = array_ops.placeholder(dtype, shape=shape, name=name)\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py", '
                 'line 2990, in placeholder\r\n'
                 '    return gen_array_ops.placeholder(dtype=dtype, shape=shape, '
                 'name=name)\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py", '
                 'line 6676, in placeholder\r\n'
                 '    "Placeholder", dtype=dtype, shape=shape, name=name)\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py", '
                 'line 759, in _apply_op_helper\r\n'
                 '    return output_structure, op_def.is_stateful, op, outputs\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/contextlib.py", '
                 'line 119, in __exit__\r\n'
                 '    next(self.gen)\r\n'
                 '  File '
                 '"/home/dani/miniconda3/envs/testtf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py", '
                 'line 418, in inner_cm\r\n'
                 '    for fn in self._scope_exit_callbacks:\r\n'
                 "TypeError: 'NoneType' object is not iterable\r\n"
                 '```\r\n'
                 'Notes: the error does not appear in tf 2.0.',
         'created_at': '2020-01-1'},
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_lite/tflite_c02_transfer_learning.ipynb\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 '![Screenshot from 2020-01-13 '
                 '12-53-58](https://user-images.githubusercontent.com/29497701/72238480-d7b53400-3603-11ea-847d-0eb7c0eb0716.png)\r\n'
                 '\r\n'
                 "In description, it should be 'cats_vs_dogs'\r\n"
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 'Yes',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu '
                 '16.04):ubuntu18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary):pip\r\n'
                 '- TensorFlow version (use command below):1.14.0\r\n'
                 '- Python version:\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 'You can collect some of this information using our environment '
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n'
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 "nnapi log said:concat axis!=-1 and reshape change data layout don't "
                 'support accelator\r\n'
                 '**Describe the expected behavior**\r\n'
                 'it should run successful on dsp or hta\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 './benmakrk --graph=myquantized.tflite --use_nnapi=true\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 'New Comment: "01-08 20:47:35.790 14384 14384 I TypeManager: Failed '
                 'to read /vendor/etc/nnapi_extensions_app_allowlist ; No app '
                 'allowlisted for vendor extensions use.\r\n'
                 '01-08 20:47:35.799 919 1023 E OperationsUtils: NN_CHECK failed: '
                 "inputShapes[0].scale == inputShapes[i].scale'\r\n"
                 '01-08 20:47:35.799 919 1023 E OperationsUtils:\r\n'
                 '01-08 20:47:35.799 919 1023 E OperationsUtils: NN_CHECK failed: '
                 "inputShapes[0].offset == inputShapes[i].offset'\r\n"
                 '01-08 20:47:35.799 919 1023 E OperationsUtils:\r\n'
                 "01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 "01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 "01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: Only supports channel '
                 'dimension for CONCATENATION\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: {CONCATENATION, '
                 'TENSOR_QUANT8} is not supported.\r\n'
                 "01-08 20:47:35.801 919 1023 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: Only supports channel '
                 'dimension for CONCATENATION\r\n'
                 '01-08 20:47:35.801 919 1023 E hta-unnhal: {CONCATENATION, '
                 'TENSOR_QUANT8} is not supported.\r\n'
                 '01-08 20:47:35.805 919 1023 E OperationsUtils: NN_CHECK failed: '
                 "inputShapes[0].scale == inputShapes[i].scale'\r\n"
                 '01-08 20:47:35.805 919 1023 E OperationsUtils:\r\n'
                 '01-08 20:47:35.806 919 1023 E OperationsUtils: NN_CHECK failed: '
                 "inputShapes[0].offset == inputShapes[i].offset'\r\n"
                 '01-08 20:47:35.806 919 1023 E OperationsUtils:\r\n'
                 '01-08 20:47:35.810 919 1023 W '
                 'android.hardware.neuralnetworks@1.2-service: pickAcceleratorByName '
                 'cannot find the accelerator:adreno\r\n'
                 '01-08 20:47:35.810 919 1023 W '
                 'android.hardware.neuralnetworks@1.2-service:\r\n'
                 '01-08 20:47:35.810 919 1023 W '
                 'android.hardware.neuralnetworks@1.2-service: '
                 'getSupportedOperations() cannot pick an accelerator\r\n'
                 '01-08 20:47:35.810 919 1023 W '
                 'android.hardware.neuralnetworks@1.2-service:\r\n'
                 "01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 "01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 "01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: Only supports channel '
                 'dimension for CONCATENATION\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: {CONCATENATION, '
                 'TENSOR_QUANT8} is not supported.\r\n'
                 "01-08 20:47:35.814 919 919 E hta-unnhal: RESHAPE doesn't support "
                 'changing data layout\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: {RESHAPE, TENSOR_QUANT8} '
                 'is not supported.\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: Only supports channel '
                 'dimension for CONCATENATION\r\n'
                 '01-08 20:47:35.814 919 919 E hta-unnhal: {CONCATENATION, '
                 'TENSOR_QUANT8} is not supported."\r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10 home\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: PC\r\n'
                 '- TensorFlow installed from (source or binary): 1.13.0\r\n'
                 '- TensorFlow version:\r\n'
                 '- Python version: 3.6\r\n'
                 '- Installed using virtualenv? pip? conda?: conda\r\n'
                 '- Bazel version (if compiling from source): 0.21.0\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10.0 7.4.2.24\r\n'
                 '- GPU model and memory: RTX 2080 Ti\r\n'
                 '- CPU model and make: AMD Ryzen 7 3800X 8-Core Processor 3.89GHz\r\n'
                 '- Anaconda Python Command Prompt\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 'Trying to build TF 1.13 from source. Issue at the creation of wheel '
                 'file with bazel. Attached exact sequence of commands issued. Trying '
                 'to conver a *.pb file into a TFLite file that was trained in '
                 'tensorflow 1.13\r\n'
                 '\r\n'
                 '**Exact sequence of commands / steps executed before running into '
                 'the problem**\r\n'
                 '\r\n'
                 'conda update -n base -c defaults conda\r\n'
                 'conda update --all\r\n'
                 'conda create -n tensorflow-build pip python=3.6\r\n'
                 'conda activate tensorflow-build\r\n'
                 'python -m pip install --upgrade pip\r\n'
                 'conda install -c anaconda git\r\n'
                 'set PATH=%PATH%;C:\\msys64\\usr\\bin\r\n'
                 'pip install six numpy wheel\r\n'
                 'pip install keras_applications==1.0.6 --no-deps\r\n'
                 'pip install keras_preprocessing==1.0.5 --no-deps\r\n'
                 'conda install -c conda-forge bazel=0.21.0\r\n'
                 'mkdir C:\\tensorflow-build\r\n'
                 'cd C:\\tensorflow-build\r\n'
                 'git clone https://github.com/tensorflow/tensorflow.git \r\n'
                 'cd tensorflow\r\n'
                 'git checkout r1.13\r\n'
                 'python ./configure.py\r\n'
                 '\r\n'
                 'You have bazel 0.21.0- (@non-git) installed. \r\n'
                 '\r\n'
                 'Please specify the location of python. [Default is '
                 'C:\\ProgramData\\Anaconda3\\envs\\tensorflow-build\\python.exe]: \r\n'
                 '  \r\n'
                 'Found possible Python library paths: \r\n'
                 '\r\n'
                 '  '
                 'C:\\ProgramData\\Anaconda3\\envs\\tensorflow-build\\lib\\site-packages \r\n'
                 '\r\n'
                 'Please input the desired Python library path to use.  Default is '
                 '[C:\\ProgramData\\Anaconda3\\envs\\tensorflow-build\\lib\\site-packages] \r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with XLA JIT support? [y/N]: n \r\n'
                 'No XLA JIT support will be enabled for TensorFlow. \r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with ROCm support? [y/N]: n \r\n'
                 'No ROCm support will be enabled for TensorFlow. \r\n'
                 '  \r\n'
                 'Do you wish to build TensorFlow with CUDA support? [y/N]: y \r\n'
                 '\r\n'
                 'Please specify the CUDA SDK version you want to use. [Leave empty '
                 'to default to CUDA 10.0]: 10.0\r\n'
                 '\r\n'
                 'Please specify the location where CUDA 10.0 toolkit is installed. '
                 'Refer to README.md for more details. [Default is C:/Program '
                 'Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n'
                 '\r\n'
                 'Please specify the cuDNN version you want to use. [Leave empty to '
                 'default to cuDNN 7]: 7.4\r\n'
                 '\r\n'
                 'Please specify the location where cuDNN 7 library is installed. '
                 'Refer to README.md for more details. [Default is C:/Program '
                 'Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0]:\r\n'
                 '\r\n'
                 'Please specify a list of comma-separated Cuda compute capabilities '
                 'you want to build with.\r\n'
                 'You can find the compute capability of your device at: '
                 'https://developer.nvidia.com/cuda-gpus.\r\n'
                 'Please note that each additional compute capability significantly '
                 'increases your build time and binary size. [Default is: 3.5,7.0]: '
                 '7.5\r\n'
                 '\r\n'
                 'Please specify optimization flags to use during compilation when '
                 'bazel option "--config=opt" is specified [Default is /arch:AVX]:\r\n'
                 '\r\n'
                 'Would you like to override eigen strong inline for some C++ '
                 'compilation to reduce the compilation time? [Y/n]: y\r\n'
                 'Eigen strong inline overridden.\r\n'
                 '\r\n'
                 'bazel build --config=opt --config=cuda '
                 '--define=no_tensorflow_py_deps=true '
                 '//tensorflow/tools/pip_package:build_pip_package\r\n'
                 '\r\n'
                 '\r\n'
                 '**info / logs**\r\n'
                 '\r\n'
                 '////////////////////////////////////////////////////////LOG/////////////////////////////////////////////////////\r\n'
                 '\r\n'
                 'ERROR: C:/tensorflow-build/tensorflow/tensorflow/BUILD:579:1: '
                 'Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit '
                 '1): bash.exe failed: error executing command\r\n'
                 '  cd '
                 'C:/users/eduar/_bazel_eduar/j7bi4x5j/execroot/org_tensorflow\r\n'
                 '  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing '
                 'Toolkit/CUDA/v10.0\r\n'
                 '    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing '
                 'Toolkit/CUDA/v10.0\r\n'
                 '    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin\r\n'
                 '    SET '
                 'PYTHON_BIN_PATH=C:/Users/eduar/.conda/envs/tensorflow-build/python.exe\r\n'
                 '    SET '
                 'PYTHON_LIB_PATH=C:/Users/eduar/.conda/envs/tensorflow-build/lib/site-packages\r\n'
                 '    SET TF_CUDA_CLANG=0\r\n'
                 '    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5\r\n'
                 '    SET TF_CUDA_VERSION=10.0\r\n'
                 '    SET TF_CUDNN_VERSION=7\r\n'
                 '    SET TF_NEED_CUDA=1\r\n'
                 '    SET TF_NEED_OPENCL_SYCL=0\r\n'
                 '    SET TF_NEED_ROCM=0\r\n'
                 '  C:/msys64/usr/bin/bash.exe '
                 'bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh\r\n'
                 'Execution platform: @bazel_tools//platforms:host_platform\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py", '
                 'line 58, in <module>\r\n'
                 '    from tensorflow.python.pywrap_tensorflow_internal import *\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py", '
                 'line 28, in <module>\r\n'
                 '    _pywrap_tensorflow_internal = swig_import_helper()\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py", '
                 'line 24, in swig_import_helper\r\n'
                 "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, "
                 'pathname, description)\r\n'
                 '  File '
                 '"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py", '
                 'line 243, in load_module\r\n'
                 '    return load_dynamic(name, filename, file)\r\n'
                 '  File '
                 '"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py", '
                 'line 343, in load_dynamic\r\n'
                 '    return _load(spec)\r\n'
                 'ImportError: DLL load failed: The specified module could not be '
                 'found.\r\n'
                 '\r\n'
                 'During handling of the above exception, another exception '
                 'occurred:\r\n'
                 '\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py", '
                 'line 27, in <module>\r\n'
                 '    from tensorflow.python.tools.api.generator import doc_srcs\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py", '
                 'line 49, in <module>\r\n'
                 '    from tensorflow.python import pywrap_tensorflow\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py", '
                 'line 74, in <module>\r\n'
                 '    raise ImportError(msg)\r\n'
                 'ImportError: Traceback (most recent call last):\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow.py", '
                 'line 58, in <module>\r\n'
                 '    from tensorflow.python.pywrap_tensorflow_internal import *\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py", '
                 'line 28, in <module>\r\n'
                 '    _pywrap_tensorflow_internal = swig_import_helper()\r\n'
                 '  File '
                 '"\\\\?\\C:\\Users\\eduar\\AppData\\Local\\Temp\\Bazel.runfiles_fxnhn1zo\\runfiles\\org_tensorflow\\tensorflow\\python\\pywrap_tensorflow_internal.py", '
                 'line 24, in swig_import_helper\r\n'
                 "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, "
                 'pathname, description)\r\n'
                 '  File '
                 '"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py", '
                 'line 243, in load_module\r\n'
                 '    return load_dynamic(name, filename, file)\r\n'
                 '  File '
                 '"C:\\Users\\eduar\\.conda\\envs\\tensorflow-build\\lib\\imp.py", '
                 'line 343, in load_dynamic\r\n'
                 '    return _load(spec)\r\n'
                 'ImportError: DLL load failed: The specified module could not be '
                 'found.\r\n'
                 '\r\n'
                 '\r\n'
                 'Failed to load the native TensorFlow runtime.\r\n'
                 '\r\n'
                 'See https://www.tensorflow.org/install/errors\r\n'
                 '\r\n'
                 'for some common reasons and solutions.  Include the entire stack '
                 'trace\r\n'
                 'above this error message when asking for help.\r\n'
                 'Target //tensorflow/tools/pip_package:build_pip_package failed to '
                 'build\r\n'
                 'INFO: Elapsed time: 1319.085s, Critical Path: 291.62s\r\n'
                 'INFO: 4651 processes: 4651 local.\r\n'
                 'FAILED: Build did NOT complete successfully\r\n'
                 '\r\n'
                 '///////////////////////////////////////////////////////////////////////////////////////////////////////////////////\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): binary(pip)\r\n'
                 '- TensorFlow version (use command below): 2.0.0\r\n'
                 '- Python version: 3.6.8\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10.0/7.3.1\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 'You can collect some of this information using our environment '
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n'
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'Currently, passing a ragged tensor into a reduce_max an computing '
                 'the gradient on the result fails. The following code is a minimal '
                 'example of this:\r\n'
                 '```python\r\n'
                 'import numpy as np\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'with tf.GradientTape() as tape:\r\n'
                 '    ragged_t = tf.RaggedTensor.from_row_splits(\r\n'
                 '        [0.0, 1.0, 2.0, 3.0, 4.0, 5.0], [0, 1, 4, 6]\r\n'
                 '    )\r\n'
                 '    tape.watch(ragged_t.values)\r\n'
                 '    max_t = tf.reduce_max(ragged_t, axis=-1)\r\n'
                 '    # max_t = '
                 'tf.reduce_max(ragged_t.to_tensor(default_value=np.nan), axis=-1)\r\n'
                 '    gradients = tape.gradient(max_t, ragged_t.values)\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'As tensorflows main purpose is to compute gradients and reduce '
                 'operations on ragged tensors is the most basic usage of ragged '
                 'tensor representations I consider this behavior a bug. The above '
                 'code shows a workaround in the commented line, but I expect the '
                 'above example to work without this workaround and without the need '
                 'of allocating in some cases a substantial amount of memory through '
                 'the usage of `to_tensor`.\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'The full error trace of the above code example on my system is:\r\n'
                 '```\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "./misc/reduce_max_ragged_gradients.py", line 13, in '
                 '<module>\r\n'
                 '    gradients = tape.gradient(max_t, ragged_t.values)\r\n'
                 '  File '
                 '"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", '
                 'line 1014, in gradient\r\n'
                 '    unconnected_gradients=unconnected_gradients)\r\n'
                 '  File '
                 '"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/imperative_grad.py", '
                 'line 76, in imperative_grad\r\n'
                 '    compat.as_str(unconnected_gradients.value))\r\n'
                 '  File '
                 '"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/eager/backprop.py", '
                 'line 138, in _gradient_function\r\n'
                 '    return grad_fn(mock_op, *out_grads)\r\n'
                 '  File '
                 '"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py", '
                 'line 455, in _UnsortedSegmentMaxGrad\r\n'
                 '    return _UnsortedSegmentMinOrMaxGrad(op, grad)\r\n'
                 '  File '
                 '"/lhome/davidj2/code/sync/unsup_flow/.venv/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py", '
                 'line 432, in _UnsortedSegmentMinOrMaxGrad\r\n'
                 '    _GatherDropNegatives(op.outputs[0], op.inputs[1])\r\n'
                 "TypeError: 'NoneType' object is not subscriptable\r\n"
                 '```\r\n',
         'created_at': '2020-01-1'},
        {'body': 'Multi Head Self Attention is reaching SOTA results in a wide '
                 'variety of fields:\r\n'
                 '* NLP (Attention is all you need, GPT, Bert, etc)\r\n'
                 '* [Graphs](https://arxiv.org/abs/1710.10903)\r\n'
                 '* Computer Vision: [1](https://arxiv.org/abs/1906.05909) and '
                 '[2](https://arxiv.org/abs/1911.03584)\r\n'
                 '* [Time Series](https://arxiv.org/pdf/1909.07369.pdf)\r\n'
                 '* [Audio/Speech '
                 'Recognition](https://arxiv.org/pdf/1910.12977.pdf)\r\n'
                 '\r\n'
                 'Nvidia already added this operation to '
                 '[cudnn](https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_750.html) '
                 'and pytorch already has an official '
                 '[MultiHeadAttention](https://pytorch.org/docs/master/nn.html#multiheadattention) '
                 'module. Its about time Tensorflow gets this operation too as it '
                 'would enable researches and developers to get started quicker.\r\n'
                 '\r\n'
                 'Currently there is an `Attention` layer in tf.keras but it only '
                 'covers the main single head equation, has no learnable parameters, '
                 "and can't be used as a basis for an efficient `MultiHeadAttention` "
                 'implementation.\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Debian Buster\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de '
                 '2.1.0\r\n'
                 '- Python version: 3.7\r\n'
                 '- Bazel version (if compiling from source): N/A\r\n'
                 '- GCC/Compiler version (if compiling from source): N/A\r\n'
                 '- CUDA/cuDNN version: N/A\r\n'
                 '- GPU model and memory: N/A\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'When using a `tf.keras.backend.ctc_decode` with a batch size <  the '
                 'size of the model input, a ValueError is raised related to failure '
                 'to broadcast input shapes.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'I expect shapes to be consistent and therefore no `ValueError` to '
                 'be raised.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 '\r\n'
                 '```python\r\n'
                 'import tensorflow as tf\r\n'
                 'import numpy as np\r\n'
                 '\r\n'
                 'def CTCDecoder():\r\n'
                 '    def decoder(y_pred):\r\n'
                 '        input_shape = tf.keras.backend.shape(y_pred)\r\n'
                 '        input_length = tf.ones(shape=input_shape[0]) * '
                 "tf.keras.backend.cast(input_shape[1], 'float32')\r\n"
                 '        return tf.keras.backend.ctc_decode(y_pred, '
                 'input_length)[0][0]\r\n'
                 "    return tf.keras.layers.Lambda(decoder, name='decode')\r\n"
                 '\r\n'
                 'input_layer = tf.keras.layers.Input((48, 37))\r\n'
                 'x = CTCDecoder()(input_layer)\r\n'
                 'model = tf.keras.models.Model(inputs=input_layer, outputs=x)\r\n'
                 '\r\n'
                 '# This never raises a ValueError. The batch size is equal to the '
                 'length\r\n'
                 '# of the input.\r\n'
                 'y = model.predict(np.random.uniform(size=(100, 48, 37)), '
                 'batch_size=100)\r\n'
                 '\r\n'
                 '# This usually raises a ValueError.\r\n'
                 'y = model.predict(np.random.uniform(size=(100, 48, 37)), '
                 'batch_size=32)\r\n'
                 '\r\n'
                 '# This always raises a ValueError.\r\n'
                 'y = model.predict(np.random.uniform(size=(100, 48, 37)), '
                 'batch_size=1)\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 '\r\n'
                 'Here is the full traceback for an example exception.\r\n'
                 '\r\n'
                 '```\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'ValueError                                Traceback (most recent '
                 'call last)\r\n'
                 '<ipython-input-145-9e000cf7055c> in <module>\r\n'
                 '----> 1 y = model.predict(np.random.uniform(size=(100, 48, 37)), '
                 'batch_size=1)\r\n'
                 '\r\n'
                 '/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py '
                 'in predict(self, x, batch_size, verbose, steps, callbacks, '
                 'max_queue_size, workers, use_multiprocessing)\r\n'
                 '   1011         max_queue_size=max_queue_size,\r\n'
                 '   1012         workers=workers,\r\n'
                 '-> 1013         use_multiprocessing=use_multiprocessing)\r\n'
                 '   1014 \r\n'
                 '   1015   def reset_metrics(self):\r\n'
                 '\r\n'
                 '/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py '
                 'in predict(self, model, x, batch_size, verbose, steps, callbacks, '
                 'max_queue_size, workers, use_multiprocessing, **kwargs)\r\n'
                 '    496         model, ModeKeys.PREDICT, x=x, '
                 'batch_size=batch_size, verbose=verbose,\r\n'
                 '    497         steps=steps, callbacks=callbacks, '
                 'max_queue_size=max_queue_size,\r\n'
                 '--> 498         workers=workers, '
                 'use_multiprocessing=use_multiprocessing, **kwargs)\r\n'
                 '    499 \r\n'
                 '    500 \r\n'
                 '\r\n'
                 '/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py '
                 'in _model_iteration(self, model, mode, x, y, batch_size, verbose, '
                 'sample_weight, steps, callbacks, max_queue_size, workers, '
                 'use_multiprocessing, **kwargs)\r\n'
                 '    473               mode=mode,\r\n'
                 '    474               training_context=training_context,\r\n'
                 '--> 475               total_epochs=1)\r\n'
                 '    476           cbks.make_logs(model, epoch_logs, result, '
                 'mode)\r\n'
                 '    477 \r\n'
                 '\r\n'
                 '/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py '
                 'in run_one_epoch(model, iterator, execution_function, dataset_size, '
                 'batch_size, strategy, steps_per_epoch, num_samples, mode, '
                 'training_context, total_epochs)\r\n'
                 '    177             batch_outs,\r\n'
                 '    178             batch_start=step * batch_size,\r\n'
                 '--> 179             batch_end=step * batch_size + '
                 'current_batch_size)\r\n'
                 '    180       cbks.make_logs(model, batch_logs, batch_outs, '
                 'mode)\r\n'
                 '    181       step += 1\r\n'
                 '\r\n'
                 '/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py '
                 'in aggregate(self, batch_outs, batch_start, batch_end)\r\n'
                 '    345     batch_outs = nest.flatten_up_to(self._structure, '
                 'batch_outs)\r\n'
                 '    346     for batch_element, result in zip(batch_outs, '
                 'self.results):\r\n'
                 '--> 347       result.aggregate(batch_element, batch_start, '
                 'batch_end)\r\n'
                 '    348 \r\n'
                 '    349   def finalize(self):\r\n'
                 '\r\n'
                 '/usr/src/.venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py '
                 'in aggregate(self, batch_element, batch_start, batch_end)\r\n'
                 '    278     num_elements = np.prod(batch_element.shape)\r\n'
                 '    279     if num_elements < self._BINARY_SIZE_THRESHOLD:\r\n'
                 '--> 280       self.results[batch_start:batch_end] = '
                 'batch_element\r\n'
                 '    281     else:\r\n'
                 '    282       is_finished = threading.Event()\r\n'
                 '\r\n'
                 'ValueError: could not broadcast input array from shape (1,46) into '
                 'shape (1,48)\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution : Windows 10\r\n'
                 '- TensorFlow installed from (source or binary): Source\r\n'
                 '- TensorFlow version: 2.1\r\n'
                 '- Python version: 3.7\r\n'
                 '- Bazel version (if compiling from source): 0.29.1\r\n'
                 '- GCC/Compiler version (if compiling from source): VS2019\r\n'
                 '- CUDA/cuDNN version: 10.1 / 7.6\r\n'
                 '- GPU model and memory: 2070 Max Q\r\n'
                 '\r\n'
                 'I setup it using configure with the following options selected '
                 ': \r\n'
                 '- XLA JIT\r\n'
                 '- Cuda\r\n'
                 '- /arch:AVX\r\n'
                 '- Eigen strong inline overridden\r\n'
                 '\r\n'
                 'When attempting to build from source using the following '
                 'command. \r\n'
                 '\r\n'
                 'bazel build --config=opt --config=cuda '
                 '--define=no_tensorflow_py_deps=true '
                 '//tensorflow:libtensorflow.so\r\n'
                 '\r\n'
                 'I get the following error.\r\n'
                 '\r\n'
                 'ERROR: '
                 'C:/sdks/tensorflow/tensorflow/compiler/xla/service/gpu/BUILD:1616:1: '
                 'C++ compilation of rule '
                 "'//tensorflow/compiler/xla/service/gpu:hlo_algorithm_blacklist' "
                 'failed (Exit 2)\r\n'
                 'cl : Command line warning D9002 : ignoring unknown option '
                 "'-std=c++14'\r\n"
                 'tensorflow/compiler/xla/service/gpu/hlo_algorithm_blacklist.cc(28): '
                 'error C2131: expression did not evaluate to a constant\r\n'
                 'external/com_google_absl\\absl/strings/string_view.h(186): note: a '
                 'non-constant (sub-)expression was encountered\r\n',
         'created_at': '2020-01-1'},
        {'body': '@tensorflow/micro\r\n'
                 '\r\n'
                 'Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 'NAME="Ubuntu"\r\n'
                 'VERSION="18.04.3 LTS"\r\n'
                 'ID=ubuntu\r\n'
                 'PRETTY_NAME="Ubuntu 18.04.3 LTS"\r\n'
                 'VERSION_ID="18.04"\r\n'
                 'Python versionL 2.7.15+\r\n'
                 'Target platform : K64F\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'I have build a GRU model in python and converted to a bin format '
                 'for ARM K64F board. When I flash the binary I get the following '
                 'error message in the serial console.  How can I fix it? Any help is '
                 'appreciated.\r\n'
                 '\r\n'
                 '`Failed to allocate memory for tensor_info, 1320 bytes required\r\n'
                 'AllocateTensors() failed\r\n'
                 '\r\n'
                 '++ MbedOS Fault Handler ++\r\n'
                 '\r\n'
                 'FaultType: HardFault\r\n'
                 '\r\n'
                 'Context:\r\n'
                 'R0   : 00000000\r\n'
                 'R1   : 00000000\r\n'
                 'R2   : 00000000\r\n'
                 'R3   : 20013FE8\r\n'
                 'R4   : 00000000\r\n'
                 'R5   : 00000000\r\n'
                 'R6   : 00000000\r\n'
                 'R7   : 20002368\r\n'
                 'R8   : 20002364\r\n'
                 'R9   : 00000000\r\n'
                 'R10  : 00000000\r\n'
                 'R11  : 00000000\r\n'
                 'R12  : 000000B0\r\n'
                 'SP   : 20013F10\r\n'
                 'LR   : 00008E89\r\n'
                 'PC   : 000010E4\r\n'
                 'xPSR : 61070000\r\n'
                 'PSP  : 20013EA8\r\n'
                 'MSP  : 2002FFC0\r\n'
                 'CPUID: 410FC241\r\n'
                 'HFSR : 40000000\r\n'
                 'MMFSR: 00000000\r\n'
                 'BFSR : 00000004\r\n'
                 'UFSR : 00000000\r\n'
                 'DFSR : 00000008\r\n'
                 'AFSR : 00000000\r\n'
                 'Mode : Thread\r\n'
                 'Priv : Privileged\r\n'
                 'Stack: PSP\r\n'
                 '\r\n'
                 '-- MbedOS Fault Handler --\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '++ MbedOS Error Info ++\r\n'
                 'Error Status: 0x80FF013D Code: 317 Module: 255\r\n'
                 'Error Message: Fault exception\r\n'
                 'Location: 0x10E4\r\n'
                 'Error Value: 0x1FFF0400\r\n'
                 'Current Thread: main Id: 0x200013E8 Entry: 0x3F07 StackSize: '
                 '0x10000 StackMem: 0x20004010 SP: 0x20013F10\r\n'
                 'For more info, visit: '
                 'https://mbed.com/s/error?error=0x80FF013D&tgt=K64F\r\n'
                 '-- MbedOS Error Info --\r\n'
                 '\r\n'
                 '= System will be rebooted due to a fatal error =\r\n'
                 '= Reboot count(=7) reached maximum, system will halt after '
                 'rebooting\r\n'
                 '`\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Custom code (as opposed to using a stock example script provided '
                 'in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google '
                 'Colab\r\n'
                 '- TensorFlow installed from (source or binary): use of '
                 '`%tensorflow_version 2.x`\r\n'
                 '- TensorFlow version (use command below): 2.1.0-rc1\r\n'
                 '- Python version: 3.6.9\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 "I tried to run @huan's Google Colab Notebook mentioned "
                 '[here](https://stackoverflow.com/questions/55541881/how-to-convert-tf-keras-model-to-tpu-using-tensorflow-2-0-in-google-colab/55686370#55686370) '
                 'and available '
                 '[here](https://colab.research.google.com/github/huan/tensorflow-handbook-tpu/blob/master/tensorflow-handbook-tpu-example.ipynb). '
                 'It raises the following error:\r\n'
                 '\r\n'
                 '```\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'InternalError                             Traceback (most recent '
                 'call last)\r\n'
                 '<ipython-input-7-79d308ea228d> in <module>()\r\n'
                 '      4   steps_per_epoch=60,\r\n'
                 '      5   validation_data=(x_test.astype(np.float32), '
                 'y_test.astype(np.float32)),\r\n'
                 '----> 6   validation_freq=5\r\n'
                 '      7 )\r\n'
                 '      8 \r\n'
                 '\r\n'
                 '11 frames\r\n'
                 '/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, '
                 'from_value)\r\n'
                 '\r\n'
                 'InternalError: Assigned device '
                 "'/job:worker/replica:0/task:0/device:TPU:0' does not have "
                 'registered OpKernel support for _Arg\r\n'
                 '\t [[{{node iteratorgetnext_iterator}}]] '
                 '[Op:__inference_distributed_function_2822]\r\n'
                 '```\r\n'
                 '\r\n'
                 'I tried `tf.compat.v1.disable_eager_execution()` as discussed '
                 '[here](https://github.com/huan/tensorflow-handbook-tpu/issues/1) '
                 "but it doesn't work:\r\n"
                 '\r\n'
                 '```\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'RuntimeError                              Traceback (most recent '
                 'call last)\r\n'
                 '<ipython-input-15-5118e7c1b79a> in <module>()\r\n'
                 '      6   steps_per_epoch=60,\r\n'
                 '      7   validation_data=(x_test.astype(np.float32), '
                 'y_test.astype(np.float32)),\r\n'
                 '----> 8   validation_freq=5\r\n'
                 '      9 )\r\n'
                 '     10 \r\n'
                 '\r\n'
                 '25 frames\r\n'
                 '/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py '
                 'in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, '
                 'dtype_hint, ctx, accepted_result_types)\r\n'
                 '   1278       graph = get_default_graph()\r\n'
                 '   1279       if not graph.building_function:\r\n'
                 '-> 1280         raise RuntimeError("Attempting to capture an '
                 'EagerTensor without "\r\n'
                 '   1281                            "building a function.")\r\n'
                 '   1282       return graph.capture(value, name=name)\r\n'
                 '\r\n'
                 'RuntimeError: Attempting to capture an EagerTensor without building '
                 'a function.\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 "Not sure if @huan's example should be working given what he "
                 'explained on '
                 '[stackoverflow](https://stackoverflow.com/questions/55541881/how-to-convert-tf-keras-model-to-tpu-using-tensorflow-2-0-in-google-colab/55686370#55686370), '
                 'but I think it should with TF 2.1.0 version.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 "Again, @huan's work available "
                 '[here](https://colab.research.google.com/github/huan/tensorflow-handbook-tpu/blob/master/tensorflow-handbook-tpu-example.ipynb).\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'I had the same issue with personal code and thought first it was '
                 'due to `ImageDataGenerator` (see my last comment '
                 '[here](https://github.com/tensorflow/tensorflow/issues/34346)), '
                 "however it seems not to be as @huan's example doesn't use it.\r\n"
                 "I have the same issue on @huan's code with TF 2.1.0 installed with "
                 '`!pip install tensorflow==2.1.0` in his notebook.',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 'Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 'Ubuntu18.04 and Raspbian 10\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 'Raspberry Pi 4\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 'Binary\r\n'
                 '- TensorFlow version (use command below):\r\n'
                 'for Ubuntu            v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n'
                 'for Raspberry Pi     v1.12.1-14948-g43dcb71 1.14.0\r\n'
                 '- Python version:\r\n'
                 '3.7\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'The tflite only utilize single core of CPU. I test it on PC and '
                 'raspberry Pi 4.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '`interpreter = tf.lite.Interpreter(model_content=tflite_model) \r\n'
                 '\r\n'
                 'input_details = interpreter.get_input_details() \r\n'
                 '\r\n'
                 'output_details = interpreter.get_output_details() \r\n'
                 '\r\n'
                 'interpreter.allocate_tensors() \r\n'
                 '\r\n'
                 "interpreter.set_tensor(input_details[0]['index'], inp) \r\n"
                 '\r\n'
                 'interpreter.invoke() \r\n'
                 '\r\n'
                 "result = interpreter.get_tensor(output_details[0]['index'])`\r\n"
                 '\r\n'
                 'The model I use is here:\r\n'
                 '[1578756226.tar.gz](https://github.com/tensorflow/tensorflow/files/4050550/1578756226.tar.gz)\r\n',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): see below\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows, '
                 'Linux\r\n'
                 '- TensorFlow installed from (source or binary): pip\r\n'
                 '- TensorFlow version (use command below): 2.1.0\r\n'
                 '- Python version: 3.7.6\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'In this example, no error is raised although session kwargs are not '
                 'supported in eager mode. kwargs are simply silently ignored.\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 'from tensorflow import keras\r\n'
                 '\r\n'
                 'fetches = [lambda: whatever_I_write_here_is_ignored]\r\n'
                 '\r\n'
                 'var = tf.Variable([[3.0]])\r\n'
                 'model = keras.models.Sequential([keras.layers.Dense(1, '
                 'input_shape=(1,))])\r\n'
                 'model.compile(loss="mse", optimizer="adam")\r\n'
                 'model._function_kwargs = {"fetches": fetches, "should_fail": '
                 '"ignored_as_well"}\r\n'
                 '\r\n'
                 'model.fit([[7.0]], [[9.0]], epochs=2)\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'An error should be raised. I am aware that `model._function_kwargs` '
                 'is not part of the public API, but `keras` (as opposed to '
                 '`tf.keras`) does raise an error here:\r\n'
                 '```\r\n'
                 'import keras\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'fetches = [lambda: whatever_I_write_here_is_ignored]\r\n'
                 '\r\n'
                 'var = tf.Variable([[3.0]])\r\n'
                 'model = keras.models.Sequential([keras.layers.Dense(1, '
                 'input_shape=(1,))])\r\n'
                 'model.compile(loss="mse", optimizer="adam")\r\n'
                 'model._function_kwargs = {"fetches": fetches, "should_fail": '
                 '"ignored_as_well"}\r\n'
                 '\r\n'
                 'model.fit([[7.0]], [[9.0]], epochs=2)\r\n'
                 '```\r\n'
                 '\r\n'
                 'outputs\r\n'
                 '```\r\n'
                 'Exception has occurred: ValueError\r\n'
                 'Session keyword arguments are not support during eager execution. '
                 "You passed: {'fetches': [<function <lambda> at 0x7f94681be3b0>], "
                 "'should_fail': 'ignored_as_well'}\r\n"
                 '  File '
                 '"/home/bersbersbers/.pyenv/versions/3.7.6/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py", '
                 'line 3759, in function\r\n'
                 '```\r\n'
                 'Somewhat related: '
                 'https://github.com/tensorflow/tensorflow/issues/34448',
         'created_at': '2020-01-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): see below\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10, Linux\r\n'
                 '- TensorFlow installed from (source or binary): pip\r\n'
                 '- TensorFlow version (use command below): 2.1.0\r\n'
                 '- Python version: 3.7.6\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'A special assign operation does nothing (not even raising an '
                 'error).\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'At least an error should be raised.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 'from tensorflow import keras\r\n'
                 '\r\n'
                 '# tf.config.experimental_run_functions_eagerly(True)\r\n'
                 'var = tf.Variable([[3.0]])\r\n'
                 'model = keras.Sequential([keras.layers.Dense(1, '
                 'input_shape=(1,))])\r\n'
                 'model.compile(loss="mse", optimizer="adam")\r\n'
                 '\r\n'
                 'tf.print(var)  # should print 3, OK\r\n'
                 'var.assign(model.inputs[0])\r\n'
                 'tf.print(var)  # should print anything else but 3, or raise an '
                 'error - but prints 3\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '```\r\n'
                 '>>> tf.print(var)  # should print 3, OK\r\n'
                 '[[3]]\r\n'
                 '>>> var.assign(model.inputs[0])\r\n'
                 "<tf.Variable 'UnreadVariable' shape=(1, 1) dtype=float32, "
                 'numpy=array([[3.]], dtype=float32)>\r\n'
                 '>>> tf.print(var)  # should print anything else but 3, or raise an '
                 'error - fail.\r\n'
                 '[[3]]\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': "I'm trying to [build tensorflow from "
                 'source](https://www.tensorflow.org/install/source) following the '
                 'Docker instructions:\r\n'
                 '>docker pull tensorflow/tensorflow:devel\r\n'
                 '\r\n'
                 '>docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS="$(id '
                 '-u):$(id -g)" \\\r\n'
                 '    tensorflow/tensorflow:devel bash\r\n'
                 '>\r\n'
                 '>git pull  # within the container, download the latest source '
                 'code\r\n'
                 '\r\n'
                 'Here are the commands I run in the terminal (on Ubuntu), along with '
                 'their output:\r\n'
                 '```\r\n'
                 '$ docker --version\r\n'
                 'Docker version 19.03.2, build 6a30dfc\r\n'
                 '\r\n'
                 '$ docker pull tensorflow/tensorflow:devel\r\n'
                 'devel: Pulling from tensorflow/tensorflow\r\n'
                 'Digest: '
                 'sha256:0ee065743f0001f922561bcba914013929a88263ec2a5af21ba35899c3ac85a7\r\n'
                 'Status: Image is up to date for tensorflow/tensorflow:devel\r\n'
                 'docker.io/tensorflow/tensorflow:devel\r\n'
                 '\r\n'
                 '$ docker run -it -w /tensorflow -v $PWD:/mnt -e HOST_PERMS="$(id '
                 '-u):$(id -g)" \\\r\n'
                 '>     tensorflow/tensorflow:devel bash\r\n'
                 '\r\n'
                 '________                               '
                 '_______________                \r\n'
                 '___  __/__________________________________  ____/__  /________      '
                 '__\r\n'
                 '__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | '
                 '/| / /\r\n'
                 '_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ '
                 '/ \r\n'
                 '/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  '
                 '\\____/____/|__/\r\n'
                 '\r\n'
                 '\r\n'
                 'WARNING: You are running this container as root, which can cause '
                 'new files in\r\n'
                 'mounted volumes to be created as the root user on your host '
                 'machine.\r\n'
                 '\r\n'
                 "To avoid this, run the container by specifying your user's "
                 'userid:\r\n'
                 '\r\n'
                 '$ docker run -u $(id -u):$(id -g) args...\r\n'
                 '\r\n'
                 'root@4746a002f18e:/tensorflow# \r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 'But now, if I run `git pull` as instructed, I get\r\n'
                 '```\r\n'
                 'fatal: not a git repository (or any of the parent directories): '
                 '.git\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': '\r\n'
                 'bazel 1.1.0\r\n'
                 '\r\n'
                 '```console\r\n'
                 '➜  tensorflow git:(master) ✗ bazel version\r\n'
                 'Build label: 1.1.0- (@non-git)\r\n'
                 'Build target: '
                 'bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n'
                 'Build time: Wed. Jan. 8 01:20:59 2020 (1578446459)\r\n'
                 'Build timestamp: 1578446459\r\n'
                 'Build timestamp as int: 1578446459\r\n'
                 '```\r\n'
                 '\r\n'
                 'failed to build tensorflow master.\r\n'
                 '```console\r\n'
                 'INFO: From Compiling '
                 'tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc '
                 '[for host]:\r\n'
                 'tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc: '
                 "In member function 'virtual tensorflow::Status "
                 'toco::PropagateArrayDataTypes::Run(toco::Model*, std::size_t, '
                 "bool*)':\r\n"
                 'tensorflow/lite/toco/graph_transformations/propagate_array_data_types.cc:173:25: '
                 'warning: comparison between signed and unsigned integer expressions '
                 '[-Wsign-compare]\r\n'
                 '       for (int i = 0; i < op->outputs.size(); ++i) {\r\n'
                 '                       ~~^~~~~~~~~~~~~~~~~~~~\r\n'
                 'ERROR: ~/....../tensorflow/python/keras/api/BUILD:130:1: Executing '
                 'genrule '
                 '//tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed '
                 '(Exit 1)\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py", '
                 'line 776, in <module>\r\n'
                 '    main()\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py", '
                 'line 772, in main\r\n'
                 '    lazy_loading, args.use_relative_imports)\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py", '
                 'line 629, in create_api_files\r\n'
                 '    compat_api_versions, lazy_loading, use_relative_imports)\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py", '
                 'line 503, in get_api_init_text\r\n'
                 '    _, attr = tf_decorator.unwrap(attr)\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py", '
                 'line 219, in unwrap\r\n'
                 '    elif _has_tf_decorator_attr(cur):\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py", '
                 'line 124, in _has_tf_decorator_attr\r\n'
                 "    hasattr(obj, '_tf_decorator') and\r\n"
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py", '
                 'line 62, in __getattr__\r\n'
                 '    module = self._load()\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py", '
                 'line 45, in _load\r\n'
                 '    module = importlib.import_module(self.__name__)\r\n'
                 '  File "/usr/lib/python3.6/importlib/__init__.py", line 126, in '
                 'import_module\r\n'
                 '    return _bootstrap._gcd_import(name[level:], package, level)\r\n'
                 '  File "<frozen importlib._bootstrap>", line 994, in _gcd_import\r\n'
                 '  File "<frozen importlib._bootstrap>", line 971, in '
                 '_find_and_load\r\n'
                 '  File "<frozen importlib._bootstrap>", line 955, in '
                 '_find_and_load_unlocked\r\n'
                 '  File "<frozen importlib._bootstrap>", line 665, in '
                 '_load_unlocked\r\n'
                 '  File "<frozen importlib._bootstrap_external>", line 678, in '
                 'exec_module\r\n'
                 '  File "<frozen importlib._bootstrap>", line 219, in '
                 '_call_with_frames_removed\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py", '
                 'line 28, in <module>\r\n'
                 '    _wrap_py_utils = swig_import_helper()\r\n'
                 '  File '
                 '"~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py", '
                 'line 24, in swig_import_helper\r\n'
                 "    _mod = imp.load_module('_wrap_py_utils', fp, pathname, "
                 'description)\r\n'
                 '  File "/usr/lib/python3.6/imp.py", line 243, in load_module\r\n'
                 '    return load_dynamic(name, filename, file)\r\n'
                 '  File "/usr/lib/python3.6/imp.py", line 343, in load_dynamic\r\n'
                 '    return _load(spec)\r\n'
                 '  File "<frozen importlib._bootstrap>", line 684, in _load\r\n'
                 '  File "<frozen importlib._bootstrap>", line 658, in '
                 '_load_unlocked\r\n'
                 '  File "<frozen importlib._bootstrap>", line 571, in '
                 'module_from_spec\r\n'
                 '  File "<frozen importlib._bootstrap_external>", line 922, in '
                 'create_module\r\n'
                 '  File "<frozen importlib._bootstrap>", line 219, in '
                 '_call_with_frames_removed\r\n'
                 'ImportError: '
                 '~/.cache/bazel/_bazel_longervision/449dd09be7229482c168e6868d06849c/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: '
                 'undefined symbol: '
                 '_ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\n'
                 'Target //tensorflow/tools/pip_package:build_pip_package failed to '
                 'build\r\n'
                 'Use --verbose_failures to see the command lines of failed build '
                 'steps.\r\n'
                 'ERROR: ~/....../tensorflow/python/tools/BUILD:81:1 Executing '
                 'genrule '
                 '//tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed '
                 '(Exit 1)\r\n'
                 'INFO: Elapsed time: 10740.244s, Critical Path: 307.06s\r\n'
                 'INFO: 15793 processes: 15793 local.\r\n'
                 'FAILED: Build did NOT complete successfully\r\n'
                 '```',
         'created_at': '2020-01-1'},
        {'body': 'Tf Version: 2.0.0\r\n'
                 'Python: 3.7.5\r\n'
                 'IDE: Spyder\r\n'
                 'OS: Windows\r\n'
                 '\r\n'
                 'I wrote a subclass of custom loss function and tried to list it in '
                 'the arguments of a checkpoint as follows. The model is an arbitrary '
                 'one\r\n'
                 '```\r\n'
                 'class SoftDiceLoss(tf.keras.losses.Loss):\r\n'
                 "    '''\r\n"
                 '    SoftDiceLoss calculates multi-class soft dice loss\r\n'
                 '    loss = '
                 'avg_batch(1-(sum(W_k*sum(yPred.*yTrue)))/(sum(W_ksum(yPred^2+yTrue^2))))\r\n'
                 '    where W_k = 1/(number of voxels in class k)^wPow\r\n'
                 '    Class number of segmented regions includes background\r\n'
                 '    Args:\r\n'
                 '        yPred/yTrue: prediced and desired outputs shaped as '
                 '[mbSize, classNum, tensor dimensions]. Also, both must be '
                 'float-point\r\n'
                 '    \twPow, power of weiight. A higher one favours classes with a '
                 'smaller number of voxels\r\n'
                 '    Return:\r\n'
                 '        loss: a scalar tensor\r\n'
                 "    '''\r\n"
                 "    def __init__(self, wPow=2.0, name='SoftDiceLoss'):\r\n"
                 '        super().__init__(name=name)\r\n'
                 '        self.epsilon = 1e-16 \r\n'
                 '        self.wPow = wPow\r\n'
                 '\r\n'
                 '    def call(self, yPred, yTrue):\r\n'
                 '        yTrue =tf.dtypes.cast(yTrue, dtype=yPred.dtype)\r\n'
                 '\t\t# Dot product yPred and yTrue and sum them up for each datum '
                 'and class\r\n'
                 '        crossProd=tf.multiply(yPred, yTrue)\r\n'
                 '\t\t# As a symbolic tensor, dimensions and shapes etc. cannot be '
                 'extracted from data, nor can it be used in subroutines.\r\n'
                 '        crossProdSum=tf.math.reduce_sum(crossProd, '
                 'axis=np.arange(2, 5)) #tf.rank(yTrue)))\r\n'
                 '\t\t# Calculate weight for each datum and class \r\n'
                 '        weight = tf.math.reduce_sum(yTrue, axis=np.arange(2, '
                 '5))#tf.rank(yTrue)))\r\n'
                 '\t\t#weight = tf.math.divide(1, '
                 'tf.math.square(weight)+self.epsilon)\r\n'
                 '        weight = tf.math.divide(1, tf.math.pow(weight, '
                 'self.wPow)+self.epsilon)\r\n'
                 '\t\t# Weighted sum over classes\r\n'
                 '        numerator = 2*tf.math.reduce_sum(tf.multiply(crossProdSum, '
                 'weight), axis=1)\r\n'
                 '\t\t# Saquared summation \r\n'
                 '        yySum = tf.math.reduce_sum(tf.math.square(yPred) + '
                 'tf.math.square(yTrue), axis=np.arange(2, 5))#tf.rank(yTrue)))\r\n'
                 '\t\t# Weighted sum over classes\r\n'
                 '        denominator = tf.math.reduce_sum(tf.multiply(weight, '
                 'yySum), axis=1)\r\n'
                 '\t\t# Get individual loss and average over minibatch\r\n'
                 '        loss = tf.math.reduce_mean(1 - tf.math.divide(numerator, '
                 'denominator+self.epsilon))\r\n'
                 '\t\t\t\r\n'
                 '        return loss\r\n'
                 '    \r\n'
                 '    def get_config(self):\r\n'
                 '        config = super(SoftDiceLoss, self).get_config()\r\n'
                 '        return config\r\n'
                 '\r\n'
                 'curOpt = tf.keras.optimizers.Adam(learning_rate=1e-4)\t\r\n'
                 'lossFunc=SoftDiceLoss(2.0)\r\n'
                 'ckpt = tf.train.Checkpoint(model=myModel(...), optimizer=curOpt, '
                 'lossFunc=lossFunc, accFunc=accFunc)\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 'I got the following error\r\n'
                 '```\r\n'
                 'ckpt = tf.train.Checkpoint(model=myModel(...), optimizer=curOpt, '
                 'lossFunc=lossFunc, accFunc=accFunc)\r\n'
                 'Traceback (most recent call last):\r\n'
                 '\r\n'
                 '  File "<ipython-input-17-a4d9163bdda3>", line 2, in <module>\r\n'
                 '    optimizer=curOpt, lossFunc=lossFunc, accFunc=accFunc)\r\n'
                 '\r\n'
                 '  File '
                 '"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py", '
                 'line 1779, in __init__\r\n'
                 '    % (v,))\r\n'
                 '\r\n'
                 'ValueError: `Checkpoint` was expecting a trackable object (an '
                 'object derived from `TrackableBase`), got <SoftDiceLoss object at '
                 '0x0000000011F6F7C8>. If you believe this object should be trackable '
                 '(i.e. it is part of the TensorFlow Python API and manages state), '
                 'please open an issue.\r\n'
                 '```\r\n'
                 'I am not sure if a subclass is trackable or not. Is that an '
                 'intention of design? Or shall such a feature be added? \r\n'
                 'By the way, if I change the subclass into a normal function, it '
                 'works fine. Actually, my accFunc(...) is just a normal function.',
         'created_at': '2020-01-1'},
        {'body': 'Hello.\xa0\r\n'
                 'I have an issue similar to\xa0#29524.\r\n'
                 '\r\n'
                 "I'm trying to generate a Keil project for the microcontroller I'm "
                 'developing.\r\n'
                 'When I was following procedures in the following link, I got the '
                 'error:\r\n'
                 '\r\n'
                 'https://www.tensorflow.org/lite/microcontrollers/library\r\n'
                 '\r\n'
                 'System informationOS Platform and Distribution (e.g., Linux Ubuntu '
                 '16.04):\xa0macOS Catalina 10.15.2 (19C57)\r\n'
                 'TensorFlow installed from (source or binary): source\r\n'
                 'TensorFlow version:\xa0e689e24\r\n'
                 'GCC/Compiler version (if compiling from source):\xa0Apple clang '
                 'version 11.0.0 (clang-1100.0.33.16)\r\n'
                 '\r\n'
                 'Describe the problem\r\n'
                 'When running$ gmake -f '
                 'tensorflow/lite/experimental/micro/tools/Makefile generate_projects '
                 'results in the following error:\r\n'
                 '\r\n'
                 'gmake: *** No rule to make target '
                 "'tensorflow/lite/micro/tools/make/gen/osx_x86_64/prj/micro_speech/make/tensorflow/lite/experimental/micro/examples/micro_speech/simple_features/simple_model_settings.h', "
                 "needed by 'generate_micro_speech_make_project'.\xa0 Stop.\r\n",
         'created_at': '2020-01-1'},
        {'body': '## URL(s) with the issue:\r\n'
                 'https://www.tensorflow.org/datasets/api_docs/python/tfds/load\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 'Add a warning that `tfds.load()` can not be used for the users own '
                 'Datasets, i.e. that he creates himself. To a new user trying to to '
                 'load a Dataset from a set of files it is not obvious that this '
                 'method is only for pre-made, immutable Datasets.\r\n'
                 '\r\n'
                 'Although it does say\r\n'
                 '> Loads the named dataset into a tf.data.Dataset.\r\n'
                 '\r\n'
                 'i initially interpreted it such that my own Dataset can be assigned '
                 'a name. \r\n'
                 '\r\n'
                 'I was looking for a way to split a Dataset into train and '
                 'validation subsets and stumbled upon this documentation. I was '
                 'redirected from https://www.tensorflow.org/datasets/splits which '
                 'comes up as one of the most prominent search results when searching '
                 'for "tensorflow Dataset splits" .\r\n'
                 '\r\n'
                 '## Result\r\n'
                 'A user who visits '
                 'https://www.tensorflow.org/datasets/api_docs/python/tfds/load will '
                 'not spend 1 h of trying to understand all the documentation but '
                 'will immediately realize that this is only for immutable pre-made '
                 'Datasets.',
         'created_at': '2020-01-1'},
        {'body': 'This is on TF2.1 from pip on Windows 10.\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '```\r\n'
                 'import tensorflow.compat.v1 as tf1\r\n'
                 '\r\n'
                 'tf1.random.set_random_seed(0)\r\n'
                 'tf1.disable_eager_execution()\r\n'
                 '\r\n'
                 'print(tf1.keras.backend.get_session().run(tf1.random.uniform((), 0, '
                 '1)))\r\n'
                 '```\r\n'
                 '\r\n'
                 'This prints a different number every time. I have a similar example '
                 'with more TF2-relevant code, where the same thing happens:\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 'import tensorflow.compat.v1 as tf1\r\n'
                 '\r\n'
                 'tf.random.set_seed(0)\r\n'
                 'tf1.disable_eager_execution()\r\n'
                 '\r\n'
                 'print(tf1.keras.backend.get_session().run(tf.random.uniform((), 0, '
                 '1)))\r\n'
                 '```\r\n'
                 '\r\n'
                 'A workaround is to set the seed after `disable_eager_execution`.',
         'created_at': '2020-01-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS '
                 '10.14.5\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: N/A\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (use command below): 2.1.0\r\n'
                 '- Python version: 3.6.8\r\n'
                 '- Bazel version (if compiling from source): N/A\r\n'
                 '- GCC/Compiler version (if compiling from source): N/A\r\n'
                 '- CUDA/cuDNN version: N/A\r\n'
                 '- GPU model and memory:  N/A\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'See the code:\r\n'
                 '\r\n'
                 '```\r\n'
                 'import numpy as np\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'm = 3\r\n'
                 'n = 3\r\n'
                 'x = tf.cast(np.random.randn(1, m, 32), tf.float32)\r\n'
                 'z = tf.tile(x, [n, 1, 1])\r\n'
                 '\r\n'
                 'layer = tf.keras.layers.Dense(32)\r\n'
                 'w = layer(z)\r\n'
                 '\r\n'
                 'tf.print(tf.norm(z[0, :, :] - z[1, :, :]), tf.norm(z[1, :, :] - '
                 'z[n-1, :, :]))\r\n'
                 'tf.print(tf.norm(w[0, :, :] - w[1, :, :]), tf.norm(w[1, :, :] - '
                 'w[n-1, :, :]))\r\n'
                 '```\r\n'
                 'In the code we replicate the input `x` 3 times and apply a dense '
                 'layer upon it. We expect to get the same results for the 3 '
                 'replicates. In fact the 1st and 2nd results are indeed same, while '
                 'the 3rd result is different. Here is the results of the script '
                 'above:\r\n'
                 '\r\n'
                 '```\r\n'
                 '0 0\r\n'
                 '0 1.0617149e-06 # this error is not fixed for each run\r\n'
                 '```\r\n'
                 'where we expect all results to be 0.\r\n'
                 '\r\n'
                 'Strangely enough, this bug only appears for some `(m,n)` pairs (in '
                 'the example above `m=n=3`). I ran the code for all `m` and `n` from '
                 '1 to 100 and found that there are ~40% combinations that will cause '
                 "a bug, but I didn't find any obvious pattern...",
         'created_at': '2020-01-1'},
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c05_forecasting_with_machine_learning.ipynb\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 'Window size should be 30 instead of 20 in the description under '
                 '"Forecasting With Machine Learning".\r\n'
                 '\r\n'
                 '### Clear description\r\n'
                 '\r\n'
                 '![Screenshot from 2020-01-10 '
                 '12-05-13](https://user-images.githubusercontent.com/29497701/72131252-b1905980-33a1-11ea-8cf5-11d089d5316e.png)\r\n'
                 '\r\n'
                 '--------------------\r\n'
                 'As we can see under "Linear Model", `window_size=30` while in '
                 'description it is mentioned as model forecasts, given previous 20 '
                 'steps.\r\n'
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 'Yes...',
         'created_at': '2020-01-1'},
        {'body': "I've reviewed one design in-depth, and two others superficially, "
                 "but Keras/TF's `recurrent_dropout` does not implement _any_ of "
                 'them; publication links below.\r\n'
                 '\r\n'
                 " 1. I see some potentially severe problems with TF's implementation "
                 "in light of the papers I've read, which explicitly advocate against "
                 "the used scheme. This said - what is TensorFlow / Keras's "
                 'justification / rationale  of its own implementation?\r\n'
                 '\r\n'
                 ' 2. The implementation is inconsistent - see below; the docstring '
                 "only mentions a performance difference, but there's also a "
                 '_reproducibility_ and _design_ difference; `==1` uses _different '
                 'masks_ per gate, whereas `==2` uses a _shared mask_. The two are '
                 'neither theoretically nor practically identical. \r\n'
                 '\r\n'
                 "Second's fixable via a docstring, but first involves significant "
                 'changes to recurrent dropout logic for `LSTM`, `GRU`, and maybe '
                 'other RNNs. This said: **is TensorFlow / Keras open to changing its '
                 'base implementations of recurrent dropout?** If so, I can go ahead '
                 'and clarify **(1)** in detail, and maybe even do the '
                 're-implementing myself in a PR, per paper 1.\r\n'
                 '\r\n'
                 '<hr>\r\n'
                 '\r\n'
                 '**Inconsistency**: `implementation==1` vs. `implementation==2`\r\n'
                 '\r\n'
                 '```python\r\n'
                 'if 0. < self.recurrent_dropout < 1.:  # implementation==1\r\n'
                 '    h_tm1_i = h_tm1 * rec_dp_mask[0]\r\n'
                 '    h_tm1_f = h_tm1 * rec_dp_mask[1]\r\n'
                 '    h_tm1_c = h_tm1 * rec_dp_mask[2]\r\n'
                 '    h_tm1_o = h_tm1 * rec_dp_mask[3]\r\n'
                 '```\r\n'
                 '```python\r\n'
                 'if 0. < self.recurrent_dropout < 1.:  # implementation==2\r\n'
                 '    h_tm1 *= rec_dp_mask[0]\r\n'
                 '```\r\n'
                 'Source codes: '
                 '[keras](https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L2014) '
                 '-- '
                 '[tf.keras](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L2391)\r\n'
                 '\r\n'
                 '<hr>\r\n'
                 '\r\n'
                 '**Publications**:\r\n'
                 '\r\n'
                 ' 1. [Recurrent Dropout without Memory '
                 'Loss](https://arxiv.org/abs/1603.05118)\r\n'
                 ' 2. [A Theoretically Grounded Application of Dropout in Recurrent '
                 'Neural Networks](https://arxiv.org/abs/1512.05287)\r\n'
                 ' 3. [RNNDrop: A novel dropout for '
                 'RNNs](https://sci-hub.se/10.1109/ASRU.2015.7404775)',
         'created_at': '2020-01-1'},
        {'body': 'Added a usage example to tf.keras.layers.Dropout',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: N/A\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version: 2.1.0\r\n'
                 '- Python version: 3.6.10\r\n'
                 '- Installed using virtualenv? pip? conda?: pip\r\n'
                 '- Bazel version (if compiling from source): N/A\r\n'
                 '- GCC/Compiler version (if compiling from source): N/A\r\n'
                 '- CUDA/cuDNN version: N/A\r\n'
                 '- GPU model and memory: N/A\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 'TensorFlow 2.1.0 added `scipy` as a required dependency.  However, '
                 "as far as I can tell TensorFlow doesn't actually need scipy. "
                 'Judging from this PR '
                 'https://github.com/tensorflow/tensorflow/pull/35278 (which '
                 'introduced the requirement), the intention was just to avoid a bug '
                 'with scipy==1.4.0. But it seems that simply not having scipy '
                 'installed would be another way to avoid that bug, and adding scipy '
                 'as a requirement to every tensorflow installation seems like kind '
                 "of a drastic solution. It also wasn't mentioned in the release "
                 "notes at all, so I'm wondering whether this was an intentional "
                 'change or not.\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '\r\n'
                 '`pip install tensorflow`\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Ubuntu 16.04:\r\n'
                 '- TensorFlow installed from (source or binary): pip binary\r\n'
                 '- TensorFlow version (use command below): 1.13.1\r\n'
                 '- Python version: 3.5.2\r\n'
                 '- CUDA/cuDNN version: 10.0 / 7.5\r\n'
                 '- GPU model and memory: NVIDIA RTX 2080TI\r\n'
                 '\r\n'
                 'I am trying the tf.io.write_graph() and tf.import_graph_def API to '
                 'implement model saving and re-building. And I encountered "Init '
                 'operations did not make model ready" when importing the graph_def '
                 'into MonitoredSession()\r\n'
                 '\r\n'
                 '**Here is the importing related source code:**\r\n'
                 '\r\n'
                 '```\r\n'
                 '# Some code to load graph_def from .pbtxt file\r\n'
                 'tf.reset_default_graph()\r\n'
                 "tf.import_graph_def(graph_def, name='')\r\n"
                 'with tf.train.MonitoredTrainingSession() as sess:\r\n'
                 '```\r\n'
                 '\r\n'
                 '**The bug information is:**\r\n'
                 'RuntimeError: Init operations did not make model ready.  Init op: '
                 'group_deps, init fn: None, local_init_op: name: "group_deps_1"\r\n'
                 'op: "NoOp"\r\n'
                 'input: "^init_2"\r\n'
                 'input: "^init_all_tables"\r\n'
                 'input: "^init_3"\r\n'
                 ', error: Variables not initialized: global_step, ......',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 'yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 'Windows 10\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 'pip\r\n'
                 '- TensorFlow version (use command below):\r\n'
                 'v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n'
                 '- Python version: \r\n'
                 '3.6.9\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '10.0, 7.6.2\r\n'
                 '- GPU model and memory:\r\n'
                 'GeForce GTX 1070 and GeForce GTX 1080\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'Crash when writing Checkpoint (or stuck when writing log for '
                 'tensorboard)\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Write a checkpoint and a Tensorboardlog\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n'
                 "os.environ['TF_CONFIG'] = json.dumps({\r\n"
                 "    'cluster': {\r\n"
                 '        "worker": ["10.10.1.168:1234"],\r\n'
                 '        \'chief\': ["10.10.1.60:2345"]\r\n'
                 '    },\r\n'
                 "    'task': {'type': 'chief', 'index': 0}\r\n"
                 '})\r\n'
                 'strategy = '
                 'tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n'
                 '\r\n'
                 'def get_label(file_path, class_names):\r\n'
                 '  parts = tf.strings.split(file_path, os.path.sep)\r\n'
                 '  return parts[-2] == class_names\r\n'
                 '\r\n'
                 'def parse_image(filename):\r\n'
                 '    parts = tf.strings.split(filename, "\\\\")\r\n'
                 '    label = get_label(filename, CLASS_NAMES)\r\n'
                 '    image = tf.io.read_file(filename)\r\n'
                 '    image = tf.image.decode_png(image, channels=3)\r\n'
                 '    image = tf.image.convert_image_dtype(image, tf.float32)\r\n'
                 '    image = tf.image.resize(image, [299,299])\r\n'
                 '    return image, label\r\n'
                 '\r\n'
                 'def make_dataset_unbatched():\r\n'
                 '    images_ds = list_ds.map(parse_image, '
                 'num_parallel_calls=AUTOTUNE)\r\n'
                 '    images_ds = images_ds.shuffle(BATCH_SIZE)\r\n'
                 '    images_ds = images_ds.repeat(epochs)\r\n'
                 '    images_ds = images_ds.prefetch(BUFFER_SIZE)\r\n'
                 '    return images_ds\r\n'
                 '\r\n'
                 'datasetFilePath = "D:\\TrainData\\BalancedData"\r\n'
                 'IMAGESIZE = 299\r\n'
                 'AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n'
                 'datasetPath = pathlib.Path(datasetFilePath)\r\n'
                 'list_ds = tf.data.Dataset.list_files(str(datasetPath/"*/*"))\r\n'
                 'num_elements = tf.data.experimental.cardinality(list_ds).numpy()\r\n'
                 '\r\n'
                 'CLASS_NAMES = np.array([item.name for item in '
                 "datasetPath.glob('*')])\r\n"
                 '\r\n'
                 'epochs = 2\r\n'
                 'def build_and_compile_model():\r\n'
                 '    base_model '
                 '=tf.keras.applications.InceptionV3(include_top=False, weights = '
                 '"imagenet", input_shape=(299,299,3))\r\n'
                 '\r\n'
                 '    base_model.trainable = True\r\n'
                 '    x = base_model.output\r\n'
                 '    x = '
                 'tf.keras.layers.GlobalAveragePooling2D(name="avg_pool")(x)\r\n'
                 '    x = tf.keras.layers.Dense(256, activation="relu")(x)\r\n'
                 '    predictions = tf.keras.layers.Dense(2, '
                 'activation="softmax")(x)\r\n'
                 '    model = tf.keras.models.Model(inputs=base_model.input, '
                 'outputs=predictions)\r\n'
                 '\r\n'
                 '    base_learning_rate = 0.00001\r\n'
                 '    '
                 'model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\r\n'
                 '                 loss="categorical_crossentropy",\r\n'
                 '                 metrics=["accuracy"])\r\n'
                 '    return model\r\n'
                 'logdir = os.path.join("Z:\\Tensorflow\\TensorboardLogs", '
                 'datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))\r\n'
                 'callbacks = '
                 '[tf.keras.callbacks.ModelCheckpoint(filepath="Z:\\Tensorflow\\Checkpoints"), \r\n'
                 '             tf.keras.callbacks.TensorBoard(logdir, '
                 'histogram_freq=1)]\r\n'
                 '\r\n'
                 'with strategy.scope():\r\n'
                 '    dataset = make_dataset_unbatched().batch(BATCH_SIZE, '
                 'drop_remainder=True)\r\n'
                 '    multi_worker_model = build_and_compile_model()\r\n'
                 '\r\n'
                 'history = multi_worker_model.fit(dataset, epochs=epochs, '
                 'steps_per_epoch=50, callbacks=callbacks)\r\n'
                 '``` \r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '[ckpt_error.txt](https://github.com/tensorflow/tensorflow/files/4040557/ckpt_error.txt)\r\n'
                 '\r\n'
                 'This log happens with the checkpoint in the code, and with only the '
                 'tensorboard-log as checkpoint the chief stops right at the end of '
                 'the first epoch and nothing else happens.\r\n'
                 '\r\n'
                 'I hope someone can help me with this.\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): **Yes**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux '
                 'Ubuntu 14.04**\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: No\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version (use command below): v1.13.2\r\n'
                 '- Python version: 2.7\r\n'
                 '- Bazel version (if compiling from source): 0.20.0\r\n'
                 '- GCC/Compiler version (if compiling from source): 4.8.4\r\n'
                 '- CUDA/cuDNN version: no CUDA\r\n'
                 '- GPU model and memory: no GPU\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'When do **distribute training** using **parameter-server '
                 'strategy**, there are some workers failed with the information '
                 '`tensorflow.python.framework.errors_impl.CancelledError: Cancelled` '
                 '**occasionally**.\r\n'
                 '\r\n'
                 'There is no more information, so I have no idea how to debug the '
                 'error. Is there anyone has seen this error or how to debug?  '
                 'appreciate your kind help.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 "It's an occasional problem, so it's not easy to reproduce.\r\n"
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '```\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 676, in run \r\n'
                 '    run_metadata=run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1171, in run \r\n'
                 '    run_metadata=run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1270, in run \r\n'
                 '    raise six.reraise(*original_exc_info)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1255, in run \r\n'
                 '    return self._sess.run(*args, **kwargs)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1327, in run \r\n'
                 '    run_metadata=run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py", '
                 'line 1091, in run \r\n'
                 '    return self._sess.run(*args, **kwargs)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", '
                 'line 929, in run \r\n'
                 '    run_metadata_ptr)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", '
                 'line 1152, in _run\r\n'
                 '    feed_dict_tensor, options, run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", '
                 'line 1328, in _do_run\r\n'
                 '    run_metadata)\r\n'
                 '  File '
                 '"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", '
                 'line 1348, in _do_call\r\n'
                 '    raise type(e)(node_def, op, message)\r\n'
                 'tensorflow.python.framework.errors_impl.CancelledError: '
                 'Cancelled\r\n'
                 '```',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 18.04\r\n'
                 '- TensorFlow installed from (source or binary):binary\r\n'
                 '- TensorFlow version:1.14\r\n'
                 '- Python version:3.7.4\r\n'
                 '- Installed using virtualenv? pip? conda?:conda\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):7.4\r\n'
                 '- CUDA/cuDNN version:10.2\r\n'
                 '- GPU model and memory:GeForce GTX 960M/PCIe/SSE2, 16GB\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '2020-01-09 12:25:17.491189: F '
                 'tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check '
                 'failed: is_rnn_state_array \r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 'I am converting a quantized graph def (.pb ) to a quantized tflite '
                 '(.tflite) using the dummy quantization and encounter error as '
                 'follows\r\n'
                 '```\r\n'
                 '(tf_gpu_clone) '
                 'ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ '
                 'tflite_convert --output_file tflite_graph.tflite --graph_def_file '
                 'tflite_graph.pb --input_arrays image_tensor --output_arrays '
                 'TFLite_Detection_PostProcess --input_shapes 1,576,720,3 '
                 '--allow_custom_ops --inference_type QUANTIZED_UINT8 '
                 '--std_dev_values 127 --mean_values 128 --default_ranges_min 0 '
                 '--default_ranges_max 6\r\n'
                 '2020-01-09 12:25:15.452049: I '
                 'tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU '
                 'supports instructions that this TensorFlow binary was not compiled '
                 'to use: AVX2 FMA\r\n'
                 '2020-01-09 12:25:15.474575: I '
                 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU '
                 'Frequency: 2808000000 Hz\r\n'
                 '2020-01-09 12:25:15.475004: I '
                 'tensorflow/compiler/xla/service/service.cc:168] XLA service '
                 '0x561bb6736540 executing computations on platform Host. Devices:\r\n'
                 '2020-01-09 12:25:15.475031: I '
                 'tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor '
                 'device (0): <undefined>, <undefined>\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "/home/ridlr/anaconda3/bin/tflite_convert", line 10, in '
                 '<module>\r\n'
                 '    sys.exit(main())\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py", '
                 'line 503, in main\r\n'
                 '    app.run(main=run_main, argv=sys.argv[:1])\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py", '
                 'line 40, in run\r\n'
                 '    _run(main=main, argv=argv, '
                 'flags_parser=_parse_flags_tolerate_undef)\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py", '
                 'line 299, in run\r\n'
                 '    _run_main(main, args)\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py", '
                 'line 250, in _run_main\r\n'
                 '    sys.exit(main(argv))\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py", '
                 'line 499, in run_main\r\n'
                 '    _convert_tf1_model(tflite_flags)\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/tflite_convert.py", '
                 'line 193, in _convert_tf1_model\r\n'
                 '    output_data = converter.convert()\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/lite.py", '
                 'line 904, in convert\r\n'
                 '    **converter_kwargs)\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py", '
                 'line 373, in toco_convert_graph_def\r\n'
                 '    input_data.SerializeToString())\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/python/convert.py", '
                 'line 172, in toco_convert_protos\r\n'
                 '    "TOCO failed. See console for info.\\n%s\\n%s\\n" % (stdout, '
                 'stderr))\r\n'
                 'tensorflow.lite.python.convert.ConverterError: TOCO failed. See '
                 'console for info.\r\n'
                 '2020-01-09 12:25:16.861669: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:1336] Converting '
                 'unsupported operation: TFLite_Detection_PostProcess\r\n'
                 '2020-01-09 12:25:16.957738: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'Before Removing unused ops: 1537 operators, 2264 arrays (0 '
                 'quantized)\r\n'
                 '2020-01-09 12:25:17.017901: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'Before general graph transformations: 1537 operators, 2264 arrays '
                 '(0 quantized)\r\n'
                 '2020-01-09 12:25:17.482076: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'After general graph transformations pass 1: 181 operators, 341 '
                 'arrays (0 quantized)\r\n'
                 '2020-01-09 12:25:17.485583: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'Before pre-quantization graph transformations: 181 operators, 341 '
                 'arrays (0 quantized)\r\n'
                 '2020-01-09 12:25:17.486877: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'After pre-quantization graph transformations pass 1: 99 operators, '
                 '259 arrays (0 quantized)\r\n'
                 '2020-01-09 12:25:17.488034: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'Before Group bidirectional sequence lstm/rnn: 99 operators, 259 '
                 'arrays (0 quantized)\r\n'
                 '2020-01-09 12:25:17.489088: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'Before default min-max range propagation graph transformations: 99 '
                 'operators, 259 arrays (0 quantized)\r\n'
                 '2020-01-09 12:25:17.489972: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'After default min-max range propagation graph transformations pass '
                 '1: 99 operators, 259 arrays (0 quantized)\r\n'
                 '2020-01-09 12:25:17.491160: I '
                 'tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] '
                 'Before quantization graph transformations: 99 operators, 259 arrays '
                 '(0 quantized)\r\n'
                 '2020-01-09 12:25:17.491189: F '
                 'tensorflow/lite/toco/graph_transformations/quantize.cc:611] Check '
                 'failed: is_rnn_state_array \r\n'
                 'Fatal Python error: Aborted\r\n'
                 '\r\n'
                 'Current thread 0x00007fb839eed740 (most recent call first):\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py", '
                 'line 33 in execute\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py", '
                 'line 250 in _run_main\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/absl/app.py", '
                 'line 299 in run\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py", '
                 'line 40 in run\r\n'
                 '  File '
                 '"/home/ridlr/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py", '
                 'line 59 in main\r\n'
                 '  File "/home/ridlr/anaconda3/bin/toco_from_protos", line 10 in '
                 '<module>\r\n'
                 'Aborted (core dumped)\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 'However if I do not include the following specifiers a *.tflite is '
                 'created.\r\n'
                 '`--inference_type QUANTIZED_UINT8 --std_dev_values 127 '
                 '--mean_values 128 --default_ranges_min 0 --default_ranges_max 6 '
                 '`\r\n'
                 '\r\n'
                 'This *.tflite file when used to convert to *_edgetpu.tflite (this '
                 'model is used to run inference on Google coral) gives the following '
                 'error\r\n'
                 '```\r\n'
                 '(tf_gpu_clone) '
                 'ridlr@ridlr107:~/TensorFlow/exported_model_12k_quantized$ '
                 'edgetpu_compiler tflite_graph.tflite \r\n'
                 'Edge TPU Compiler version 2.0.267685300\r\n'
                 'Invalid model: tflite_graph.tflite\r\n'
                 'Model not quantized\r\n'
                 '```\r\n'
                 '\r\n'
                 'Hence it is necessary to include the specifiers for '
                 'quantization.\r\n'
                 '\r\n',
         'created_at': '2020-01-0'},
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c03_moving_average.ipynb\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 "It should be 'mean absolute error' instead of squared error while "
                 'Naive Forecasting\r\n'
                 '\r\n'
                 '![Screenshot from 2020-01-09 '
                 '12-10-09](https://user-images.githubusercontent.com/29497701/72044240-4bd89a80-32d9-11ea-937f-a189784b83b0.png)\r\n'
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 "Yes, I'll be submitting one shortly",
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 18.04\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: 2.0\r\n'
                 '- Bazel version (if compiling from source): 0.26.1\r\n'
                 '- GCC/Compiler version (if compiling from source): 7.4.0\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'I need just a TFLite header file without the whole TensorFlow '
                 'source code. Is there any method to install the TFLite header?\r\n'
                 '\r\n'
                 'I found a similar issue at #3536. But, '
                 '`//tensorflow:install_headers` Bazel target does not install a '
                 'TFLite header file.\r\n',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a feature request. As per our '
                 '[GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. '
                 'tag:feature_template</em>\r\n'
                 '\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- TensorFlow version (you are using): 2.0\r\n'
                 '- Are you willing to contribute it (Yes/No): Yes (if necessary)\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 '\r\n'
                 'Typically for RL, I use tensorflow in combination with Ray, where a '
                 'remote agent collects data using a policy model, and the episodes '
                 'are fed to a tf-agents EpisodicReplayBuffer in a second train-loop '
                 'process. A tf.dataset is used to feed the main model training on '
                 'this thread, which is then serialized via get_weights and sent back '
                 'to the remote actor via a ray remote call to set_weights.\r\n'
                 '\r\n'
                 'Currently I use tf.Checkpoint.restore.expect_partial() to restore '
                 'my subclassed tf.keras.Model. In practice, this is extremely '
                 'convoluted on initialization:\r\n'
                 '* I make a forward pass through my remote Agent on random policy '
                 '(since I have not initialized the weights), retrieve this forward '
                 'pass, and send it to the train loop\r\n'
                 '* I make a second forward pass through my train loop (again, '
                 'randomly initialized network still), this has the side effect of '
                 'initializing the network, allowing my checkpoint to restore the '
                 'variables in this model\r\n'
                 '* I call get_weights on the train_loop model, and pass the weights '
                 'to my remote model via set_weights.\r\n'
                 '* I can now collect a full replay buffer with my Agent on policy\r\n'
                 '* I can now run the train loop\r\n'
                 '\r\n'
                 "Maybe I'm missing something, but most of my labmates are also "
                 'confused by what the best practice for this currently is.\r\n'
                 '\r\n'
                 '**Will this change the current api? How?**\r\n'
                 "I'm not sure what the best approach is. Some ideas for "
                 'discussion? \r\n'
                 '\r\n'
                 '* Allow set_weights to define the shapes and names of tf.Variables, '
                 'such that calls to __build__ in the tf.keras.Model on first run '
                 'will align with the initialized tf.Variables.\r\n'
                 '* Allow tf.keras.model.load_weights() to set the weights '
                 'immediately rather than waiting for first call to build()\r\n'
                 '\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 'RL community \r\n'
                 '**Any Other info.**\r\n',
         'created_at': '2020-01-0'},
        {'body': 'In working with TF 2.0, I noticed that the TF Feature Columns API '
                 'seems to overlap with the Keras Preprocessing layers (or Keras '
                 'Utils). For example, you can create an '
                 '`tf.feature_column.indicator_column()` which creates a bunch of '
                 'dummy variables or a one-hot encoded matrix based upon a '
                 'categorical variable. With Keras preprocessing or Utils, you can '
                 'use the `tf.keras.backend.one_hot()` function to perform the same '
                 'operation. I think there are similar overlaps between TF Feature '
                 'Columns like the embedding columns and similar Keras functions for '
                 'embedding columns.\r\n'
                 '\r\n'
                 'I was just wondering if the steering committees for Tensorflow have '
                 'any direction on whether they plan to promote one set of functions '
                 'versus the other? Are there any plans to deprecate one set versus '
                 'the other. For me, it is just a question of where to invest time '
                 'and planning for code that might have to change in the future. '
                 'Seems like maintaining the redundancy in the package will '
                 'potentially lead to performance differences between similar '
                 'functions, or confusion in setting up the code, etc. \r\n'
                 '\r\n'
                 '**NB** \r\n'
                 'Oh yes, I actually asked this question in the Tensorflow Discussion '
                 'forum, but no one answered it. @dynamicwebpaige even forwarded the '
                 'message to @karmel and Mark Omernick, but no one responded. Hence, '
                 'I posted here. \r\n',
         'created_at': '2020-01-0'},
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/examples/blob/d631c0545dac90c6390da76ed8df7c4f6a2a25bc/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c01_common_patterns.ipynb#L307\r\n'
                 '\r\n'
                 '`def white_noise(time, noise_level=1, seed=None):`\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 'I think, we should add explanation of `seed` parameter here since '
                 "it's quite an important one.\r\n"
                 '\r\n'
                 '### Clear description\r\n'
                 '\r\n'
                 'Some explanation about how `seed` affects generation of random '
                 'numbers every time along with links for reference can be added.\r\n'
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 'Yes',
         'created_at': '2020-01-0'},
        {'body': '------------------------\r\n'
                 '\r\n'
                 '### System information\r\n'
                 '- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: '
                 'Windows 10 Version 1909\r\n'
                 '- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device**:\r\n'
                 '- **TensorFlow version (use command below)**: 2.0.0\r\n'
                 '- **Python version**: 3.6.9\r\n'
                 '- **CUDA/cuDNN version**:10.0.130\r\n'
                 '- **GPU model and memory**:GTX 1660 Ti 6GB\r\n'
                 '\r\n'
                 '\r\n'
                 '[Jazz improvisation with '
                 'LSTM.zip](https://github.com/tensorflow/tensorflow/files/4035452/Jazz.improvisation.with.LSTM.zip)\r\n'
                 '\r\n'
                 'I uploaded a zip file in which the .ipynb file when runs gets that '
                 "error. Please you can run and check what's the problem. How to fix "
                 'this issuse?',
         'created_at': '2020-01-0'},
        {'body': 'I get an error similar to the issue described in '
                 'https://github.com/tensorflow/tensorflow/issues/31451\r\n'
                 'When using tf.keras.estimator.model_to_estimator in TF 1.14 and '
                 '1.15 I get the InvalidArgumentError described below. The problem '
                 "doesn't occur in TF 1.13.2. \r\n"
                 '\r\n'
                 'I use Linux Mint with Python 3.6 with 2 GPUs (Nvidia GTX1080Ti). To '
                 'be complete, there are 3 NVidia video cards in the machine (that '
                 'seems relevant if I understand the error correctly).\r\n'
                 '\r\n'
                 '```\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'InvalidArgumentError                      Traceback (most recent '
                 'call last)\r\n'
                 '<ipython-input-6-3ce5a1d7f637> in <module>\r\n'
                 '      2     keras_model=model_f,\r\n'
                 "      3     custom_objects={'Merge': Merge},\r\n"
                 "----> 4     model_dir='./data/estimator')\r\n"
                 '\r\n'
                 '~/.../python3.6/site-packages/tensorflow_core/python/keras/estimator/__init__.py '
                 'in model_to_estimator(keras_model, keras_model_path, '
                 'custom_objects, model_dir, config, checkpoint_format)\r\n'
                 '    105       config=config,\r\n'
                 '    106       checkpoint_format=checkpoint_format,\r\n'
                 '--> 107       use_v2_estimator=False)\r\n'
                 '    108 \r\n'
                 '    109 \r\n'
                 '\r\n'
                 '~/.../python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py '
                 'in model_to_estimator(keras_model, keras_model_path, '
                 'custom_objects, model_dir, config, checkpoint_format, '
                 'use_v2_estimator)\r\n'
                 '    574   if keras_model._is_graph_network:\r\n'
                 '    575     warm_start_path = _save_first_checkpoint(keras_model, '
                 'custom_objects,\r\n'
                 '--> 576                                              config, '
                 'save_object_ckpt)\r\n'
                 '    577   elif keras_model.built:\r\n'
                 "    578     logging.warning('You are creating an Estimator from a "
                 "Keras model manually '\r\n"
                 '\r\n'
                 '~/.../python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py '
                 'in _save_first_checkpoint(keras_model, custom_objects, config, '
                 'save_object_ckpt)\r\n'
                 '    390 \r\n'
                 '    391       # save to checkpoint\r\n'
                 '--> 392       with session.Session(config=config.session_config) as '
                 'sess:\r\n'
                 '    393         if keras_weights:\r\n'
                 '    394           model.set_weights(keras_weights)\r\n'
                 '\r\n'
                 '~/.../python3.6/site-packages/tensorflow_core/python/client/session.py '
                 'in __init__(self, target, graph, config)\r\n'
                 '   1583           protocol buffer with configuration options for '
                 'the session.\r\n'
                 '   1584     """\r\n'
                 '-> 1585     super(Session, self).__init__(target, graph, '
                 'config=config)\r\n'
                 '   1586     # NOTE(mrry): Create these on first `__enter__` to '
                 'avoid a reference cycle.\r\n'
                 '   1587     self._default_graph_context_manager = None\r\n'
                 '\r\n'
                 '~/.../python3.6/site-packages/tensorflow_core/python/client/session.py '
                 'in __init__(self, target, graph, config)\r\n'
                 '    697     try:\r\n'
                 '    698       # pylint: disable=protected-access\r\n'
                 '--> 699       self._session = '
                 'tf_session.TF_NewSessionRef(self._graph._c_graph, opts)\r\n'
                 '    700       # pylint: enable=protected-access\r\n'
                 '    701     finally:\r\n'
                 '\r\n'
                 'InvalidArgumentError: Invalid device ordinal value (2). Valid range '
                 'is [0, 1].\r\n'
                 '\twhile setting up XLA_GPU_JIT device number 2\r\n'
                 '```',
         'created_at': '2020-01-0'},
        {'body': 'Hi, guys!\r\n'
                 '\r\n'
                 'Here are the prefetch-enabled input stream and yet another random '
                 'access file with buffer we talked about earlier in #33023. Here '
                 "come's the code, benchmark numbers are on the way.",
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 'a = tf.Variable(2)\r\n'
                 'a.assign(5)\r\n'
                 'assert a.numpy() == 5\r\n'
                 '\r\n'
                 '# ValueError: Shapes () and (2,) are incompatible\r\n'
                 'a.assign([1,2])  \r\n'
                 '\r\n'
                 '# TypeError: assign() got an unexpected keyword argument '
                 "'validate_shape'\r\n"
                 'a.assign([1,2], validate_shape=False)\r\n'
                 '\r\n'
                 '# ValueError: Shapes () and (2,) are incompatible\r\n'
                 'tf.compat.v1.assign(a, [1,2], validate_shape=False)  \r\n'
                 '\r\n'
                 '```\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10, Linux\r\n'
                 '- TensorFlow installed from (source or binary): pip\r\n'
                 '- TensorFlow version (use command below): 2.0.0, 2.1.0\r\n'
                 '- Python version: 3.7\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 '`tf.assign` had a `validate_shape` parameter that `Variable.assign` '
                 'seems to be missing.\r\n'
                 '\r\n'
                 'In addition, the docs say:\r\n'
                 '> If you want to change the shape of a variable later you have to '
                 'use an `assign` Op with `validate_shape=False`.\r\n'
                 '\r\n'
                 'https://www.tensorflow.org/api_docs/python/tf/Variable\r\n'
                 '\r\n'
                 'How should one change the shape of a variable?\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'See above.\r\n',
         'created_at': '2020-01-0'},
        {'body': 'This PR enables complex-type BLAS operations GEMM, GEMV, and SCAL '
                 'on the ROCm platform, and  does some housekeeping (e.g. corrects '
                 'misspellings) in stream_executor/rocm/rocm_blas.cc.',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: 2.0\r\n'
                 '- Python version: 3.7.4\r\n'
                 '- Installed using virtualenv? pip? conda?: conda\r\n'
                 '- Bazel version (if compiling from source): 0.26.1\r\n'
                 '- GCC/Compiler version (if compiling from source): 7.4.0\r\n'
                 '- CUDA/cuDNN version:CUDA 10.0/cuDNN7.4\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '\r\n'
                 ' I want to get rid of the message like \r\n'
                 '\r\n'
                 '>Your CPU supports instructions that this TensorFlow binary was not '
                 'compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n'
                 '\r\n'
                 'Therefore, I build TF2.0 following the '
                 '[guide](https://www.tensorflow.org/install/source) on the official '
                 "site. Here's my configuration\r\n"
                 '\r\n'
                 '```\r\n'
                 'Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n\r\n'
                 'No XLA JIT support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: '
                 'n\r\n'
                 'No OpenCL SYCL support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with ROCm support? [y/N]: n\r\n'
                 'No ROCm support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n'
                 'CUDA support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with TensorRT support? [y/N]: n\r\n'
                 'No TensorRT support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Found CUDA 10.0 in:\r\n'
                 '    /usr/local/cuda/lib64\r\n'
                 '    /usr/local/cuda/include\r\n'
                 'Found cuDNN 7 in:\r\n'
                 '    /usr/local/cuda/lib64\r\n'
                 '    /usr/local/cuda/include\r\n'
                 '\r\n'
                 '\r\n'
                 'Please specify a list of comma-separated CUDA compute capabilities '
                 'you want to build with.\r\n'
                 'You can find the compute capability of your device at: '
                 'https://developer.nvidia.com/cuda-gpus.\r\n'
                 'Please note that each additional compute capability significantly '
                 'increases your build time and binary size, and that TensorFlow only '
                 'supports compute capabilities >= 3.5 [Default is: 7.5]:\r\n'
                 '\r\n'
                 '\r\n'
                 'Do you want to use clang as CUDA compiler? [y/N]: n\r\n'
                 'nvcc will be used as CUDA compiler.\r\n'
                 '\r\n'
                 'Please specify which gcc should be used by nvcc as the host '
                 'compiler. [Default is /usr/bin/gcc]:\r\n'
                 '\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with MPI support? [y/N]: n\r\n'
                 'No MPI support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Please specify optimization flags to use during compilation when '
                 'bazel option "--config=opt" is specified [Default is -march=native '
                 '-Wno-sign-compare]:\r\n'
                 '\r\n'
                 '\r\n'
                 'Would you like to interactively configure ./WORKSPACE for Android '
                 'builds? [y/N]: n\r\n'
                 'Not configuring the WORKSPACE for Android builds.\r\n'
                 '\r\n'
                 'Preconfigured Bazel build configs. You can use any of the below by '
                 'adding "--config=<>" to your build command. See .bazelrc for more '
                 'details.\r\n'
                 '        --config=mkl            # Build with MKL support.\r\n'
                 '        --config=monolithic     # Config for mostly static '
                 'monolithic build.\r\n'
                 '        --config=gdr            # Build with GDR support.\r\n'
                 '        --config=verbs          # Build with libverbs support.\r\n'
                 '        --config=ngraph         # Build with Intel nGraph '
                 'support.\r\n'
                 '        --config=numa           # Build with NUMA support.\r\n'
                 '        --config=dynamic_kernels        # (Experimental) Build '
                 'kernels into separate shared objects.\r\n'
                 '        --config=v2             # Build TensorFlow 2.x instead of '
                 '1.x.\r\n'
                 'Preconfigured Bazel build configs to DISABLE default on '
                 'features:\r\n'
                 '        --config=noaws          # Disable AWS S3 filesystem '
                 'support.\r\n'
                 '        --config=nogcp          # Disable GCP support.\r\n'
                 '        --config=nohdfs         # Disable HDFS support.\r\n'
                 '        --config=noignite       # Disable Apache Ignite support.\r\n'
                 '        --config=nokafka        # Disable Apache Kafka support.\r\n'
                 '        --config=nonccl         # Disable NVIDIA NCCL support.\r\n'
                 'Configuration finished\r\n'
                 '```\r\n'
                 'I use the following command to build\r\n'
                 '```\r\n'
                 'bazel build -c opt — copt=-mavx — copt=-mavx2 — copt=-mfma — '
                 'copt=-mfpmath=both — copt=-msse4.2 -k '
                 '//tensorflow/tools/pip_package:build_pip_package\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 '\r\n'
                 '```\r\n'
                 'external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:308:16:   '
                 'required from ‘void '
                 'EigenForTFLite::internal::TensorContractionKernel<ResScalar, '
                 'LhsScalar, RhsScalar, StorageIndex, OutputMapper, LhsMapper, '
                 'RhsMappe\r\n'
                 'r>::packRhs(RhsScalar**, const typename RhsMapper::SubMapper&, '
                 'StorageIndex, StorageIndex) [with ResScalar = float; LhsScalar = '
                 'float; RhsScalar = float; StorageIndex = long int; OutputMapper = '
                 'EigenForTFLite::internal::blas_data_mapper<f\r\n'
                 'loat, long int, 0, 0>; LhsMapper = '
                 'EigenForTFLite::internal::TensorContractionInputMapper<float, long '
                 'int, 1, EigenForTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long '
                 'int, 2>, const EigenFo\r\n'
                 'rTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long '
                 'int>, 16> >, EigenForTFLite::ThreadPoolDevice>, std::array<long '
                 'int, 1>, std::array<long int, 1>, 4, true, false, 0, '
                 'EigenForTFLite::MakePointer>; RhsMapper = EigenForTFLit\r\n'
                 'e::internal::TensorContractionInputMapper<float, long int, 0, '
                 'EigenForTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long '
                 'int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const '
                 'EigenF\r\n'
                 'orTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, long '
                 'int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, std::array<long '
                 'int, 1>, std::array<long int, 1>, 4, true, false, 0, '
                 'EigenForTFLite::MakePointer>; EigenForTFLite::intern\r\n'
                 'al::TensorContractionKernel<ResScalar, LhsScalar, RhsScalar, '
                 'StorageIndex, OutputMapper, LhsMapper, RhsMapper>::RhsBlock = '
                 'float*; typename RhsMapper::SubMapper = '
                 'EigenForTFLite::internal::TensorContractionSubMapper<float, long '
                 'int, 0, Ei\r\n'
                 'genForTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long '
                 'int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, '
                 'lo\r\n'
                 'ng int>, 16> > >, EigenForTFLite::ThreadPoolDevice>, '
                 'std::array<long int, 1>, std::array<long int, 1>, 4, true, false, '
                 '0, EigenForTFLite::MakePointer>]’\r\n'
                 'external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:898:11:   '
                 'required from ‘void '
                 'EigenForTFLite::TensorContractionEvaluatorBase<Derived>::evalGemmPartial(EigenForTFLite::TensorContractionEvaluatorBase<Derived>::\r\n'
                 'Scalar*, '
                 'EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index, '
                 'EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index, '
                 'int) const [with bool lhs_inner_dim_contiguous = true; bool '
                 'rhs_inner_dim_contiguous = true; bool rhs\r\n'
                 '_inner_dim_reordered = false; int Alignment = 0; bool '
                 'use_output_kernel = false; Derived = '
                 'EigenForTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorContractionOp<const '
                 'std::array<EigenForTFLite::IndexPair<long int>, 1>, const '
                 'EigenForT\r\n'
                 'FLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, 2>, '
                 'const EigenForTFLite::TensorImagePatchOp<-1, -1, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, '
                 'long int>, 16> > >, const EigenForTFLite::Tenso\r\n'
                 'rReshapingOp<const EigenForTFLite::DSizes<long int, 2>, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, '
                 'long int>, 16> >, const EigenForTFLite::NoOpOutputKernel>, '
                 'EigenForTFLite::ThreadPoolDevice>; EigenForTFLite\r\n'
                 '::TensorContractionEvaluatorBase<Derived>::Scalar = float; '
                 'EigenForTFLite::TensorContractionEvaluatorBase<Derived>::Index = '
                 'long int]’\r\n'
                 'external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorContraction.h:813:37:   '
                 '[ skipping 7 instantiation contexts, use '
                 '-ftemplate-backtrace-limit=0 to disable ]\r\n'
                 'external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMorphing.h:175:44:   '
                 'required from ‘bool EigenForTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, '
                 'Device>::evalSubExprsIfNeeded(EigenF\r\n'
                 'orTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, '
                 'Device>::EvaluatorPointerType) [with NewDimensions = const '
                 'EigenForTFLite::DSizes<long int, 4>; ArgType = const '
                 'EigenForTFLite::TensorContractionOp\r\n'
                 '<const std::array<EigenForTFLite::IndexPair<long int>, 1>, const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long '
                 'int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite:\r\n'
                 ':Tensor<const float, 4, 1, long int>, 16> > >, const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long '
                 'int, 2>, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, '
                 'long int>, 16> >, const EigenForTF\r\n'
                 'Lite::NoOpOutputKernel>; Device = EigenForTFLite::ThreadPoolDevice; '
                 'EigenForTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorReshapingOp<NewDimensions, XprType>, '
                 'Device>::EvaluatorPointerType = float*]’\r\n'
                 'external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:148:62:   '
                 'required from ‘bool EigenForTFLite::TensorEvaluator<const '
                 'EigenForTFLite::TensorAssignOp<LhsXprType, RhsXprType>, '
                 'Device>::evalSubExprsIfNeeded(EigenForTFL\r\n'
                 'ite::TensorEvaluator<const '
                 'EigenForTFLite::TensorAssignOp<LhsXprType, RhsXprType>, '
                 'Device>::EvaluatorPointerType) [with LeftArgType = '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, 4, 1, long '
                 'int>, 16>; RightArgType = const Eigen\r\n'
                 'ForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long int, '
                 '4>, const EigenForTFLite::TensorContractionOp<const '
                 'std::array<EigenForTFLite::IndexPair<long int>, 1>, const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSiz\r\n'
                 'es<long int, 2>, const EigenForTFLite::TensorImagePatchOp<-1, -1, '
                 'const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, '
                 '4, 1, long int>, 16> > >, const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long '
                 'int,\r\n'
                 ' 2>, const EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const '
                 'float, 4, 1, long int>, 16> >, const '
                 'EigenForTFLite::NoOpOutputKernel> >; Device = '
                 'EigenForTFLite::ThreadPoolDevice; '
                 'EigenForTFLite::TensorEvaluator<const EigenForTFLite::T\r\n'
                 'ensorAssignOp<LhsXprType, RhsXprType>, '
                 'Device>::EvaluatorPointerType = float*]’\r\n'
                 'external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:277:16:   '
                 'required from ‘static void '
                 'EigenForTFLite::internal::TensorExecutor<Expression, '
                 'EigenForTFLite::ThreadPoolDevice, Vectorizable, '
                 'Tileable>::run(const Expr\r\n'
                 'ession&, const EigenForTFLite::ThreadPoolDevice&) [with Expression '
                 '= const '
                 'EigenForTFLite::TensorAssignOp<EigenForTFLite::TensorMap<EigenForTFLite::Tensor<float, '
                 '4, 1, long int>, 16>, const EigenForTFLite::TensorReshapingOp<const '
                 'EigenFor\r\n'
                 'TFLite::DSizes<long int, 4>, const '
                 'EigenForTFLite::TensorContractionOp<const '
                 'std::array<EigenForTFLite::IndexPair<long int>, 1>, const '
                 'EigenForTFLite::TensorReshapingOp<const EigenForTFLite::DSizes<long '
                 'int, 2>, const EigenForTFLite::Tens\r\n'
                 'orImagePatchOp<-1, -1, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, '
                 'long int>, 16> > >, const EigenForTFLite::TensorReshapingOp<const '
                 'EigenForTFLite::DSizes<long int, 2>, const '
                 'EigenForTFLite::TensorMap<EigenF\r\n'
                 'orTFLite::Tensor<const float, 4, 1, long int>, 16> >, const '
                 'EigenForTFLite::NoOpOutputKernel> > >; bool Vectorizable = true; '
                 'bool Tileable = false]’\r\n'
                 'external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35:62:   '
                 'required from ‘EigenForTFLite::TensorDevice<ExpressionType, '
                 'DeviceType>& EigenForTFLite::TensorDevice<ExpressionType, '
                 'DeviceType>::operator=(const OtherDeri\r\n'
                 'ved&) [with OtherDerived = EigenForTFLite::TensorReshapingOp<const '
                 'EigenForTFLite::DSizes<long int, 4>, const '
                 'EigenForTFLite::TensorContractionOp<const '
                 'std::array<EigenForTFLite::IndexPair<long int>, 1>, const '
                 'EigenForTFLite::TensorReshap\r\n'
                 'ingOp<const EigenForTFLite::DSizes<long int, 2>, const '
                 'EigenForTFLite::TensorImagePatchOp<-1, -1, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, '
                 'long int>, 16> > >, const '
                 'EigenForTFLite::TensorReshapingOp<const\r\n'
                 'EigenForTFLite::DSizes<long int, 2>, const '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<const float, 4, 1, '
                 'long int>, 16> >, const EigenForTFLite::NoOpOutputKernel> >; '
                 'ExpressionType = '
                 'EigenForTFLite::TensorMap<EigenForTFLite::Tensor<\r\n'
                 'float, 4, 1, long int>, 16>; DeviceType = '
                 'EigenForTFLite::ThreadPoolDevice]’\r\n'
                 './tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:128:29:   '
                 'required from ‘void '
                 'tflite::multithreaded_ops::EigenTensorConvFunctor<T>::operator()(const '
                 'EigenForTFLite::ThreadPoolDevice&, const T*, int, int, int, int, '
                 'const\r\n'
                 'T*, int, int, int, int, int, int, int, tflite::PaddingType, T*, '
                 'int, int) [with T = float]’\r\n'
                 './tensorflow/lite/kernels/internal/optimized/multithreaded_conv.h:169:65:   '
                 'required from here\r\n'
                 './tensorflow/core/kernels/eigen_spatial_convolutions-inl.h:600:15: '
                 'warning: ignoring attributes on template argument '
                 '‘EigenForTFLite::internal::packet_traits<float>::type {aka '
                 '__vector(4) float}’ [-Wignored-attributes]\r\n'
                 '     const int packetSize = '
                 'internal::unpacket_traits<Packet>::size;\r\n'
                 '               ^~~~~~~~~~\r\n'
                 'Target //tensorflow/tools/pip_package:build_pip_package '
                 'up-to-date:\r\n'
                 '  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\n'
                 'ERROR: command succeeded, but there were errors parsing the target '
                 'pattern\r\n'
                 'INFO: Elapsed time: 6524.563s, Critical Path: 252.51s\r\n'
                 'INFO: 18226 processes: 18226 local.\r\n'
                 'FAILED: Build did NOT complete successfully\r\n'
                 '```\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): No\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS '
                 'Linux\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): pip install\r\n'
                 '- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 '
                 '2.0.0\r\n'
                 '- Python version: 3.6.9\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory: CPU\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'For reference, numpy is planning to register numpy ndarray as a '
                 'Sequence:\r\n'
                 'https://github.com/numpy/numpy/issues/2776\r\n'
                 '\r\n'
                 'The sample ResNet50 code from https://keras.io/applications/ runs '
                 'fine. But, if ndarray is registered as a sequence, then TF2 throws '
                 'an InvalidArgumentError.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 '```\r\n'
                 'from keras.applications.resnet50 import ResNet50\r\n'
                 'from keras.preprocessing import image\r\n'
                 'from keras.applications.resnet50 import preprocess_input, '
                 'decode_predictions\r\n'
                 'import numpy as np\r\n'
                 'import typing\r\n'
                 'typing.Sequence.register(np.ndarray)\r\n'
                 '\r\n'
                 "model = ResNet50(weights='imagenet')\r\n"
                 '\r\n'
                 "img_path = 'elephant.jpg'\r\n"
                 'img = image.load_img(img_path, target_size=(224, 224))\r\n'
                 'x = image.img_to_array(img)\r\n'
                 'x = np.expand_dims(x, axis=0)\r\n'
                 'x = preprocess_input(x)\r\n'
                 '\r\n'
                 'preds = model.predict(x)\r\n'
                 '# decode the results into a list of tuples (class, description, '
                 'probability)\r\n'
                 '# (one such list for each sample in the batch)\r\n'
                 "print('Predicted:', decode_predictions(preds, top=3)[0])\r\n"
                 "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), "
                 "(u'n01871265', u'tusker', 0.1122357), (u'n02504458', "
                 "u'African_elephant', 0.061040461)]\r\n"
                 '```\r\n'
                 '\r\n'
                 'Two extra lines added to the sample ResNet50 code are:\r\n'
                 '\r\n'
                 '```\r\n'
                 'import typing\r\n'
                 'typing.Sequence.register(np.ndarray)\r\n'
                 '```\r\n'
                 '\r\n'
                 'The error is:\r\n'
                 '\r\n'
                 '```\r\n'
                 '2020-01-07 13:48:16.421816: W '
                 'tensorflow/core/common_runtime/base_collective_executor.cc:216] '
                 'BaseCollectiveExecutor::StartAbort Invalid argument: The first '
                 'dimension of padding\\s must be the rank of inputs[4,2] []\r\n'
                 '         [[{{node conv1_pad/Pad}}]]\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "test_d3m_imports.py", line 16, in <module>\r\n'
                 '    preds = model.predict(x)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/keras/engine/training.py", '
                 'line 1462, in predict\r\n'
                 '    callbacks=callbacks)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/keras/engine/training_arrays.py", '
                 'line 324, in predict_loop\r\n'
                 '    batch_outs = f(ins_batch)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py", '
                 'line 3740, in __call__\r\n'
                 '    outputs = self._graph_fn(*converted_inputs)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 1081, in __call__\r\n'
                 '    return self._call_impl(args, kwargs)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 1121, in _call_impl\r\n'
                 '    return self._call_flat(args, self.captured_inputs, '
                 'cancellation_manager)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 1224, in _call_flat\r\n'
                 '    ctx, args, cancellation_manager=cancellation_manager)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 511, in call\r\n'
                 '    ctx=ctx)\r\n'
                 '  File '
                 '"/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py", '
                 'line 67, in quick_execute\r\n'
                 '    six.raise_from(core._status_to_exception(e.code, message), '
                 'None)\r\n'
                 '  File "<string>", line 3, in raise_from\r\n'
                 'tensorflow.python.framework.errors_impl.InvalidArgumentError:  The '
                 'first dimension of paddings must be the rank of inputs[4,2] []\r\n'
                 '         [[node conv1_pad/Pad (defined at '
                 '/data/dsbox/kyao/miniconda3/envs/dsbox-eval-2019-winter/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) '
                 ']] [O\\p:__inference_keras_scratch_graph_10370]\r\n'
                 '\r\n'
                 'Function call stack:\r\n'
                 'keras_scratch_graph\r\n'
                 '\r\n'
                 '```\r\n',
         'created_at': '2020-01-0'},
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=stable\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 'After the references list there is a stray malformed tag:\r\n'
                 '\r\n'
                 '{ {TRAINABLE_ATTRIBUTE_NOTE}}\r\n'
                 '\r\n'
                 'I suspect that this is supposed to resolve to the note that '
                 '`moving_mean` and `moving_variance` are placed in `UPDATE_OPS` and '
                 'need to be executed alongside the training op.  (This note is '
                 'present in the `tf.layers` doc: '
                 'https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/batch_normalization)  '
                 "But without knowing what this tag refers to I can't really say for "
                 'sure.  (The tag is present in only three places in the Tensorflow '
                 'codebase and shows up malformed on the website in each case.)\r\n',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: 2.0.0\r\n'
                 '- Python version: 3.6.4\r\n'
                 '- Installed using virtualenv? pip? conda?: pip\r\n'
                 '- Bazel version (if compiling from source): 0.26.1\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10/7\r\n'
                 '- GPU model and memory: NVIDIA GeForce MX150\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'I am trying to build tensorflow with AVX2 instructions for my CPU '
                 'to increase speeds, but bazel keeps failing when loading '
                 'tensorflow\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '1. Cloned the tensorflow source from github and checked out the '
                 'r2.0 branch.\r\n'
                 '2. Ran python ./configure.py\r\n'
                 '3. Proceded through steps, selecting no for all other builds and '
                 'set the bazel config option to /arch:AVC2\r\n'
                 '4. Ran bazel build --config=opt '
                 '//tensorflow/tools/pip_package:build_pip_package\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 'The error text is attached here:\r\n'
                 '[error.txt](https://github.com/tensorflow/tensorflow/files/4032375/error.txt)\r\n',
         'created_at': '2020-01-0'},
        {'body': 'This patch allows devices configured with numa enabled to not share '
                 'the thread pool with the default CPU device.\r\n'
                 '\r\n'
                 "This has come around as I've been experimenting with NUMA aware "
                 'datasets/iterators which share the same sessions, but we create a '
                 'DeviceMgr and a dataset iterator with a CPU device per NUMA node. '
                 'Without this change the CPU:0 from the new DeviceMgr might share a '
                 'thread pool which was not created with NUMA awareness.',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): official '
                 'tensorflow image from docker hub\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: n/a\r\n'
                 '- TensorFlow installed from (source or binary): n/a\r\n'
                 '- TensorFlow version: 1.15.0\r\n'
                 '- Python version: 3\r\n'
                 '- Installed using virtualenv? pip? conda?: Docker\r\n'
                 '- Bazel version (if compiling from source): n/a\r\n'
                 '- GCC/Compiler version (if compiling from source): n/a\r\n'
                 '- CUDA/cuDNN version: 10\r\n'
                 '- GPU model and memory: n/a\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'Simple image built `FROM tensorflow/tensorflow:tag` fails to build '
                 'when including `mysql-client`\r\n'
                 '\r\n'
                 '```dockerfile\r\n'
                 'FROM tensorflow/tensorflow:1.15.0-gpu-py3-jupyter\r\n'
                 '\r\n'
                 '# Avoid ERROR: invoke-rc.d: policy-rc.d denied execution of '
                 'start.\r\n'
                 '# RUN sed -i "s/^exit 101$/exit 0/" /usr/sbin/policy-rc.d\r\n'
                 '\r\n'
                 '# --- Install any needed packages specified in requirements.apt\r\n'
                 'COPY requirements.apt .\r\n'
                 'RUN apt-get update && xargs apt-get install -y < '
                 'requirements.apt\r\n'
                 '\r\n'
                 '# --- Install any needed packages specified in requirements.pip\r\n'
                 'COPY requirements.pip .\r\n'
                 'RUN pip install -U --trusted-host pypi.python.org -r '
                 'requirements.pip\r\n'
                 '\r\n'
                 '# activate jupyter extensions\r\n'
                 'RUN jupyter contrib nbextension install \\\r\n'
                 '  && jupyter nbextension enable codefolding/main \\\r\n'
                 '  && jupyter nbextension enable collapsible_headings/main\r\n'
                 '```\r\n'
                 '\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '\r\n'
                 'A MWE [repo](https://gitlab.com/SumNeuron/mytf) contains the '
                 'following files:\r\n'
                 '\r\n'
                 '- `Dockerfile.ai`: custom image built on top of official '
                 'tensorflow/tensorflow\r\n'
                 '- `docker-compose.ai.development.yml`: specifies `Dockerfile.ai` as '
                 'build file and mounts `notebooks` directory\r\n'
                 '- `requirements.pip`: pip requirements that may be used in images '
                 'other than `Dockerfile.ai`\r\n'
                 '- `requirements.apt`: packages needed to be installed via '
                 '`apt-get`\r\n'
                 '\r\n'
                 'For convenience a python script `docker.py` is provided, e.g. \r\n'
                 '```\r\n'
                 'python docker.py -c {build | up | down}\r\n'
                 '```\r\n'
                 'instead of \r\n'
                 '```\r\n'
                 'docker-compose -f docker-compose.ai.development.yml {build | up | '
                 'down}\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2020-01-0'},
        {'body': '## System information\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): '
                 'Darwin-19.2.0-x86_64-i386-64bit\r\n'
                 '- TensorFlow installed from (source or binary): Pipenv\r\n'
                 '- TensorFlow version (use command below): 2.0.0\r\n'
                 '- Python version: 3.7.5\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '## Current behavior\r\n'
                 '\r\n'
                 'The EarlyStopping callback will restore the best weights only if it '
                 'itself stops training. For example, if a model is set to train for '
                 '40 epochs, uses early stopping with a patience of 5, and the best '
                 'weights occur in epoch 37, then the best weights will not be '
                 'restored, and the model will have the weights from epoch 40.\r\n'
                 '\r\n'
                 '## Expected behavior\r\n'
                 '\r\n'
                 'EarlyStopping should, when initialized with '
                 '`restore_best_weights=True`, always restore the best weights when '
                 'training stops, regardless of the reason why training stopped.\r\n'
                 '\r\n'
                 '## Code to reproduce the issue\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 '\r\n'
                 '```python\r\n'
                 'import tensorflow as tf\r\n'
                 'from numpy.random import RandomState\r\n'
                 'from tensorflow.keras.callbacks import EarlyStopping\r\n'
                 'from tensorflow.keras.layers import Dense\r\n'
                 'from tensorflow.keras.models import Sequential\r\n'
                 '\r\n'
                 'N_CLASSES, N_SAMPLES = 5, 500\r\n'
                 '\r\n'
                 'for seed in [1, 2]:\r\n'
                 '    print(f"=== Random seed is {seed} ===")\r\n'
                 '\r\n'
                 '    tf.random.set_seed(seed)\r\n'
                 '    rng = RandomState(seed)\r\n'
                 '\r\n'
                 '    x_train = rng.standard_normal(size=(N_SAMPLES, 10))\r\n'
                 '    x_test = rng.standard_normal(size=(N_SAMPLES, 10))\r\n'
                 '    y_train = rng.random_integers(N_CLASSES, size=N_SAMPLES)\r\n'
                 '    y_test = rng.random_integers(N_CLASSES, size=N_SAMPLES)\r\n'
                 '\r\n'
                 '    model = Sequential([Dense(32), Dense(N_CLASSES)])\r\n'
                 '    model.compile("adam", "categorical_crossentropy", '
                 '["accuracy"])\r\n'
                 '    early_stopping = EarlyStopping(\r\n'
                 '        "val_accuracy", patience=10, verbose=1, '
                 'restore_best_weights=True\r\n'
                 '    )\r\n'
                 '    history = model.fit(\r\n'
                 '        x_train,\r\n'
                 '        y_train,\r\n'
                 '        epochs=20,\r\n'
                 '        callbacks=[early_stopping],\r\n'
                 '        verbose=0,\r\n'
                 '        validation_data=(x_test, y_test),\r\n'
                 '    )\r\n'
                 '    best_acc = max(history.history["val_accuracy"])\r\n'
                 '    _, eval_acc = model.evaluate(x_test, y_test, verbose=0)\r\n'
                 '    print(f"Best accuracy in training: {best_acc}. In evaluation: '
                 '{eval_acc}\\n")\r\n'
                 '```\r\n'
                 '\r\n'
                 '### Output from example:\r\n'
                 '\r\n'
                 '```txt\r\n'
                 '=== Random seed is 1 ===\r\n'
                 'Restoring model weights from the end of the best epoch.\r\n'
                 'Epoch 00011: early stopping\r\n'
                 'Best accuracy in training: 0.19200000166893005. In evaluation: '
                 '0.19200000166893005\r\n'
                 '\r\n'
                 '=== Random seed is 2 ===\r\n'
                 'Best accuracy in training: 0.15199999511241913. In evaluation: '
                 '0.14000000059604645\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 '## Other info / logs\r\n'
                 '\r\n'
                 '- Numpy version: 1.18.1',
         'created_at': '2020-01-0'},
        {'body': 'I trying to run one model on TFite mobile GPU using opencl and '
                 'getting below error:\r\n'
                 '\r\n'
                 '> INFO: Initialized TensorFlow Lite '
                 'runtime.                                                                                           \r\n'
                 '> INFO: Created TensorFlow Lite delegate for '
                 'GPU.                                                                                      \r\n'
                 '> ERROR: Next operations are not supported by GPU '
                 'delegate:                                                                            \r\n'
                 '> SPLIT_V: Operation is not '
                 'supported.                                                                                                 \r\n'
                 '> First 294 operations will run on the GPU, and the remaining 40 on '
                 'the '
                 'CPU.                                                           \r\n'
                 '> INFO: Initialized OpenCL-based '
                 'API.                                                                                                  \r\n'
                 '> Applied GPU '
                 'delegate.                                                                                                                \r\n'
                 '> Initialized session in '
                 '107237ms.                                                                                                     \r\n'
                 '> Running benchmark for at least 1 iterations and at least 0.5 '
                 'seconds but terminate if exceeding 150 '
                 'seconds.                         \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to read data from GPU '
                 '(clEnqueueReadBuffer) - Execution status error for events in wait '
                 'list \r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to '
                 'invoke.                                                                       \r\n'
                 '>                                                                                                                                      \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to '
                 'invoke.                                                                                                            \r\n'
                 '>                                                                                                                                      \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> ERROR: TfLiteGpuDelegate Invoke: Failed to upload data to GPU '
                 '(clEnqueueWriteBuffer) - Execution status error for events in wait '
                 'list\r\n'
                 '> ERROR: Node number 334 (TfLiteGpuDelegateV2) failed to invoke.\r\n'
                 '> \r\n'
                 '> count=138 first=73906 curr=1840 min=1414 max=73906 avg=3142.52 '
                 'std=6057\r\n'
                 '> \r\n'
                 '\r\n'
                 'Please help me to resolve this issue\r\n',
         'created_at': '2020-01-0'},
        {'body': 'Please be aware this issue was originally posted in '
                 'tensorflow/datasets but I got directed here as it seems that the '
                 'issue is related to the GFile implementation:\r\n'
                 'https://github.com/tensorflow/datasets/issues/1337\r\n'
                 '\r\n'
                 '\r\n'
                 '**Short description**\r\n'
                 'When the download of imagenet_resized finished and tfds starts '
                 'extracting/writing records, the program crashes.\r\n'
                 '\r\n'
                 'You can reproduce this error by downloading the particular zip file '
                 'manually and extracting it with tensorflow:\r\n'
                 '\r\n'
                 'http://www.image-net.org/image/downsample/Imagenet32_train_npz.zip\r\n'
                 '\r\n'
                 '**Environment information**\r\n'
                 '* Operating System: Windows 10\r\n'
                 '* Python version: 3.7\r\n'
                 '* tensorflow-datasets version: 1.3.2\r\n'
                 '* tensorflow-gpu version: 2.0.0\r\n'
                 '\r\n'
                 '**Reproduction instructions**\r\n'
                 'Without TFDS:\r\n'
                 '```\r\n'
                 'import zipfile\r\n'
                 'import tensorflow.compat.v2 as tf\r\n'
                 '\r\n'
                 "path = 'path/to/file.zip'\r\n"
                 "with tf.io.gfile.GFile(path, 'rb') as fobj:\r\n"
                 '  z = zipfile.ZipFile(fobj)\r\n'
                 '  for member in z.infolist():\r\n'
                 '    extract_file = z.open(member)\r\n'
                 '    print(member.filename)\r\n'
                 '```\r\n'
                 '\r\n'
                 '\r\n'
                 'With TFDS:\r\n'
                 '\r\n'
                 '```\r\n'
                 'import tensorflow_datasets as tfds\r\n'
                 '\r\n'
                 'imagenet_data, info = tfds.load(name="imagenet_resized/32x32", '
                 'with_info=True, as_supervised=True)\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Link to logs**\r\n'
                 '```\r\n'
                 'Dl Size...: 100%|██████████| 3414/3414 [22:47<00:00,  2.60 '
                 'MiB/s]\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '0 examples [00:00, ? examples/s]Traceback (most recent call '
                 'last):\r\n'
                 '  File "C:\\Program Files\\Python37\\lib\\contextlib.py", line 130, '
                 'in __exit__\r\n'
                 '    self.gen.throw(type, value, traceback)\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py", '
                 'line 199, in incomplete_dir\r\n'
                 '    yield tmp_dir\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py", '
                 'line 333, in download_and_prepare\r\n'
                 '    download_config=download_config)\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py", '
                 'line 1008, in _download_and_prepare\r\n'
                 '    '
                 'max_examples_per_split=download_config.max_examples_per_split,\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py", '
                 'line 871, in _download_and_prepare\r\n'
                 '    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py", '
                 'line 1033, in _prepare_split\r\n'
                 '    total=split_info.num_examples, leave=False):\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\_tqdm.py", '
                 'line 1005, in __iter__\r\n'
                 '    for obj in iterable:\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\image\\imagenet_resized.py", '
                 'line 141, in _generate_examples\r\n'
                 '    for fname, fobj in archive:\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_datasets\\core\\download\\extractor.py", '
                 'line 179, in iter_zip\r\n'
                 '    z = zipfile.ZipFile(fobj)\r\n'
                 '  File "C:\\Program Files\\Python37\\lib\\zipfile.py", line 1225, '
                 'in __init__\r\n'
                 '    self._RealGetContents()\r\n'
                 '  File "C:\\Program Files\\Python37\\lib\\zipfile.py", line 1288, '
                 'in _RealGetContents\r\n'
                 '    endrec = _EndRecData(fp)\r\n'
                 '  File "C:\\Program Files\\Python37\\lib\\zipfile.py", line 259, in '
                 '_EndRecData\r\n'
                 '    fpin.seek(0, 2)\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\util\\deprecation.py", '
                 'line 507, in new_func\r\n'
                 '    return func(*args, **kwargs)\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py", '
                 'line 167, in seek\r\n'
                 '    offset += self.size()\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py", '
                 'line 102, in size\r\n'
                 '    return stat(self.__name).length\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py", '
                 'line 727, in stat\r\n'
                 '    return stat_v2(filename)\r\n'
                 '  File '
                 '"C:\\Users\\[username]\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py", '
                 'line 744, in stat_v2\r\n'
                 '    pywrap_tensorflow.Stat(compat.as_bytes(path), '
                 'file_statistics)\r\n'
                 'tensorflow.python.framework.errors_impl.OutOfRangeError: '
                 'C:\\Users\\[username]\\tensorflow_datasets\\downloads\\image-net.org_image_downs_Image_train_npzlCJjN-zBsDCdn80BZxJ6qtyTFYcDX7y1OSUjXtuuxPw.zip; '
                 'Unknown error\r\n'
                 '\r\n'
                 'Process finished with exit code 1\r\n'
                 '```\r\n'
                 '**Expected behavior**\r\n'
                 'No error',
         'created_at': '2020-01-0'},
        {'body': 'Thank you for submitting a TensorFlow documentation issue. Per our '
                 'GitHub\r\n'
                 'policy, we only address code/doc bugs, performance issues, feature '
                 'requests, and\r\n'
                 'build/installation issues on GitHub.\r\n'
                 '\r\n'
                 'The TensorFlow docs are open source! To get involved, read the '
                 'documentation\r\n'
                 'contributor guide: '
                 'https://www.tensorflow.org/community/contribute/docs\r\n'
                 '\r\n'
                 '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'Please provide a link to the documentation entry, for example:\r\n'
                 'https://www.tensorflow.org/api_docs/python/tf/gradients?version=stable\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 "It's unclear how many list items are returned from "
                 '`tf.gradients`.\r\n'
                 '\r\n'
                 'The second paragraph states that "It returns a list of Tensor of '
                 'length `len(xs)` where each tensor is the `sum(dy/dx)` for y in '
                 '`ys`." The "Returns" section says, "A list of `sum(dy/dx)` for each '
                 'x in `xs`."\r\n'
                 '\r\n'
                 'So... which one is it? `sum(dy/dx)` for x in `xs` or `sum(dy/dx)` '
                 'for y in `ys`? Besides the inconsistency, the summation notation in '
                 'this documentation is ambiguous. When it says "`sum(dy/dx)` for x '
                 'in `xs`" does that mean `dy/dx` is summed over the `ys` axis and '
                 'there is one element produced for each `xs` or the other way '
                 'around?\r\n'
                 '\r\n'
                 'A clarifying example would help and a statement along the lines of '
                 '"returns a list of <whatever> with as many elements as `xs`" (or '
                 "`ys` -- I don't know).\r\n"
                 '\r\n'
                 '### Clear description\r\n'
                 '\r\n'
                 'For example, why should someone use this method? How is it '
                 'useful?\r\n'
                 '\r\n'
                 '### Correct links\r\n'
                 '\r\n'
                 'Is the link to the source code correct?\r\n'
                 '\r\n'
                 '### Parameters defined\r\n'
                 '\r\n'
                 'Are all parameters defined and formatted correctly?\r\n'
                 '\r\n'
                 '### Returns defined\r\n'
                 '\r\n'
                 'Are return values defined?\r\n'
                 '\r\n'
                 '### Raises listed and defined\r\n'
                 '\r\n'
                 'Are the errors defined? For example,\r\n'
                 'https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n'
                 '\r\n'
                 '### Usage example\r\n'
                 '\r\n'
                 'Is there a usage example?\r\n'
                 '\r\n'
                 'See the API guide: '
                 'https://www.tensorflow.org/community/contribute/docs_ref\r\n'
                 'on how to write testable usage examples.\r\n'
                 '\r\n'
                 '### Request visuals, if applicable\r\n'
                 '\r\n'
                 'Are there currently visuals? If not, will it clarify the '
                 'content?\r\n'
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 'Are you planning to also submit a pull request to fix the issue? '
                 'See the docs\r\n'
                 'contributor guide: '
                 'https://www.tensorflow.org/community/contribute/docs,\r\n'
                 'docs API guide: '
                 'https://www.tensorflow.org/community/contribute/docs_ref and the\r\n'
                 'docs style guide: '
                 'https://www.tensorflow.org/community/contribute/docs_style\r\n',
         'created_at': '2020-01-0'},
        {'body': 'I download source code of SSD from github.com. I want to convert '
                 'the model to run in edgetpu.\r\n'
                 "When I convert saved model to tflite, then run 'edgetpu_compiler "
                 "XXX.tflite' for running in tpu. But it tell me that not "
                 'quantization, I change parameter for quantization.Such as \r\n'
                 'converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8], \r\n'
                 'but occur error: RuntimeError: Invalid quantization params for op '
                 'RESHAPE at index 36 in subgraph 0\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '16.04.6 LTS\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (or github SHA if from source):1.15\r\n'
                 '\r\n'
                 '\r\n'
                 '**Command used to run the converter or code if you’re using the '
                 'Python API**\r\n'
                 '\r\n'
                 '```\r\n'
                 'import tensorflow as tf\r\n'
                 'from PIL import Image\r\n'
                 'import numpy as np\r\n'
                 '\r\n'
                 'ckpt_filename = '
                 "'/home/lanjunc/python_project/SSD-Tensorflow/convert/model'\r\n"
                 '\r\n'
                 "img = Image.open('./dog.jpg')\r\n"
                 'img = img.resize((300, 300))\r\n'
                 'input_data = np.array(img, dtype=np.float32)\r\n'
                 'def representative_dataset_gen():\r\n'
                 '    for i in range(1):\r\n'
                 '        yield [input_data]\r\n'
                 '\r\n'
                 '\r\n'
                 'converter = '
                 'tf.lite.TFLiteConverter.from_saved_model(ckpt_filename,\r\n'
                 '                                                     # '
                 "tag_set=['test_saved_model1'], signature_key='signature_test1',\r\n"
                 '                                                     '
                 'input_shapes={"Placeholder":[300,300,3]})\r\n'
                 '\r\n'
                 'converter.optimizations = [tf.lite.Optimize.DEFAULT]  # DEFAULT, '
                 'OPTIMIZE_FOR_SIZE, OPTIMIZE_FOR_LATENCY\r\n'
                 '\r\n'
                 'converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, '
                 'tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n'
                 '# converter.target_spec.supported_ops = [\r\n'
                 '#                                        '
                 'tf.lite.OpsSet.SELECT_TF_OPS]\r\n'
                 '#\r\n'
                 'converter.inference_input_type = tf.uint8\r\n'
                 'converter.inference_output_type = tf.uint8\r\n'
                 'converter.representative_dataset = representative_dataset_gen\r\n'
                 '\r\n'
                 'tflite_model = converter.convert()\r\n'
                 '\r\n'
                 'open("converted_model.tflite", "wb").write(tflite_model)\r\n'
                 '```\r\n'
                 '\r\n'
                 '**The output from the converter invocation**\r\n'
                 '\r\n'
                 '```\r\n'
                 'RuntimeError: Invalid quantization params for op RESHAPE at index '
                 '36 in subgraph 0\r\n'
                 '```\r\n'
                 '\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- TensorFlow version (you are using): 2.0\r\n'
                 '- Are you willing to contribute it (Yes/No): -\r\n'
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 '\r\n'
                 'I would like to see the `erfcx(x) = exp(x**2) * erfc(x)` function '
                 '(e.g., `scipy.special.erfcx`) implemented in Tensorflow. The naive '
                 'implementation `exp(x**2) * erfc(x)` fails for large `x`.\r\n'
                 '\r\n'
                 'This is useful when one needs to work with truncated normal '
                 'distributions. \r\n'
                 '\r\n'
                 '`erfcx` is available in many numerical packages, such as '
                 '[Matlab](https://www.mathworks.com/help/matlab/ref/erfcx.html), '
                 '[Julia](https://juliamath.github.io/SpecialFunctions.jl/latest/functions_list/#SpecialFunctions.erfcx), '
                 '[SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erfcx.html) '
                 '[R](https://www.rdocumentation.org/packages/pracma/versions/1.9.9/topics/erf), '
                 'and others.\r\n'
                 '\r\n'
                 'An alternative is to implement `log_erfc(x) = log(erfc(x))`, from '
                 'which one can obtain `erfcx(x)` accurately. The function '
                 '`log_erfc(x)` is implemented in '
                 '[GSL](https://github.com/ampl/gsl/blob/644e768630841bd085cb7121085a688c4ff424d0/specfunc/erfc.c#L441).\r\n'
                 '\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 '\r\n'
                 'Anyone working with truncated normal distributions.\r\n',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a feature request. As per our '
                 '[GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. '
                 'tag:feature_template</em>\r\n'
                 '\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- TensorFlow version (you are using):2.0\r\n'
                 '- Are you willing to contribute it (Yes/No):Yes\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 'perfectly normal\r\n'
                 '**Will this change the current api? How?**\r\n'
                 'Yes. The beginner models can be a different api in tensorflow '
                 'itself. Like `tf.BeginnerModel`.\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 'Beginners in machine learning who want to write less code and '
                 'acheive a lot\r\n'
                 '**Any Other info.**\r\n'
                 'This will make it easier to use tensorflow as a beginner',
         'created_at': '2020-01-0'},
        {'body': 'The codelab still links to the experimental folder for the '
                 'makefile, which is incorrect.\r\n'
                 '\r\n'
                 'Visual:\r\n'
                 '![image](https://user-images.githubusercontent.com/997157/71787651-3bc27180-2fe0-11ea-8318-424dc887efc5.png)\r\n'
                 '\r\n'
                 'Update the codelab at this link '
                 'https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n'
                 '\r\n'
                 'Code should read:\r\n'
                 '```\r\n'
                 'make -f tensorflow/lite/micro/tools/make/Makefile '
                 '\\                                    \r\n'
                 'TARGET=sparkfun_edge micro_speech_bin\r\n'
                 '```\r\n'
                 '\r\n'
                 '## URL(s) with the issue:\r\n'
                 'https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#3\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: a2cf331a2073d6f5e9a52802cbc2809f9e2667b8\r\n'
                 '- Python version: Python 3.6.8\r\n'
                 '- Installed using virtualenv? pip? conda?:\r\n'
                 '- Bazel version (if compiling from source): 0.29.1\r\n'
                 '- GCC/Compiler version (if compiling from source): '
                 'arm-none-eabi-gcc (15:6.3.1+svn253039-1build1) 6.3.1 20170620\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'I am unable to build hello_world for sparkfun_edge.\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 'I followed the instructions on '
                 'https://www.tensorflow.org/lite/microcontrollers/library\r\n'
                 'and issued the command:\r\n'
                 '`make -f tensorflow/lite/micro/tools/make/Makefile '
                 'TARGET=sparkfun_edge hello_world_bin`\r\n'
                 '\r\n'
                 'This results in the following build error:\r\n'
                 '```\r\n'
                 'arm-none-eabi-ar: creating '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/lib/libtensorflow-microlite.a\r\n'
                 'arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g '
                 '-DTF_LITE_STATIC_MEMORY -fno-rtti -DPART_apollo3 -DAM_PACKAGE_BGA '
                 '-DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '
                 '-DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D '
                 '__FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 '
                 '-fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections '
                 '-fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb '
                 '-mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra '
                 '-Wno-unused-parameter -Wno-missing-field-initializers '
                 '-Wno-write-strings -Wno-sign-compare '
                 '-fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive '
                 '-nostdlib -ggdb -O3 -I. '
                 '-Itensorflow/lite/micro/tools/make/downloads/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/gemmlowp '
                 '-Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include '
                 '-isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ '
                 '-isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/CMSIS_ext/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/gcc_embedded//arm-none-eabi/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/CMSIS/AmbiqMicro/Include/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp '
                 '-Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/devices/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/utils/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/kissfft '
                 '-Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_accelerometer/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/examples/example1_edge_test/src/tf_adc/ '
                 '-o '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/main.o '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sine_model_data.o '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sparkfun_edge/output_handler.o '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/obj/tensorflow/lite/micro/examples/hello_world/sparkfun_edge/constants.o  '
                 'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/lib/libtensorflow-microlite.a '
                 '-mthumb -mcpu=cortex-m4 -mfpu=fpv4-sp-d16 -mfloat-abi=hard '
                 '-nostartfiles -static -Wl,--gc-sections -Wl,--entry,Reset_Handler '
                 '-Wl,--start-group -lm -lc -lgcc -Wl,--end-group -fno-exceptions '
                 '-nostdlib --specs=nano.specs -t -lstdc++ -lc -lnosys -lm '
                 '-Wl,-T,tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/apollo3_evb/examples/hello_world/gcc_patched/apollo3evb.ld '
                 '-Wl,-Map=tensorflow/lite/micro/tools/make/gen/sparkfun_edge.map,--cref '
                 'tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/boards/SparkFun_TensorFlow_Apollo3_BSP/bsp/gcc/bin/libam_bsp.a '
                 'tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.0.0/mcu/apollo3/hal/gcc/bin/libam_hal.a '
                 'tensorflow/lite/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o '
                 '-lm\r\n'
                 'arm-none-eabi-g++: error: '
                 'tensorflow/lite/micro/tools/make/downloads/gcc_embedded//lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/fpv4-sp/hard/crtbegin.o: '
                 'No such file or directory\r\n'
                 'tensorflow/lite/micro/examples/hello_world/Makefile.inc:41: recipe '
                 'for target '
                 "'tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world' "
                 'failed\r\n'
                 'make: *** '
                 '[tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/hello_world] '
                 'Error 1\r\n'
                 '```\r\n'
                 '\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 'Google Colab system (GPU mode) with TensorFlow 2.1-rc2 and Python '
                 '3.\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 "Bahdanau attention get a lot gpu memory, it's right? Even bahdanau "
                 'attention using concatenation approach, Luong attention get 2.5gb '
                 'of memory while bahdanau over 15gb. (using scale=False, '
                 'batch_size=128, units=512 both)\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'To use less memory\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'The model detail is '
                 '[here](https://drive.google.com/open?id=170KpjIGFmNNt-SS-6oXgVbRnZR4-7ah_) '
                 '(the same of this issue '
                 'https://github.com/tensorflow/tensorflow/issues/35553)',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: NO\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: 1.15.0\r\n'
                 '- Python version: 3.6.9\r\n'
                 '- Installed using virtualenv? pip? conda?: virtualenv\r\n'
                 '- Bazel version (if compiling from source): 0.26.1\r\n'
                 '- GCC/Compiler version (if compiling from source): 7.4.0\r\n'
                 '- CUDA/cuDNN version: 10.0.326\r\n'
                 '- GPU model and memory: 1080ti 11MB\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 'Build command:\r\n'
                 '\r\n'
                 '`bazel build -c opt --jobs 32 --copt=-mavx --copt=-mavx2 '
                 '--copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 '
                 '-k //tensorflow/tools/pip_package:build_pip_package '
                 '--cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" --define=grpc_no_ares=true\r\n'
                 '`\r\n'
                 '\r\n'
                 'Completed the build with bazel, and want to produce the libraries '
                 "and includes to install in my system, but I'm unable to find "
                 'anywhere ()many years experience with CMAKE, 0 with bazel).\r\n'
                 '\r\n'
                 'I found '
                 '[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md](url) '
                 "for C (NOT what I'm looking for but wanted to get started with "
                 'something..), and ran the command instructed there\r\n'
                 '\r\n'
                 '`bazel test --config opt '
                 '//tensorflow/tools/lib_package:libtensorflow_test`\r\n'
                 '\r\n'
                 'But returns \r\n'
                 '\r\n'
                 '`ERROR: Config value opt is not defined in any .rc file`\r\n'
                 '\r\n'
                 'But the real question is how do I install tensorflow 1.15 C++ \r\n'
                 '\r\n'
                 'Many thanks!!\r\n',
         'created_at': '2020-01-0'},
        {'body': 'Hi all, \r\n'
                 '\r\n'
                 'I am running a training loop using gradientTape which works well, '
                 'however I am getting different training accuracy metrics when '
                 'training using the gradientTape loop vs a straight model.fit '
                 'method. I apologise if this should be a question for stack '
                 'overflow, however, to the best of my knowledge the parameters are '
                 'the same and therefore should be producing exactly the same results '
                 '(or very close at least).. I therefore think there may be a bug and '
                 'if any one can help me elucidate this i would really appreciate '
                 'it!\r\n'
                 '\r\n'
                 'I have prepared a sequential model as follows:\r\n'
                 '\r\n'
                 '```\r\n'
                 'model=tf.keras.models.Sequential()\r\n'
                 'model.add(tf.keras.layers.Dense(units=64, input_dim=5078, '
                 'activation="relu"))\r\n'
                 'model.add(tf.keras.layers.Dense(units=32, activation="relu"))\r\n'
                 'model.add(tf.keras.layers.Dense(units=100, activation="relu"))\r\n'
                 'model.add(tf.keras.layers.Dense(units=24, activation="sigmoid"))\r\n'
                 '```\r\n'
                 'and for the ` model.fit` method, fit as follows:\r\n'
                 '\r\n'
                 '```\r\n'
                 'model.compile(optimizer="Adam", loss="binary_crossentropy", '
                 'metrics=["acc"])\r\n'
                 '\r\n'
                 'model.fit(X_train, y_train,\r\n'
                 ' batch_size=32,\r\n'
                 ' epochs=100, verbose=1,\r\n'
                 ' validation_split=0.15,\r\n'
                 ' shuffle=True)\r\n'
                 '```\r\n'
                 'This works well and produces the following results (please note 100 '
                 'epochs is overkill and the model overfits, however this is just to '
                 'keep the same epochs as the as the gradientTape loop, otherwise '
                 'there would be an early-stopping callback normally...\r\n'
                 '\r\n'
                 'The model metrics are as follows:\r\n'
                 '\r\n'
                 '```\r\n'
                 ' 32/119 [=======>......................] - ETA: 0s - loss: 0.0699 - '
                 'acc: 0.9753\r\n'
                 '119/119 [==============================] - 0s 168us/sample - '
                 '**loss: 0.0668** - acc: **0.9779** - val_loss: **0.2350** - '
                 'val_acc: **0.9048**\r\n'
                 '```\r\n'
                 '\r\n'
                 'This is the expected behaviour (minus the overfitting)... Now when '
                 'I create the gradientTape loop as follows, the accuracy metrics are '
                 'of by about ~4-5% during the same 100 epochs, and the reason i '
                 'suspect a bug is because i believe i am using the appropriate '
                 'metrics:\r\n'
                 '\r\n'
                 '```\r\n'
                 'def random_batch(X,y, batch_size=32):\r\n'
                 '    idx= np.random.randint(len(X), size=batch_size)\r\n'
                 '    return X[idx], y[idx]\r\n'
                 '\r\n'
                 '##Further split train data to training set and validation set\r\n'
                 '\r\n'
                 'X_train, X_val, y_train, y_val = train_test_split(\r\n'
                 '    X_train, y_train, test_size=0.15, random_state=1)\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 '```\r\n'
                 '##Run autodiff on model\r\n'
                 '\r\n'
                 'n_epochs=100\r\n'
                 'batch_size=32\r\n'
                 'n_steps=len(X_train)//batch_size\r\n'
                 '\r\n'
                 'optimizer=tf.keras.optimizers.Adam()\r\n'
                 'loss=tf.keras.losses.BinaryCrossentropy()\r\n'
                 '\r\n'
                 'metricLoss=tf.keras.metrics.BinaryCrossentropy()\r\n'
                 'metricsAcc=tf.keras.metrics.BinaryAccuracy()\r\n'
                 '\r\n'
                 'val_acc_metric=tf.keras.metrics.BinaryAccuracy()\r\n'
                 'val_acc_loss=tf.keras.metrics.BinaryCrossentropy()\r\n'
                 '\r\n'
                 '\r\n'
                 'train_loss_results = []\r\n'
                 'train_accuracy_results = []\r\n'
                 '\r\n'
                 'validation_loss_results = []\r\n'
                 'validation_accuracy_results = []\r\n'
                 '\r\n'
                 '# for loop iterate over epochs\r\n'
                 'for epoch in range(n_epochs):\r\n'
                 '\r\n'
                 '    print("Epoch {}/{}".format(epoch, n_epochs))\r\n'
                 '\r\n'
                 '    # for loop iterate over batches\r\n'
                 '    for step in range(1, n_steps + 1):\r\n'
                 '        X_batch, y_batch=random_batch(X_train.values, y_train)\r\n'
                 '\r\n'
                 '        # gradientTape autodiff\r\n'
                 '        with tf.GradientTape() as tape:\r\n'
                 '            y_pred=model(X_batch, training=True)\r\n'
                 '            loss_values=loss(y_batch, y_pred)\r\n'
                 '        gradients=tape.gradient(loss_values, '
                 'model.trainable_weights)\r\n'
                 '        optimizer.apply_gradients(zip(gradients, '
                 'model.trainable_weights))\r\n'
                 '\r\n'
                 '        metricLoss(y_batch, y_pred)\r\n'
                 '        metricsAcc.update_state(y_batch, y_pred)\r\n'
                 '\r\n'
                 '        # Loss and accuracy\r\n'
                 '        train_loss_results.append(loss_values)\r\n'
                 '        train_accuracy_results.append(metricsAcc.result())\r\n'
                 '\r\n'
                 '        # Read out training results\r\n'
                 "        readout = 'Epoch {}, Training loss: {}, Training accuracy: "
                 "{}'\r\n"
                 '        print(readout.format(epoch + 1, loss_values,\r\n'
                 '                              metricsAcc.result() * 100))\r\n'
                 '\r\n'
                 '        metricsAcc.reset_states\r\n'
                 '\r\n'
                 '        # Run a validation loop at the end of each epoch\r\n'
                 '\r\n'
                 '    for valbatch in range(1+ n_steps +1):\r\n'
                 '        X_batchVal, y_batchVal = random_batch(X_val.values, '
                 'y_val)\r\n'
                 '\r\n'
                 '        val_logits = model(X_batchVal)\r\n'
                 '        # Update val metrics\r\n'
                 '        val_acc_metric(y_batchVal, val_logits)\r\n'
                 '        val_acc = val_acc_metric.result()\r\n'
                 '\r\n'
                 '        val_acc_metric.update_state(y_batchVal, val_logits)\r\n'
                 '\r\n'
                 '        val_loss=val_acc_loss(y_batchVal, val_logits)\r\n'
                 '\r\n'
                 '        validation_loss_results.append(val_loss)\r\n'
                 '        '
                 'validation_accuracy_results.append(val_acc_metric.result())\r\n'
                 '\r\n'
                 '        # Read out validation results\r\n'
                 "        print( 'Validation loss: ' , float(val_loss),'Validation "
                 "acc: %s' % (float(val_acc * 100),) )\r\n"
                 '\r\n'
                 '        val_acc_metric.reset_states()\r\n'
                 '```\r\n'
                 '\r\n'
                 'When i run this code, it works fine, and the iterations update the '
                 'states of the accuracy and loss: however, the training accuracy is '
                 'much lower than the model.fit method, after running also for 100 '
                 'epochs: showing final epoch result that is printed (each same epoch '
                 'is iterating over each batch):\r\n'
                 '\r\n'
                 'Epoch 100, Training loss: 0.027735430747270584, Training accuracy: '
                 '93.6534423828125\r\n'
                 'Epoch 100, Training loss: 0.03832387551665306, Training accuracy: '
                 '93.67249298095703\r\n'
                 '**Epoch 100, Training loss: 0.035500235855579376, Training '
                 'accuracy: 93.69097900390625**\r\n'
                 'Validation loss:  0.3204055726528168 Validation acc: '
                 '90.36458587646484\r\n'
                 'Validation loss:  0.32066160440444946 Validation acc: '
                 '89.71354675292969\r\n'
                 'Validation loss:  0.32083287835121155 Validation acc: '
                 '90.49479675292969\r\n'
                 'Validation loss:  0.3209479749202728 Validation acc: '
                 '90.10416412353516\r\n'
                 '**Validation loss:  0.32088229060173035 Validation acc: 90.625**\r\n'
                 '\r\n'
                 'As you can see,  the training accuracy is ~4-5% lower compared to '
                 'the model.fit method. The loss records fine, and also, the '
                 'validation data looks pretty much just like the validation data in '
                 'the model.fit method. \r\n'
                 '\r\n'
                 'Additionally, when i plot accuracy and loss in both model.fit and '
                 'geadientTape methods, the shape of the curves look pretty much the '
                 'same, and they both begin to overfit at similar points! but again, '
                 'there is a huge discrepancy in the training accuracy. \r\n'
                 '\r\n'
                 'I have specified the adam optimizer as well binary_crossentropy '
                 'loss in model.fit and gradientTape. For model.fit, when I specific '
                 "'accuracy' or 'acc' for metrics, my understanding is that it will "
                 'call on the binary_accuracy for calculating the accuracy. So as far '
                 'as I am aware the parameters are similar that results should be '
                 'fairly similar. \r\n'
                 '\r\n'
                 'Additionally, when i call` model.compile` after training the model '
                 'with `gradientTape` just to confirm evaluation, the results are '
                 'slightly different again and look more like the model.fit '
                 'method:\r\n'
                 '\r\n'
                 '```\r\n'
                 '**Training**\r\n'
                 'model.compile(optimizer=optimizer, '
                 "loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\r\n"
                 "print('\\n', model.evaluate(X_train, y_train, verbose=1)[1])\r\n"
                 '\r\n'
                 '32/101 [========>.....................] - ETA: 0s - loss: 0.0336 - '
                 'acc: 0.9948\r\n'
                 '101/101 [==============================] - 0s 307us/sample - '
                 '**loss: 0.0330 - acc: 0.9942**\r\n'
                 '\r\n'
                 '**Validation**\r\n'
                 'model.compile(optimizer=optimizer, '
                 "loss=tf.keras.losses.binary_crossentropy, metrics=['acc'])\r\n"
                 "print('\\n', model.evaluate(X_val, y_val, verbose=1)[1])\r\n"
                 '\r\n'
                 '18/18 [==============================] - 0s 111us/sample - **loss: '
                 '0.3879 - acc: 0.9028**\r\n'
                 '```\r\n'
                 '\r\n'
                 'Now model.evaluate shows a loss and accuracy that are very similar '
                 'to the model.fit method when i call evaluate on X_train and '
                 'y_train. This is why i am suspect of a bug? Interestingly, the '
                 'model.evaluate on validation data look similar to the gradientTape '
                 'loop which leaves me really confused as i am therefore unsure of '
                 'the true training accuracy and loss!\r\n'
                 '\r\n'
                 'If anyone can help i would really appreciate this... I am happy to '
                 'provide further code upstream of the model etc.. Again, apologies '
                 'if this is not a bug but this seems really confusing to me like an '
                 'incorrect behaviour...\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetpack '
                 '4.3 (L4T 32.3.1 Ubuntu 18.04)\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): Nvidia Official '
                 'Wheel\r\n'
                 '- TensorFlow version (use command below): 1.13.1 & 1.15.0\r\n'
                 '- Python version: 3.6\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10.0.326 / 7.5.0 & 7.6.3\r\n'
                 '- GPU model and memory: Tegra Xavier 16 GB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'I am currently trying some examples within from docker containers '
                 'on new release of Jetpack (4.3). I wanted to do some benchmarkings '
                 'and compare TensorRT 6.0.1 and cuDNN 7.6.3 versions with the old '
                 'ones (5.1.6 and 7.5.0 respectively). I have some issues regarding '
                 'to this as follows;\r\n'
                 '\r\n'
                 '1. **24 FPS** with --> protobuf=3.6.1, '
                 'tensorflow-gpu=1.13.1+nv19.3, tensorrt=5.1.6, cudnn=7.5.0 '
                 'opencv=3.3.1\r\n'
                 '2. **9.6 FPS** with --> protobuf=3.8.0, '
                 'tensorflow-gpu=1.15.0+nv19.11, tensorrt=6.0.1, cudnn=7.6.3, '
                 'opencv=4.1.1\r\n'
                 '3. **8.5 FPS** with --> protobuf=3.6.1, '
                 'tensorflow-gpu=1.15.0+nv19.11, tensorrt=6.0.1, cudnn=7.6.3, '
                 'opencv=4.1.1\r\n'
                 '4. **6.5 FPS** with --> protobuf=3.6.1, '
                 'tensorflow-gpu=1.15.0+nv19.11, tensorrt=5.1.6, cudnn=7.5.0, '
                 'opencv=3.4.6\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Improved inference performance (above 24 FPS) with the new versions '
                 'of Tensorflow, cuDNN and TensorRT\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 'https://github.com/jkjung-avt/tf_trt_models --> `python3 '
                 'camera_tf_trt.py --usb --model ssd_mobilenet_v1_coco --build`\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 '\r\n'
                 "I've created a "
                 '[topic](https://devtalk.nvidia.com/default/topic/1069238/jetson-agx-xavier/tensorrt-6-0-1-performs-worse-than-tensorrt-5-1-6-on-jetson-agx-xavier/) '
                 'on Nvidia DevTalk forum and here is the answer I got from Nvidia '
                 'people: [DevTalk forum '
                 'comment](https://devtalk.nvidia.com/default/topic/1069238/jetson-agx-xavier/tensorrt-6-0-1-performs-worse-than-tensorrt-5-1-6-on-jetson-agx-xavier/post/5416335/#5416335)',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Custom implementation of ALBERT for TF2.0\r\n'
                 '- Training on DataBricks\r\n'
                 '- TensorFlow installed from (source or binary): Binary pypi\r\n'
                 '- TensorFlow version (use command below): 2.0-gpu\r\n'
                 '- Python version: 3.7\r\n'
                 '- CUDA/cuDNN version: 10.1\r\n'
                 '- GPU model and memory: Databricks 4xGPU cluster X8\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'Current behavior: Training proceeds through 3 epochs and barfs on '
                 'Train Step 399/1330 in the _save_checkpoint routine. We are using '
                 'ADLSgen2 to store data, and we are mounting to that filesystem. '
                 'Filesystem works transparently for all tasks so far & we have '
                 'DataBricks team support. There are many read/writes to this mount '
                 "point so I am not quick to blame the filesystem - but it's still "
                 'worth noting.\r\n'
                 '\r\n'
                 'The actual error is:\r\n'
                 '2020-01-03 20:51:34.688654: W '
                 'tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at '
                 'save_restore_v2_ops.cc:137 : Invalid argument: '
                 '/dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; '
                 'Invalid argument\r\n'
                 '\r\n'
                 'When we navigate to this location we find the folder is genuinely '
                 'there, however inside that folder there is not this temp file. '
                 'Instead there are two files:\r\n'
                 '1)part-00000-of-00002.data-00000-of-00001\r\n'
                 '2)part-00000-of-00002.index\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'I would expect to see a new checkpoint at this step.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'This is made following the examples of '
                 'https://github.com/kamalkraj/ALBERT-TF2.0\r\n'
                 'If you simply run this example top to bottom on DataBricks you will '
                 'encounter this error.\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '\r\n'
                 'loss = 0.2804690897464752\r\n'
                 'I0103 20:48:47.790678 140033029736192 model_training_utils.py:346] '
                 'Train Step: 396/1330  / loss = 0.3561434745788574\r\n'
                 'I0103 20:48:50.499201 140033029736192 model_training_utils.py:346] '
                 'Train Step: 397/1330  / loss = 0.26206889748573303\r\n'
                 'I0103 20:48:53.168039 140033029736192 model_training_utils.py:346] '
                 'Train Step: 398/1330  / loss = 0.3312344551086426\r\n'
                 'I0103 20:48:55.838387 140033029736192 model_training_utils.py:346] '
                 'Train Step: 399/1330  / loss = 0.41013699769973755\r\n'
                 '2020-01-03 20:51:34.688654: W '
                 'tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at '
                 'save_restore_v2_ops.cc:137 : Invalid argument: '
                 '/dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; '
                 'Invalid argument\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py", '
                 'line 455, in <module>\r\n'
                 '    app.run(main)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/absl/app.py", '
                 'line 299, in run\r\n'
                 '    _run_main(main, args)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/absl/app.py", '
                 'line 250, in _run_main\r\n'
                 '    sys.exit(main(argv))\r\n'
                 '  File '
                 '"/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py", '
                 'line 356, in main\r\n'
                 '    custom_callbacks = custom_callbacks)\r\n'
                 '  File '
                 '"/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/model_training_utils.py", '
                 'line 354, in run_customized_training_loop\r\n'
                 '    checkpoint_name.format(step=current_step))\r\n'
                 '  File '
                 '"/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/model_training_utils.py", '
                 'line 33, in _save_checkpoint\r\n'
                 '    saved_path = checkpoint.save(checkpoint_path)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", '
                 'line 1889, in save\r\n'
                 '    file_path = self.write("%s-%d" % (file_prefix, '
                 'checkpoint_number))\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", '
                 'line 1819, in write\r\n'
                 '    output = self._saver.save(file_prefix=file_prefix)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", '
                 'line 1155, in save\r\n'
                 '    file_prefix=file_prefix_tensor, '
                 'object_graph_tensor=object_graph_tensor)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py", '
                 'line 1103, in _save_cached_when_graph_building\r\n'
                 '    save_op = saver.save(file_prefix)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", '
                 'line 230, in save\r\n'
                 '    sharded_saves.append(saver.save(shard_prefix))\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py", '
                 'line 72, in save\r\n'
                 '    return io_ops.save_v2(file_prefix, tensor_names, tensor_slices, '
                 'tensors)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py", '
                 'line 1933, in save_v2\r\n'
                 '    ctx=_ctx)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_io_ops.py", '
                 'line 1970, in save_v2_eager_fallback\r\n'
                 '    ctx=_ctx, name=name)\r\n'
                 '  File '
                 '"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py", '
                 'line 67, in quick_execute\r\n'
                 '    six.raise_from(core._status_to_exception(e.code, message), '
                 'None)\r\n'
                 '  File "<string>", line 3, in raise_from\r\n'
                 'tensorflow.python.framework.errors_impl.InvalidArgumentError: '
                 '/dbfs/mnt/devscoutprototype/eval_out/checkpoint/ctl_step_399.ckpt-3_temp_08f0418651184fbd97263e13fc11b45c/part-00001-of-00002.data-00000-of-00001.tempstate9461307533243821894; '
                 'Invalid argument [Op:SaveV2]\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'CalledProcessError                        Traceback (most recent '
                 'call last)\r\n'
                 '<command-1805221645764486> in <module>\r\n'
                 '----> 1 get_ipython().run_cell_magic(\'sh\', \'\', "\\n# Running '
                 'classifier:\\n\\nexport '
                 "GLUE_DIR='/dbfs/mnt/devscoutprototype/dataset/'\\nexport "
                 "ALBERT_DIR='/dbfs/mnt/devscoutprototype/models/base_2/'\\nexport "
                 "TASK_NAME='CoLA'\\nexport "
                 "OUTPUT_DIR='/dbfs/mnt/devscoutprototype/train_out'\\nexport "
                 "MODEL_DIR='/dbfs/mnt/devscoutprototype/eval_out/'\\n\\npython "
                 '/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py '
                 '--train_data_path=${OUTPUT_DIR}/${TASK_NAME}_train.tf_record '
                 '--eval_data_path=${OUTPUT_DIR}/${TASK_NAME}_eval.tf_record '
                 '--input_meta_data_path=${OUTPUT_DIR}/${TASK_NAME}_meta_data '
                 '--init_checkpoint=${ALBERT_DIR}/tf2_model.h5 '
                 '--spm_model_file=${ALBERT_DIR}/vocab/30k-clean.model '
                 '--albert_config_file=${ALBERT_DIR}/config.json '
                 '--output_dir=${MODEL_DIR} --do_train --task_name=${TASK_NAME} '
                 '--do_eval --custom_training_loop --train_batch_size=64 '
                 '--learning_rate=1e-5 --num_train_epochs=10\\n")\r\n'
                 '\r\n'
                 '/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py '
                 'in run_cell_magic(self, magic_name, line, cell)\r\n'
                 '   2350             with self.builtin_trap:\r\n'
                 '   2351                 args = (magic_arg_s, cell)\r\n'
                 '-> 2352                 result = fn(*args, **kwargs)\r\n'
                 '   2353             return result\r\n'
                 '   2354 \r\n'
                 '\r\n'
                 '/databricks/python/lib/python3.7/site-packages/IPython/core/magics/script.py '
                 'in named_script_magic(line, cell)\r\n'
                 '    140             else:\r\n'
                 '    141                 line = script\r\n'
                 '--> 142             return self.shebang(line, cell)\r\n'
                 '    143 \r\n'
                 '    144         # write a basic docstring:\r\n'
                 '\r\n'
                 '</databricks/python/lib/python3.7/site-packages/decorator.py:decorator-gen-110> '
                 'in shebang(self, line, cell)\r\n'
                 '\r\n'
                 '/databricks/python/lib/python3.7/site-packages/IPython/core/magic.py '
                 'in <lambda>(f, *a, **k)\r\n'
                 "    185     # but it's overkill for just that one bit of state.\r\n"
                 '    186     def magic_deco(arg):\r\n'
                 '--> 187         call = lambda f, *a, **k: f(*a, **k)\r\n'
                 '    188 \r\n'
                 '    189         if callable(arg):\r\n'
                 '\r\n'
                 '/databricks/python/lib/python3.7/site-packages/IPython/core/magics/script.py '
                 'in shebang(self, line, cell)\r\n'
                 '    243             sys.stderr.flush()\r\n'
                 '    244         if args.raise_error and p.returncode!=0:\r\n'
                 '--> 245             raise CalledProcessError(p.returncode, cell, '
                 'output=out, stderr=err)\r\n'
                 '    246 \r\n'
                 '    247     def _run_script(self, p, cell, to_close):\r\n'
                 '\r\n'
                 'CalledProcessError: Command \'b"\\n# Running '
                 'classifier:\\n\\nexport '
                 "GLUE_DIR='/dbfs/mnt/devscoutprototype/dataset/'\\nexport "
                 "ALBERT_DIR='/dbfs/mnt/devscoutprototype/models/base_2/'\\nexport "
                 "TASK_NAME='CoLA'\\nexport "
                 "OUTPUT_DIR='/dbfs/mnt/devscoutprototype/train_out'\\nexport "
                 "MODEL_DIR='/dbfs/mnt/devscoutprototype/eval_out/'\\n\\npython "
                 '/dbfs/mnt/devscoutprototype/ALBERT-TF2.0-master/run_classifer.py ',
         'created_at': '2020-01-0'},
        {'body': 'Using MacPorts with the latest up-to-date versions (tensorflow '
                 '2.0.0, protobuf 3.10.1),\r\n'
                 '\r\n'
                 "> `python3 -c 'import tensorflow'`\r\n"
                 '```\r\n'
                 '[libprotobuf ERROR google/protobuf/descriptor_database.cc:394] '
                 'Invalid file descriptor data passed to '
                 'EncodedDescriptorDatabase::Add().\r\n'
                 '[libprotobuf FATAL google/protobuf/descriptor.cc:1359] CHECK '
                 'failed: GeneratedDatabase()->Add(encoded_file_descriptor, '
                 'size): \r\n'
                 'libc++abi.dylib: terminating with uncaught exception of type '
                 'google::protobuf::FatalException: CHECK failed: '
                 'GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\n'
                 'Abort trap: 6\r\n'
                 '```\r\n'
                 '\r\n'
                 'This is reported and closed in '
                 'https://github.com/tensorflow/tensorboard/issues/2985, but the '
                 'issue persists.\r\n'
                 '\r\n'
                 "A workaround is `pip-3.7 install protobuf==3.8`, but this doesn't "
                 'work with system-level, up-to-date library installs like with '
                 'MacPorts. Also see: https://trac.macports.org/ticket/59826\r\n'
                 '\r\n'
                 'Is this an issue with TF, or with protobuf?',
         'created_at': '2020-01-0'},
        {'body': 'Hi,\r\n'
                 '\r\n'
                 "We have a TF code to create a graph which later on it's being "
                 'loaded into our Scala/Java application. \r\n'
                 '[Code to generate the '
                 'graph](https://github.com/JohnSnowLabs/spark-nlp/blob/9407da076eced850ec840cfc61e665887400de12/python/tensorflow/lib/ner/ner_model.py)\r\n'
                 '\r\n'
                 'In TF `1.12.0` and TF `1.13.1` we used to only see the device '
                 'placement logs in the console, but in TF `1.15.0` it logs over '
                 'thousands of lines complaining about:\r\n'
                 '\r\n'
                 '```\r\n'
                 'Failed to place the graph without changing the devices of some '
                 'resources. Some of the operations (that had to be colocated with '
                 "resource generating operations) are not supported on the resources' "
                 'devices. Current candidate devices are [\r\n'
                 '  /job:localhost/replica:0/task:0/device:CPU:0].\r\n'
                 '```\r\n'
                 '\r\n'
                 'The full log is here due to the length:\r\n'
                 'https://gist.github.com/maziyarpanahi/83f179e01634db2de12cb82501177c8d\r\n'
                 '\r\n'
                 'I understand what the log is saying and I know there is no way to '
                 'control the logging level through Java API, but I would like to '
                 'understand what has changed in TF 1.15.0 that we have now all these '
                 'logs without any change in our process.  \r\n'
                 '\r\n'
                 'Issue: It is really hard to debug with that many logs in the '
                 'console and also the Travis fails due to exceeding logs limit since '
                 'we have many unit tests using this graph and every time there will '
                 "be 1000 lines of logs saying something we really don't care.\r\n"
                 '\r\n'
                 'Any help to manage to suppress these logs would be highly '
                 'appreciated.\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n',
         'created_at': '2020-01-0'},
        {'body': '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l05c03_exercise_flowers_with_data_augmentation.ipynb\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 'In the directory structure, it should be "daisy" instead of '
                 '"diasy"\r\n'
                 '\r\n'
                 '![Screenshot from 2020-01-03 '
                 '18-39-11](https://user-images.githubusercontent.com/29497701/71725075-a8aaff80-2e58-11ea-9ac8-076078500026.png)\r\n'
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 'Yes\r\n',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): No\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04.3\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: NA\r\n'
                 '- TensorFlow installed from (source or binary): Binary\r\n'
                 '- TensorFlow version (use command below): v1.12.1-21401-gd908b50 '
                 '2.1.0-dev20191230\r\n'
                 '- Python version: 3.6.9\r\n'
                 '- Bazel version (if compiling from source): NA\r\n'
                 '- GCC/Compiler version (if compiling from source): NA\r\n'
                 '- CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1\r\n'
                 '- GPU model and memory: TITAN V, 12GB\r\n'
                 '\r\n'
                 'You can collect some of this information using our environment '
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n'
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'I have tried both combinations of copy_to_device("/gpu:0") followed '
                 'by prefetch and prefetch_to_device("/gpu:0"). In both cases the '
                 'resulting tensors are placed in "/cpu:0" after consuming them from '
                 'an iterator.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'My expectation is that the resulting tensors should still sit in '
                 'the device where they have been copied ("/gpu:0" in this case). '
                 'Executing simply a copy_to_device("/gpu:0") does return tensors '
                 'placed in the GPU.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 '\r\n'
                 '>>> import tensorflow as tf\r\n'
                 '>>> dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, '
                 '1])\r\n'
                 '>>> ds = dataset.batch(2).prefetch(1)\r\n'
                 '>>> for x in ds: print(x.device)\r\n'
                 '... \r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '>>> ds = '
                 'dataset.batch(2).apply(tf.data.experimental.copy_to_device("/gpu:0"))\r\n'
                 '>>> for x in ds: print(x.device)\r\n'
                 '... \r\n'
                 '/job:localhost/replica:0/task:0/device:GPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:GPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:GPU:0\r\n'
                 '>>> ds = '
                 'dataset.batch(2).apply(tf.data.experimental.copy_to_device("/gpu:0")).prefetch(1)\r\n'
                 '>>> for x in ds: print(x.device)\r\n'
                 '... \r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '>>> ds = '
                 'dataset.batch(2).apply(tf.data.experimental.prefetch_to_device("/gpu:0", '
                 '1))\r\n'
                 '>>> for x in ds: print(x.device)\r\n'
                 '... \r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '>>> \r\n'
                 '\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2020-01-0'},
        {'body': 'Actually  this is not a tensorflow problem. I had a project my own, '
                 'it run well. But it comes to an runtime error when I link it to the '
                 "the tensorflow_framework.so, even I didn't change any codes of my "
                 "project(it doesn't include any of tensoflow headers).  It seems "
                 "related to openssl certification, but it's hard to trace where the "
                 'collapse occurred. Is there anybody can help me? Thanks very '
                 'much!\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '...\r\n'
                 'terminate called after throwing an instance of '
                 "'std::runtime_error'\r\n"
                 '  what():  Unable to parse cert 1: error:0900006e:PEM '
                 'routines:OPENSSL_internal:NO_START_LINE\r\n'
                 '*** Aborted at 1578043256 (unix time) try "date -d @1578043256" if '
                 'you are using GNU date ***\r\n'
                 'PC: @     0x7ffb9e4b8e97 gsignal\r\n'
                 '*** SIGABRT (@0x3e800006475) received by PID 25717 (TID '
                 '0x7ffbaa404500) from PID 25717; stack trace: ***\r\n'
                 '    @     0x7ffba03c7890 (unknown)\r\n'
                 '    @     0x7ffb9e4b8e97 gsignal\r\n'
                 '    @     0x7ffb9e4ba801 abort\r\n'
                 '    @     0x7ffb9eead957 (unknown)\r\n'
                 '    @     0x7ffb9eeb3ab6 (unknown)\r\n'
                 '    @     0x7ffb9eeb3af1 std::terminate()\r\n'
                 '    @     0x7ffb9eeb3d24 __cxa_throw\r\n'
                 '    @     0x555e5286eea7 (unknown)\r\n'
                 '    @     0x555e5272d044 (unknown)\r\n'
                 '    @     0x555e5272d342 (unknown)\r\n'
                 '    @     0x555e5245ec1c (unknown)\r\n'
                 '    @     0x555e52479c7e (unknown)\r\n'
                 '    @     0x555e52479e98 (unknown)\r\n'
                 '    @     0x555e5244452e (unknown)\r\n'
                 '    @     0x7ffb9e49bb97 __libc_start_main\r\n'
                 '    @     0x555e52449dea (unknown)\r\n'
                 '\r\n'
                 '\r\n'
                 'version: tensorflow 1.14.0 \r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 'Google Colab system (GPU mode) with TensorFlow 2.1-rc2 and Python '
                 '3.\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'Take a long time to build/compile a seq2seq model when use '
                 "``set_floatx('float16')`` instead float32 (default)\r\n"
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 "I don't know if is normal, but default value ``float32`` takes 1-2 "
                 'second\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'I coded an example '
                 '[here](https://drive.google.com/open?id=170KpjIGFmNNt-SS-6oXgVbRnZR4-7ah_)',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 '- TensorFlow version:\r\n'
                 '- Python version:\r\n'
                 '- Installed using virtualenv? pip? conda?:\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 "Users in China can not download third-party tools from GitHub. It's "
                 'so frustrating. Can some kind person help me compile an optimized '
                 'version of TensorFlow-GPU-v2.0. THKS!\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Custom code for training ResNet50\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04\r\n'
                 '- TensorFlow installed from (source or binary): pip install '
                 '(precompiled binary, stable version)\r\n'
                 '- TensorFlow version (use command below): 2.0.0\r\n'
                 '- Python version: 3.7\r\n'
                 '- Bazel version (if compiling from source): NA\r\n'
                 '- GCC/Compiler version (if compiling from source): NA\r\n'
                 '- CUDA/cuDNN version: 10.0.130\r\n'
                 '- GPU model and memory: RTX 2060 (8GB)\r\n'
                 '- CPU: AMD Ryzen 3200\r\n'
                 '\r\n'
                 'I am seeing slow training in order of 41.4%  in TF2.0 because of '
                 '~50% CPU and GPU utilization. Most likely the issue seems to be in '
                 'memory transfer between CPU and GPU.  Other tf performance issues '
                 'are closed without any satisfactory answer hence I have written '
                 'standalone code in TF and pytorch for comparison. Please let me '
                 'know how I can improve performance of my tf training code given '
                 'below from 224ms/step to 133ms/step. This difference is almost '
                 'doubling days of training time for the large model that I am '
                 'training. \r\n'
                 '\r\n'
                 '-------TF code Start ------------\r\n'
                 '```python\r\n'
                 'import tensorflow as tf\r\n'
                 'import numpy as np\r\n'
                 'physical_devices = '
                 "tf.config.experimental.list_physical_devices('GPU')\r\n"
                 'tf.config.experimental.set_memory_growth(physical_devices[0], '
                 'True)\r\n'
                 '#tf.compat.v1.disable_eager_execution()\r\n'
                 '\r\n'
                 'class Lossfunc(tf.keras.losses.Loss):\r\n'
                 '    def __init__(self,\r\n'
                 '                 reduction=tf.keras.losses.Reduction.AUTO,\r\n'
                 '                    name = "Lossfunc"):\r\n'
                 '        super(Lossfunc, self).__init__(reduction=reduction,\r\n'
                 '                                                           '
                 'name=name)\r\n'
                 '\r\n'
                 '    def call(self, target, output):\r\n'
                 '        #target = NoneCHW\r\n'
                 '        loss = tf.keras.losses.MSE(target, output)\r\n'
                 '        return loss\r\n'
                 '\r\n'
                 "#channel = 'channels_last'\r\n"
                 '#IMG_SHAPE = tuple((256,256,3))\r\n'
                 '#output_shape = tuple((8,8,2048))\r\n'
                 "channel = 'channels_first'\r\n"
                 'IMG_SHAPE = tuple((3, 256,256))\r\n'
                 'output_shape = tuple((2048,8,8))\r\n'
                 'tf.keras.backend.set_image_data_format(channel)\r\n'
                 '\r\n'
                 'def data_gen():\r\n'
                 '    input = np.random.normal(0, 0.1, IMG_SHAPE)\r\n'
                 '    y = np.random.normal(0.5, 0.1, output_shape)\r\n'
                 '    yield input, y\r\n'
                 '\r\n'
                 'def ResNet():\r\n'
                 '      backbone = tf.keras.applications.ResNet50(\r\n'
                 '                                              '
                 'input_shape=IMG_SHAPE,\r\n'
                 '                                              include_top=False,\r\n'
                 '                                              '
                 "#weights='imagenet')\r\n"
                 '                                              weights=None)\r\n'
                 '\r\n'
                 '      final_output = backbone.output\r\n'
                 '      model = tf.keras.Model(inputs=backbone.input, '
                 'outputs=final_output)\r\n'
                 '      return model\r\n'
                 '\r\n'
                 'def main():\r\n'
                 '    # cudnn related setting ???\r\n'
                 '    model = ResNet()\r\n'
                 '    model.summary()\r\n'
                 '\r\n'
                 '    train_loader = tf.data.Dataset.from_generator(\r\n'
                 '                        data_gen,\r\n'
                 '                        output_types=(tf.float32, tf.float32),\r\n'
                 '                        output_shapes=((IMG_SHAPE),\r\n'
                 '                                       (output_shape)\r\n'
                 '                        )\r\n'
                 '                    )\r\n'
                 '    train_loader = '
                 'train_loader.repeat().batch(16).prefetch(tf.data.experimental.AUTOTUNE)\r\n'
                 '    criterion = Lossfunc()\r\n'
                 '    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n'
                 '    model.compile(loss=criterion, optimizer=optimizer)\r\n'
                 '    history = model.fit(train_loader,\r\n'
                 '                                  initial_epoch = 0,\r\n'
                 '                                  epochs=10,\r\n'
                 '                                  shuffle=False,\r\n'
                 '                                  steps_per_epoch=100,\r\n'
                 '                                  use_multiprocessing=True,\r\n'
                 '                                  workers=4)\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n"
                 '        main()\r\n'
                 '```\r\n'
                 'Output:\r\n'
                 '```\r\n'
                 '100/100 [==============================] - 31s 306ms/step - loss: '
                 '0.1300\r\n'
                 'Epoch 2/10\r\n'
                 '100/100 [==============================] - 23s 226ms/step - loss: '
                 '0.0171\r\n'
                 'Epoch 3/10\r\n'
                 '100/100 [==============================] - 22s 224ms/step - loss: '
                 '0.0138\r\n'
                 'Epoch 4/10\r\n'
                 '100/100 [==============================] - 23s 227ms/step - loss: '
                 '0.0123\r\n'
                 'Epoch 5/10\r\n'
                 '100/100 [==============================] - 23s 227ms/step - loss: '
                 '0.0121\r\n'
                 '```\r\n'
                 'CPU utilization: ~50% (None of the CPU thread is more than 60% '
                 'utilized at anytime)\r\n'
                 'GPU utilization: 56%\r\n'
                 '\r\n'
                 '---Pytorch code Start---\r\n'
                 '```python\r\n'
                 'import torch\r\n'
                 'import torchvision.models as models\r\n'
                 'import torch.backends.cudnn as cudnn\r\n'
                 'import torch.nn as nn\r\n'
                 'from torch.utils.data import Dataset\r\n'
                 'import numpy as np\r\n'
                 'import time\r\n'
                 '\r\n'
                 '# cudnn related setting\r\n'
                 'cudnn.benchmark = True\r\n'
                 'torch.backends.cudnn.deterministic = False\r\n'
                 'torch.backends.cudnn.enabled = True\r\n'
                 '\r\n'
                 'class Lossfunc(nn.Module):\r\n'
                 '    def __init__(self):\r\n'
                 '        super(Lossfunc, self).__init__()\r\n'
                 "        self.criterion = nn.MSELoss(reduction='mean')\r\n"
                 '    def forward(self, output, target):\r\n'
                 '        loss = self.criterion(output, target)\r\n'
                 '        return loss\r\n'
                 '\r\n'
                 'IMG_SHAPE = tuple((3, 256,256))\r\n'
                 'output_shape = tuple((2048,8,8))\r\n'
                 '\r\n'
                 'class GenData(Dataset):\r\n'
                 '    def __init__(self ):\r\n'
                 '        self.db = None\r\n'
                 '\r\n'
                 '    def __getitem__(self, idx):\r\n'
                 '        input = np.float32(np.random.normal(0, 0.1, IMG_SHAPE))\r\n'
                 '        y = np.float32(np.random.normal(0.5, 0.1, output_shape))\r\n'
                 '        return input, y\r\n'
                 '\r\n'
                 '    def __len__(self,):\r\n'
                 '        return 1600\r\n'
                 '\r\n'
                 'class ResNet(nn.Module):\r\n'
                 '    def  __init__(self):\r\n'
                 '        super(ResNet, self).__init__()\r\n'
                 '        backbone = models.resnet50(pretrained=True)\r\n'
                 '        self.backbone = '
                 'nn.Sequential(*list(backbone.children())[:-2])\r\n'
                 '\r\n'
                 '    def forward(self, input):\r\n'
                 '        x = self.backbone(input)\r\n'
                 '        return x\r\n'
                 '\r\n'
                 'def main():\r\n'
                 '    # cudnn related setting\r\n'
                 '\r\n'
                 '    model = ResNet()\r\n'
                 '    model = model.cuda()\r\n'
                 '    print(model)\r\n'
                 '\r\n'
                 '    train_dataset = GenData()\r\n'
                 '    train_loader = torch.utils.data.DataLoader(\r\n'
                 '                        train_dataset,\r\n'
                 '                        batch_size=16,\r\n'
                 '                        shuffle=False,\r\n'
                 '                        num_workers=4,\r\n'
                 '                        pin_memory=True\r\n'
                 '                    )\r\n'
                 '\r\n'
                 '    criterion = Lossfunc()\r\n'
                 '    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n'
                 '\r\n'
                 '    model.train()\r\n'
                 '    for epoch in range(10):\r\n'
                 '        start = time.time()\r\n'
                 '        for i, (input, y) in enumerate(train_loader):\r\n'
                 '            input = input.cuda()\r\n'
                 '            output = model(input)\r\n'
                 '            y = y.cuda(non_blocking=True)\r\n'
                 '            loss = criterion(output, y)\r\n'
                 '            optimizer.zero_grad()\r\n'
                 '            loss.backward()\r\n'
                 '            optimizer.step()\r\n'
                 '\r\n'
                 '        i +=1\r\n'
                 '        end = time.time() - start\r\n'
                 '        print("{} Steps in {} sec @ {} msec/step".format(i,end, '
                 '(end/i)*1000  ))\r\n'
                 '\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n"
                 '        main()\r\n'
                 '```\r\n'
                 'OUtput:\r\n'
                 '```\r\n'
                 '100 iterations in 14.790956020355225 sec @ 147.90956020355225 '
                 'msec/step\r\n'
                 '100 iterations in 13.290162563323975 sec @ 132.90162563323975 '
                 'msec/step\r\n'
                 '100 iterations in 13.326005935668945 sec @ 133.26005935668945 '
                 'msec/step\r\n'
                 '100 iterations in 13.390087127685547 sec @ 133.90087127685547 '
                 'msec/step\r\n'
                 '```\r\n'
                 'CPU Utilization: 60%\r\n'
                 'GPU Utilization: 98%\r\n'
                 '\r\n'
                 'The difference in GPU % utilization is same as % time '
                 'difference.\r\n'
                 '\r\n'
                 '\r\n',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '19.10\r\n'
                 '- TensorFlow installed from (source or binary): binary, conda\r\n'
                 '- TensorFlow version (use command below): unknown 2.0.0\r\n'
                 '- Python version: 3.7\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'Recursive calls with a `tf.function` decorated python function '
                 'results in a deadlock. A minimal example is provided below.\r\n'
                 '\r\n'
                 'Where it gets stuck: by recursively calling '
                 '`_maybe_define_function`, which internally requires a lock (line '
                 '2118 in `tensorflow_core/python/eager.function.py`). This seems to '
                 'deadlock when trying to create a graph function again invoking '
                 'itself.\r\n'
                 '\r\n'
                 'About use-cases: yes, there are. But yes, in principle, there are '
                 '"workarounds". But I assume this is not in general intended to '
                 'produce a deadlock anyway, should it? If so, I would rather propose '
                 'a loud failure.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n'
                 '@tf.function(autograph=False)\r\n'
                 'def func1(depth=0):\r\n'
                 '    if depth > 1:\r\n'
                 '        return depth\r\n'
                 '    else:\r\n'
                 '        return func1(depth + 1)\r\n'
                 '\r\n'
                 'func1(0)\r\n'
                 '```',
         'created_at': '2020-01-0'},
        {'body': '** Base System information**\r\n'
                 '- Linux Ubuntu 18.04\r\n'
                 '- nvidia-docker: Docker version 19.03.5, build 633a0ea838\r\n'
                 '- nvidia driver via: `sudo apt-get install nvidia-driver-418`\r\n'
                 '- CUDA/cuDNN version: 10.1\r\n'
                 '- GPU model and memory:\r\n'
                 '```\r\n'
                 '$ nvidia-smi \r\n'
                 'Thu Jan  2 12:08:27 2020       \r\n'
                 '+-----------------------------------------------------------------------------+\r\n'
                 '| NVIDIA-SMI 430.64       Driver Version: 430.64       CUDA '
                 'Version: 10.1     |\r\n'
                 '|-------------------------------+----------------------+----------------------+\r\n'
                 '| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile '
                 'Uncorr. ECC |\r\n'
                 '| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  '
                 'Compute M. |\r\n'
                 '|===============================+======================+======================|\r\n'
                 '|   0  GeForce GT 710      Off  | 00000000:AF:00.0 N/A '
                 '|                  N/A |\r\n'
                 '| 40%   46C    P0    N/A /  N/A |      0MiB /  2002MiB |     '
                 'N/A      Default |\r\n'
                 '+-------------------------------+----------------------+----------------------+\r\n'
                 '|   1  GeForce RTX 207...  Off  | 00000000:D8:00.0 Off '
                 '|                  N/A |\r\n'
                 '| 34%   39C    P0     1W / 215W |      0MiB /  7982MiB |      '
                 '0%      Default |\r\n'
                 '+-------------------------------+----------------------+----------------------+\r\n'
                 '                                                                               \r\n'
                 '+-----------------------------------------------------------------------------+\r\n'
                 '| Processes:                                                       '
                 'GPU Memory |\r\n'
                 '|  GPU       PID   Type   Process name                             '
                 'Usage      |\r\n'
                 '|=============================================================================|\r\n'
                 '|    0                    Not '
                 'Supported                                       |\r\n'
                 '+-----------------------------------------------------------------------------+\r\n'
                 '```\r\n'
                 '\r\n'
                 '\r\n'
                 '** Docker container information**\r\n'
                 '- Linux Ubuntu 18.04\r\n'
                 '- TensorFlow installed from: pip3\r\n'
                 '- TensorFlow version: 1.14.0\r\n'
                 '- Python version: 3.6.9\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory: nvidia-smi output (see above)\r\n'
                 '- my Dockerfile:\r\n'
                 '```\r\n'
                 '#https://www.tensorflow.org/install/gpu\r\n'
                 'FROM nvidia/cuda:10.0-base-ubuntu18.04\r\n'
                 '\r\n'
                 'ENV '
                 'PATH=/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/conda/bin:$PATH\r\n'
                 'RUN apt-get update && apt-get upgrade -y && apt-get install -y '
                 '\\\r\n'
                 '        nodejs \\\r\n'
                 '        npm \\\r\n'
                 '        python3-pip \\\r\n'
                 '        wget \\\r\n'
                 '        libmysqlclient-dev \\\r\n'
                 '        python-dev\r\n'
                 '#RUN wget -nv '
                 'https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh '
                 '&& bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda && rm '
                 'Miniconda3-latest-Lin$\r\n'
                 'RUN mkdir /etc/skel/notebooks\r\n'
                 'RUN npm install -g configurable-http-proxy && pip3 install \\\r\n'
                 '        jupyterhub \\\r\n'
                 '        jupyterhub-ldapauthenticator \\\r\n'
                 '        jupyterlab \\\r\n'
                 '        notebook \\\r\n'
                 '        tensorflow-gpu\r\n'
                 'RUN pip3 install \\\r\n'
                 '        folium \\\r\n'
                 '        keras \\\r\n'
                 '        matplotlib \\\r\n'
                 '        mysql \\\r\n'
                 '        mysql-connector \\\r\n'
                 '        pandas \\\r\n'
                 '        pymysql \\\r\n'
                 '        seaborn \\\r\n'
                 '        sklearn\r\n'
                 'VOLUME ["/home"]\r\n'
                 'RUN useradd -ms /bin/bash user -p "$(openssl passwd -1 test)"\r\n'
                 'COPY ./jupyterhub_config.py /etc/jupyterhub/jupyterhub_config.py\r\n'
                 'COPY ./jupyterhub_cookie_secret '
                 '/etc/jupyterhub/jupyterhub_cookie_secret\r\n'
                 'COPY ./jupyterhub.sqlite /etc/jupyterhub/jupyterhub.sqlite\r\n'
                 '#CMD ["/usr/local/bin/jupyterhub", "upgrade-db", '
                 '"--db=sqlite:////etc/jupyterhub/jupyterhub.sqlite"]\r\n'
                 'EXPOSE 8000\r\n'
                 'CMD ["/usr/local/bin/jupyterhub", "-f", '
                 '"/etc/jupyterhub/jupyterhub_config.py", "--debug"]\r\n'
                 '```\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 'I created a Docker container based on the Dockerfile (see above) '
                 'for a multiuser jupyterhub (with notebook and lab). The basics '
                 'works fine, I can log in and use the desired python packages and '
                 'running my projects. However, the container lacks the GPU '
                 'capabiltites.\r\n'
                 '\r\n'
                 '- container will be started by: `$ nvidia-docker run -d -p '
                 '8000:8000 --runtime=nvidia --restart unless-stopped --gpus all -v '
                 '~/myDocker/home:/home jupyterhub`\r\n'
                 '- `nvidia-smi` works within the container\r\n'
                 '- `device_lib.list_local_devices()` prints:\r\n'
                 '```[name: "/device:CPU:0"\r\n'
                 'device_type: "CPU"\r\n'
                 'memory_limit: 268435456\r\n'
                 'locality {\r\n'
                 '}\r\n'
                 'incarnation: 9710542890831123693\r\n'
                 ', name: "/device:XLA_GPU:0"\r\n'
                 'device_type: "XLA_GPU"\r\n'
                 'memory_limit: 17179869184\r\n'
                 'locality {\r\n'
                 '}\r\n'
                 'incarnation: 12724683280898329209\r\n'
                 'physical_device_desc: "device: XLA_GPU device"\r\n'
                 ', name: "/device:XLA_GPU:1"\r\n'
                 'device_type: "XLA_GPU"\r\n'
                 'memory_limit: 17179869184\r\n'
                 'locality {\r\n'
                 '}\r\n'
                 'incarnation: 12608392157467121046\r\n'
                 'physical_device_desc: "device: XLA_GPU device"\r\n'
                 ', name: "/device:XLA_CPU:0"\r\n'
                 'device_type: "XLA_CPU"\r\n'
                 'memory_limit: 17179869184\r\n'
                 'locality {\r\n'
                 '}\r\n'
                 'incarnation: 16188673672643731433\r\n'
                 'physical_device_desc: "device: XLA_CPU device"\r\n'
                 ']\r\n'
                 '```\r\n'
                 '\r\n'
                 '- `from keras import backend as K\r\n'
                 'K.tensorflow_backend._get_available_gpus()` prints an empty '
                 'field\r\n'
                 '- `tf.test.is_gpu_available()` says `False`\r\n'
                 '\r\n'
                 'I can see that tensorflow is looking for the following ones and '
                 'cannot find them:\r\n'
                 '- libcublas.so.10.0\r\n'
                 '- libcufft.so.10.0\r\n'
                 '- libcurand.so.10.0\r\n'
                 '- libcusolver.so.10.0\r\n'
                 '- libcusparse.so.10.0\r\n'
                 '- libcudnn.so.7\r\n'
                 '\r\n'
                 'It is strange, I never installed nvidia-smi within the container, I '
                 'seems to be deployed by docker, but necessary parts are missing for '
                 'tensorflow. However, if I install CUDA or/and nvidia-toolkits and '
                 'other software parts in the container, tensorflow is reporting '
                 'about incorpatible versions:\r\n'
                 '\r\n'
                 '```\r\n'
                 '2020-01-01 20:50:11.429852: I '
                 'tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU '
                 'supports instructions that this TensorFlow binary was not compiled '
                 'to use: AVX2 AVX512F FMA\r\n'
                 '2020-01-01 20:50:11.466847: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:42] '
                 'Successfully opened dynamic library libcuda.so.1\r\n'
                 '2020-01-01 20:50:11.468037: E '
                 'tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to '
                 'cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported '
                 'display driver / cuda driver combination\r\n'
                 '2020-01-01 20:50:11.468085: I '
                 'tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving '
                 'CUDA diagnostic information for host: 9f9f93453aee\r\n'
                 '2020-01-01 20:50:11.468093: I '
                 'tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: '
                 '9f9f93453aee\r\n'
                 '2020-01-01 20:50:11.468236: I '
                 'tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda '
                 'reported version is: 440.33.1\r\n'
                 '2020-01-01 20:50:11.468265: I '
                 'tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel '
                 'reported version is: 430.64.0\r\n'
                 '2020-01-01 20:50:11.468285: E '
                 'tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel '
                 'version 430.64.0 does not match DSO version 440.33.1 -- cannot find '
                 'working devices in this configuration\r\n'
                 '2020-01-01 20:50:11.489100: I '
                 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU '
                 'Frequency: 2100000000 Hz\r\n'
                 '2020-01-01 20:50:11.493082: I '
                 'tensorflow/compiler/xla/service/service.cc:168] XLA service '
                 '0x4c12f30 executing computations on platform Host. Devices:\r\n'
                 '2020-01-01 20:50:11.493143: I '
                 'tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor '
                 'device (0): <undefined>, <undefined>\r\n'
                 '```\r\n'
                 'I do not think that it is an tensorflow issue. It is moreover the '
                 'question, how to create an own docker container with tensorflow-gpu '
                 'with CUDA support. Is the docker base image the right one?',
         'created_at': '2020-01-0'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google '
                 'Colab / Android Studio\r\n'
                 '- TensorFlow installed from (source or binary): Source\r\n'
                 '- TensorFlow version (or github SHA if from source):  1.14\r\n'
                 '\r\n'
                 '\r\n'
                 '**Provide the text output from tflite_convert**\r\n'
                 'TFLite Model output probabilities on same input text:\r\n'
                 '```\r\n'
                 '[0.07323484, 0.021794094, 0.013985702, 0.013658127, 0.010268052]\r\n'
                 '```\r\n'
                 '\r\n'
                 'H5 Model file output on same input text: \r\n'
                 '```\r\n'
                 '[0.03437233, 0.02771266, 0.01885756, 0.01123235, 0.01093488]\r\n'
                 '```\r\n'
                 'However, in both cases the first probabilities belong to the same '
                 'classes. The predicted classes in both cases are not meaningless '
                 'but also they are not same, which should be the case.\r\n',
         'created_at': '2020-01-0'},
        {'body': '### 1. Network define\r\n'
                 '```\r\n'
                 'import pathlib\r\n'
                 'from concurrent.futures import ProcessPoolExecutor\r\n'
                 '\r\n'
                 'import gensim\r\n'
                 'import numpy as np\r\n'
                 'import tensorflow as tf\r\n'
                 'from absl import app\r\n'
                 'from absl import flags\r\n'
                 'from absl import logging\r\n'
                 'from tensorflow import keras as tfk\r\n'
                 'from tqdm import tqdm\r\n'
                 '\r\n'
                 'from src.preprocessing.sequence import create_dataset\r\n'
                 'from src.preprocessing.text import JiebaTokenizer\r\n'
                 '\r\n'
                 '\r\n'
                 'class TextCNN(tfk.Model):\r\n'
                 '\r\n'
                 '    LABEL_IDX_DICT = {"constellation": 0, "education": 1, '
                 '"entertainment": 2, "fashion": 3, "finance": 4, "game": 5,\r\n'
                 '                      "house": 6, "land": 7, "lottery": 8, '
                 '"political": 9, "social": 10, "sports": 11, "stock": 12,\r\n'
                 '                      "technology": 13}\r\n'
                 '    IDX_LABEL_DICT = {idx: label for label, idx in '
                 'LABEL_IDX_DICT.items()}\r\n'
                 '\r\n'
                 '    def __init__(self, embeddings):\r\n'
                 '        super(TextCNN, self).__init__()\r\n'
                 '        self.embedding_layer = '
                 'tfk.layers.Embedding(embeddings.shape[0], embeddings.shape[1], '
                 'trainable=False,\r\n'
                 '                                                    '
                 'embeddings_initializer=tf.initializers.Constant(embeddings))\r\n'
                 '        self.conv_layers = [tfk.layers.Conv1D(FLAGS.filters, '
                 'FLAGS.kernel_size + idx * FLAGS.kernel_distance,\r\n'
                 '                                              '
                 'strides=FLAGS.conv_strides, padding=FLAGS.conv_padding)\r\n'
                 '                            for idx in '
                 'range(FLAGS.conv_pool_num)]\r\n'
                 '        self.pool_layers = '
                 '[tfk.layers.MaxPool1D(pool_size=FLAGS.pool_size, '
                 'strides=FLAGS.pool_strides,\r\n'
                 '                                                 '
                 'padding=FLAGS.pool_padding)\r\n'
                 '                            for idx in '
                 'range(FLAGS.conv_pool_num)]\r\n'
                 '        self.dense1_layer = tfk.layers.Dense(FLAGS.dense_units, '
                 'activation=FLAGS.activation)\r\n'
                 '        self.batchnorm_layer = tfk.layers.BatchNormalization()\r\n'
                 '        self.dense2_layer = '
                 'tfk.layers.Dense(len(self.LABEL_IDX_DICT), '
                 'activation=tfk.activations.softmax)\r\n'
                 '        self.dropout_layer = '
                 'tfk.layers.Dropout(FLAGS.dropout_rate)\r\n'
                 '\r\n'
                 '    def call(self, inputs, training=None):\r\n'
                 '        x = inputs\r\n'
                 '        x_embed = self.embedding_layer(x)\r\n'
                 '        assert FLAGS.conv_pool_num > 0, ValueError("conv_pool_num '
                 'must > 0")\r\n'
                 '        encodings = [self.conv_layers[idx](x_embed) for idx in '
                 'range(FLAGS.conv_pool_num)]\r\n'
                 '        encodings_pool = [self.pool_layers[idx](encodings[idx]) for '
                 'idx in range(FLAGS.conv_pool_num)]\r\n'
                 '        encodings_concat = tf.concat(encodings_pool, axis=-1)\r\n'
                 '        encodings_concat = self.batchnorm_layer(encodings_concat, '
                 'training=training)\r\n'
                 '        encodings_flatten = '
                 'tfk.layers.Flatten()(encodings_concat)\r\n'
                 '        encodings_flatten = self.dropout_layer(encodings_flatten, '
                 'training=training)\r\n'
                 '        hidden = self.dense1_layer(encodings_flatten)\r\n'
                 '        y_pred = self.dense2_layer(hidden)\r\n'
                 '        return y_pred\r\n'
                 '```\r\n'
                 '### 2. Train with model.fit\r\n'
                 '```\r\n'
                 'w2v = '
                 'gensim.models.KeyedVectors.load_word2vec_format(FLAGS.embeddings_path)\r\n'
                 'words = list(w2v.vocab.keys())[:FLAGS.vocab_size]\r\n'
                 'tokenizer = JiebaTokenizer(words)\r\n'
                 'gpus = tf.config.experimental.list_physical_devices("GPU")\r\n'
                 'for gpu in gpus:\r\n'
                 '    tf.config.experimental.set_memory_growth(gpu, True)\r\n'
                 'embeddings = np.zeros(shape=(FLAGS.vocab_size + 2, '
                 'w2v.vector_size), dtype=np.float32)\r\n'
                 'for word, idx in tokenizer.word2idx_dict.items():\r\n'
                 '    if word in w2v.vocab:\r\n'
                 '        embeddings[idx] = w2v.word_vec(word)\r\n'
                 'x, y = TextCNN.preprocess_dataset(tokenizer, '
                 'FLAGS.trainset_path)\r\n'
                 'trainset = create_dataset([x, y])\r\n'
                 'trainset = '
                 'trainset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)\r\n'
                 'val_x, val_y = TextCNN.preprocess_dataset(tokenizer, '
                 'FLAGS.valset_path)\r\n'
                 'valset = create_dataset([val_x, val_y])\r\n'
                 'valset = '
                 'valset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)\r\n'
                 'model = TextCNN(embeddings)\r\n'
                 'optimizer = tf.optimizers.Adam(FLAGS.lr)\r\n'
                 'model.compile(optimizer=optimizer, '
                 'loss=tf.losses.sparse_categorical_crossentropy, metrics=["acc"])\r\n'
                 'model.fit(x=x, y=y, batch_size=FLAGS.batch_size, '
                 'epochs=FLAGS.epochs, validation_data=(val_x, val_y))\r\n'
                 '```\r\n'
                 '### 3. Train with GradientTape\r\n'
                 '```\r\n'
                 'w2v = '
                 'gensim.models.KeyedVectors.load_word2vec_format(FLAGS.embeddings_path)\r\n'
                 'words = list(w2v.vocab.keys())[:FLAGS.vocab_size]\r\n'
                 'tokenizer = JiebaTokenizer(words)\r\n'
                 'gpus = tf.config.experimental.list_physical_devices("GPU")\r\n'
                 'for gpu in gpus:\r\n'
                 '    tf.config.experimental.set_memory_growth(gpu, True)\r\n'
                 'embeddings = np.zeros(shape=(FLAGS.vocab_size + 2, '
                 'w2v.vector_size), dtype=np.float32)\r\n'
                 'for word, idx in tokenizer.word2idx_dict.items():\r\n'
                 '    if word in w2v.vocab:\r\n'
                 '        embeddings[idx] = w2v.word_vec(word)\r\n'
                 'x, y = TextCNN.preprocess_dataset(tokenizer, '
                 'FLAGS.trainset_path)\r\n'
                 'trainset = create_dataset([x, y])\r\n'
                 'trainset = '
                 'trainset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)\r\n'
                 'val_x, val_y = TextCNN.preprocess_dataset(tokenizer, '
                 'FLAGS.valset_path)\r\n'
                 'valset = create_dataset([val_x, val_y])\r\n'
                 'valset = '
                 'valset.batch(FLAGS.batch_size).shuffle(FLAGS.buffer_size)\r\n'
                 'model = TextCNN(embeddings)\r\n'
                 'optimizer = tf.optimizers.Adam(FLAGS.lr)\r\n'
                 'loss_object = tf.losses.SparseCategoricalCrossentropy()\r\n'
                 'train_acc = tf.metrics.SparseCategoricalAccuracy()\r\n'
                 'val_acc = tf.metrics.SparseCategoricalAccuracy()\r\n'
                 'train_loss = tf.metrics.Mean()\r\n'
                 'val_loss = tf.metrics.Mean()\r\n'
                 '\r\n'
                 '@tf.function\r\n'
                 'def train_op(x, y):\r\n'
                 '    with tf.GradientTape() as tape:\r\n'
                 '        y_pred = model(x, training=True)\r\n'
                 '        loss = loss_object(y, y_pred)\r\n'
                 '        train_acc.update_state(y, y_pred)\r\n'
                 '        train_loss.update_state(loss)\r\n'
                 '        grads = tape.gradient(loss, model.trainable_variables)\r\n'
                 '        optimizer.apply_gradients(grads_and_vars=zip(grads, '
                 'model.trainable_variables))\r\n'
                 '\r\n'
                 '@tf.function\r\n'
                 'def val_op(x, y):\r\n'
                 '    y_pred = model(x, training=False)\r\n'
                 '    loss = loss_object(y, y_pred)\r\n'
                 '    val_acc.update_state(y, y_pred)\r\n'
                 '    val_loss.update_state(loss)\r\n'
                 '\r\n'
                 'for epoch in range(FLAGS.epochs):\r\n'
                 '    tf.print("Epoch {}/{}".format(epoch + 1, FLAGS.epochs))\r\n'
                 '    bar = tfk.utils.Progbar(target=len(x), unit_name="sample")\r\n'
                 '    train_acc.reset_states()\r\n'
                 '    val_acc.reset_states()\r\n'
                 '    train_loss.reset_states()\r\n'
                 '    val_loss.reset_states()\r\n'
                 '    for batch_x, batch_y in trainset:\r\n'
                 '        train_op(batch_x, batch_y)\r\n'
                 '        bar.add(len(batch_y))\r\n'
                 '    for batch_x, batch_y in valset:\r\n'
                 '        val_op(batch_x, batch_y)\r\n'
                 '    template = "loss: {:.4f}\\nacc: {:.4f}\\nval_loss: '
                 '{:.4f}\\nval_acc: {:.4f}"\r\n'
                 '    message = template.format(train_loss.result().numpy(), '
                 'train_acc.result().numpy(),\r\n'
                 '                              val_loss.result().numpy(), '
                 'val_acc.result().numpy())\r\n'
                 '    tf.print(message)\r\n'
                 '```\r\n'
                 '### 4. model.fit result\r\n'
                 '![image](https://user-images.githubusercontent.com/22722147/71655214-d452ba80-2d70-11ea-9ef8-294d90944ea1.png)\r\n'
                 '### 5. GradientTape result\r\n'
                 '![image](https://user-images.githubusercontent.com/22722147/71654550-3e696080-2d6d-11ea-896e-e72ade58974a.png)\r\n'
                 '![image](https://user-images.githubusercontent.com/22722147/71654563-5214c700-2d6d-11ea-880b-10744b899d6f.png)\r\n'
                 '![image](https://user-images.githubusercontent.com/22722147/71654570-5b059880-2d6d-11ea-896c-a132b383ee32.png)\r\n'
                 '### 6. Question\r\n'
                 'Like the result above, I use same parameter, but get very different '
                 'result. Who can help me to figure out it. I have read some relevant '
                 "tensorflow2.0 source code, but can't solve it.\r\n",
         'created_at': '2020-01-0'},
        {'body': 'i can search "Tensorflow", but can not install example : '
                 'https://github.com/tensorflow/examples/tree/master/lite/examples/gesture_classification/ios\r\n'
                 '\r\n'
                 'Error info:\r\n'
                 '![image](https://user-images.githubusercontent.com/3898789/71649968-f6871100-2d4d-11ea-9225-f2b90fb8991b.png)\r\n'
                 '![image](https://user-images.githubusercontent.com/3898789/71649972-fab32e80-2d4d-11ea-9878-d6fbca7548b5.png)\r\n',
         'created_at': '2020-01-0'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 'yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 'Linux Ubuntu 18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 'NA\r\n'
                 '- TensorFlow installed from (source or binary): binary wheel via '
                 'PyPI\r\n'
                 '- TensorFlow version (use command below):\r\n'
                 '2.1.0-dev20191231 (v1.12.1-21412-g3a094e6 2.1.0-dev20191231)\r\n'
                 '- Python version:\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 'NA\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 'NA\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 'CUDA 10.0 \r\n'
                 '- GPU model and memory:\r\n'
                 'V100 32 GB\r\n'
                 'You can collect some of this information using our environment '
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n'
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 "I'm suspecting a CPU memory leak when loading multiple models.\r\n"
                 'When im running infinite loop that keeps loading the same model '
                 'while using the same variable the memory (private bytes and working '
                 'set) of the process keep increasing. At some points the working set '
                 'seems to free some memory, but the trend is that the memory keeps '
                 'on rising.\r\n'
                 'I used a simple model (attached).\r\n'
                 '\r\n'
                 'This trend happens even though I call gc.collect() on every '
                 'iteration and tf.keras.backend.clear_session().\r\n'
                 '\r\n'
                 'the issue also happens in TF 2.0 (v2.0.0-rc2-26-g64c3d38 2.0.0).\r\n'
                 'for a specific model:\r\n'
                 '    running in TF 2.0 each iteration adds 16 MiB\r\n'
                 '    running in TF 2.1 each iteration adds 2 MiB\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'The memory shouldnt increase on each interation\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '`\r\n'
                 '```\r\n'
                 'import os\r\n'
                 'import tensorflow as tf\r\n'
                 'import gc # garbage collector\r\n'
                 'import objgraph\r\n'
                 'from memory_profiler import profile\r\n'
                 '\r\n'
                 'def mem_stat():\r\n'
                 '  objs = gc.get_objects()\r\n'
                 '  print("total objects count", len(objs))\r\n'
                 '\r\n'
                 '@profile\r\n'
                 'def profile_own_model():\r\n'
                 '    model = tf.keras.models.Sequential([\r\n'
                 '        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n'
                 "        tf.keras.layers.Dense(128, activation='relu'),\r\n"
                 '        tf.keras.layers.Dropout(0.2),\r\n'
                 "        tf.keras.layers.Dense(10, activation='softmax')\r\n"
                 '    ])\r\n'
                 "    # model.save('my_model')\r\n"
                 '    tf.keras.backend.clear_session()\r\n'
                 '    del model\r\n'
                 '    gc.collect()\r\n'
                 '\r\n'
                 '@profile\r\n'
                 'def profile_load_model(path):\r\n'
                 '    model = tf.keras.models.load_model(model_path, '
                 'compile=False)\r\n'
                 '    tf.keras.backend.clear_session()\r\n'
                 '    del model\r\n'
                 '    gc.collect()\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 "model_path = f'/my_model.hd5'\r\n"
                 'print("load model in loops:")\r\n'
                 '\r\n'
                 'c = 1\r\n'
                 'while True:\r\n'
                 '    print("----------- iter", c)\r\n'
                 '    profile_load_model(model_path)\r\n'
                 '\r\n'
                 '    print("mem stat after model creation:")\r\n'
                 '    mem_stat()\r\n'
                 '    objgraph.show_growth(limit=30)\r\n'
                 '    c += 1\r\n'
                 '```\r\n'
                 '`\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '![memory tf 2 '
                 '1](https://user-images.githubusercontent.com/27951762/71644038-d9f5c500-2cca-11ea-96e3-b8aedd2e4efb.png)\r\n'
                 '\r\n'
                 '\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 '\r\n',
         'created_at': '2020-01-0'},
        {'body': 'This commit mostly either enables or disables unittests on the ROCM '
                 'platform, details listed below\r\n'
                 '* adding/removing no_rocm tag for tests in the '
                 '//tensorflow/compiler/mlir dir\r\n'
                 '* enabling / disabling subtests within '
                 '//tensorflow/core/grappler/optimizers:constant_folding_test for the '
                 'ROCm platform\r\n'
                 '* disabling a subtest within '
                 '//tensorflow/core/distributed_runtime:collective_param_resolver_distributed_test '
                 'fir the ROCm platform\r\n'
                 '* adding no_rocm tag to tests that are failing on the ROCm '
                 'platform\r\n'
                 '* minor bug fix to ensure that the '
                 '//tensorflow/compiler/mlir/tensorflow:error_util_test passes on a '
                 'consistent basis\r\n'
                 '\r\n'
                 '-----------------------\r\n'
                 '\r\n'
                 '/cc @whchung @chsigg ',
         'created_at': '2019-12-3'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 16.04.6 LTS (Xenial Xerus)\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: N/A\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: 1.15\r\n'
                 '- Python version: 3.5\r\n'
                 '- Installed using virtualenv? pip? conda?: from source, within a '
                 'virtualenv. Bazel creates a pip wheels which is then installed.\r\n'
                 '- Bazel version (if compiling from source): 0.26.1\r\n'
                 '- GCC/Compiler version (if compiling from source): gcc version '
                 '5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.12) \r\n'
                 '- CUDA/cuDNN version: not used, CPU only\r\n'
                 '- GPU model and memory: not used, CPU only\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'I am trying to build TF from source with support for `omp simd`. '
                 'Compiling, creating and installing the pip wheel works fine using '
                 'bazel. However, as soon as I try to import tensorflow, I get the '
                 'error `ImportError: '
                 '/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: '
                 'undefined symbol: omp_get_thread_num` \r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 'As described in https://www.tensorflow.org/install/source, I first '
                 'run `bazel clean`. Then I run `./configure` and always select the '
                 'default values:\r\n'
                 '\r\n'
                 '```\r\n'
                 '(t2t) brix@cluster:/work/tensorflow$ ./configure \r\n'
                 'WARNING: Running Bazel server needs to be killed, because the '
                 'startup options are different.\r\n'
                 'WARNING: Waiting for server process to terminate (waited 5 seconds, '
                 'waiting at most 60)\r\n'
                 'WARNING: --batch mode is deprecated. Please instead explicitly shut '
                 'down your Bazel server using the command "bazel shutdown".\r\n'
                 'You have bazel 0.26.1 installed.\r\n'
                 'Please specify the location of python. [Default is '
                 '/work/venv/t2t/bin/python]: \r\n'
                 '\r\n'
                 'Found possible Python library paths:\r\n'
                 '  /work/venv/t2t/lib/python3.5/site-packages\r\n'
                 'Please input the desired Python library path to use.  Default is '
                 '[/work/venv/t2t/lib/python3.5/site-packages]\r\n'
                 'Do you wish to build TensorFlow with XLA JIT support? [Y/n]: XLA '
                 'JIT support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No '
                 'OpenCL SYCL support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm '
                 'support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA '
                 'support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to download a fresh release of clang? (Experimental) '
                 '[y/N]: Clang will not be downloaded.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with MPI support? [y/N]: No MPI '
                 'support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Please specify optimization flags to use during compilation when '
                 'bazel option "--config=opt" is specified [Default is -march=native '
                 '-Wno-sign-compare]: \r\n'
                 '\r\n'
                 'Would you like to interactively configure ./WORKSPACE for Android '
                 'builds? [y/N]: \r\n'
                 'Not configuring the WORKSPACE for Android builds.\r\n'
                 '\r\n'
                 'Preconfigured Bazel build configs. You can use any of the below by '
                 'adding "--config=<>" to your build command. See .bazelrc for more '
                 'details.\r\n'
                 '        --config=mkl            # Build with MKL support.\r\n'
                 '        --config=monolithic     # Config for mostly static '
                 'monolithic build.\r\n'
                 '        --config=gdr            # Build with GDR support.\r\n'
                 '        --config=verbs          # Build with libverbs support.\r\n'
                 '        --config=ngraph         # Build with Intel nGraph '
                 'support.\r\n'
                 '        --config=numa           # Build with NUMA support.\r\n'
                 '        --config=dynamic_kernels        # (Experimental) Build '
                 'kernels into separate shared objects.\r\n'
                 '        --config=v2             # Build TensorFlow 2.x instead of '
                 '1.x.\r\n'
                 'Preconfigured Bazel build configs to DISABLE default on '
                 'features:\r\n'
                 '        --config=noaws          # Disable AWS S3 filesystem '
                 'support.\r\n'
                 '        --config=nogcp          # Disable GCP support.\r\n'
                 '        --config=nohdfs         # Disable HDFS support.\r\n'
                 '        --config=noignite       # Disable Apache Ignite support.\r\n'
                 '        --config=nokafka        # Disable Apache Kafka support.\r\n'
                 '        --config=nonccl         # Disable NVIDIA NCCL support.\r\n'
                 'Configuration finished\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n'
                 'Afterwards, I run `bazel build -s --config=opt --copt="-fopenmp" '
                 '--copt="-lgomp" //tensorflo\r\n'
                 "w/tools/pip_package:build_pip_package`. Note that I've added "
                 '`--copt="-fopenmp" --copt="-lgomp"` to ensure that `omp` is linked. '
                 'This was suggested by the comment in this SO question: '
                 'https://stackoverflow.com/questions/45667374/import-tensorlfow-failed-with-errors-undefined-symbol-omp-get-num-threads\r\n'
                 '\r\n'
                 'Finally, I create the pip package using '
                 '`./bazel-bin/tensorflow/tools/pip_package/build_pip_package '
                 '/tmp/tensorflow_pkg` and install it.\r\n'
                 '\r\n'
                 'Unfortunately, when I then run `python -c "import tensorflow"`, I '
                 'get the following stack trace:\r\n'
                 '\r\n'
                 '```\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py", '
                 'line 58, in <module>\r\n'
                 '    from tensorflow.python.pywrap_tensorflow_internal import *\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py", '
                 'line 28, in <module>\r\n'
                 '    _pywrap_tensorflow_internal = swig_import_helper()\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py", '
                 'line 24, in swig_import_helper\r\n'
                 "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, "
                 'pathname, description)\r\n'
                 '  File "/usr/lib/python3.5/imp.py", line 242, in load_module\r\n'
                 '    return load_dynamic(name, filename, file)\r\n'
                 '  File "/usr/lib/python3.5/imp.py", line 342, in load_dynamic\r\n'
                 '    return _load(spec)\r\n'
                 'ImportError: '
                 '/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: '
                 'undefined symbol: omp_get_thread_num\r\n'
                 '\r\n'
                 'During handling of the above exception, another exception '
                 'occurred:\r\n'
                 '\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "<stdin>", line 1, in <module>\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py", '
                 'line 99, in <module>\r\n'
                 '    from tensorflow_core import *\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/__init__.py", '
                 'line 28, in <module>\r\n'
                 '    from tensorflow.python import pywrap_tensorflow  # pylint: '
                 'disable=unused-import\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py", '
                 'line 50, in __getattr__\r\n'
                 '    module = self._load()\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow/__init__.py", '
                 'line 44, in _load\r\n'
                 '    module = _importlib.import_module(self.__name__)\r\n'
                 '  File "/usr/lib/python3.5/importlib/__init__.py", line 126, in '
                 'import_module\r\n'
                 '    return _bootstrap._gcd_import(name[level:], package, level)\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/__init__.py", '
                 'line 49, in <module>\r\n'
                 '    from tensorflow.python import pywrap_tensorflow\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py", '
                 'line 74, in <module>\r\n'
                 '    raise ImportError(msg)\r\n'
                 'ImportError: Traceback (most recent call last):\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow.py", '
                 'line 58, in <module>\r\n'
                 '    from tensorflow.python.pywrap_tensorflow_internal import *\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py", '
                 'line 28, in <module>\r\n'
                 '    _pywrap_tensorflow_internal = swig_import_helper()\r\n'
                 '  File '
                 '"/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py", '
                 'line 24, in swig_import_helper\r\n'
                 "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, "
                 'pathname, description)\r\n'
                 '  File "/usr/lib/python3.5/imp.py", line 242, in load_module\r\n'
                 '    return load_dynamic(name, filename, file)\r\n'
                 '  File "/usr/lib/python3.5/imp.py", line 342, in load_dynamic\r\n'
                 '    return _load(spec)\r\n'
                 'ImportError: '
                 '/work/venv/t2t/lib/python3.5/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so: '
                 'undefined symbol: omp_get_thread_num\r\n'
                 '\r\n'
                 '\r\n'
                 'Failed to load the native TensorFlow runtime.\r\n'
                 '\r\n'
                 'See https://www.tensorflow.org/install/errors\r\n'
                 '\r\n'
                 'for some common reasons and solutions.  Include the entire stack '
                 'trace\r\n'
                 'above this error message when asking for help.\r\n'
                 '```\r\n',
         'created_at': '2019-12-3'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): No\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iPAD OS '
                 '13.2.2\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: iPAD Pro 2018\r\n'
                 '- TensorFlow installed from (source or binary): Binary '
                 '(TfLiteGPUExperimental)\r\n'
                 '- TensorFlow version (use command below):\r\n'
                 '- Python version: NA\r\n'
                 '- Bazel version (if compiling from source): NA\r\n'
                 '- GCC/Compiler version (if compiling from source): NA\r\n'
                 '- CUDA/cuDNN version: NA\r\n'
                 '- GPU model and memory: iPAD GPU\r\n'
                 '\r\n'
                 'You can collect some of this information using our environment '
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n'
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'When running the model on iPAD using CPU, we are able to get the '
                 'output. But when doing GPU delegate, we get the error the following '
                 'error:\r\n'
                 '_failed assertion `Cannot create a buffer of zero length._\r\n'
                 '**Describe the expected behavior**\r\n'
                 'No error\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 'delegate = NewGpuDelegate(nullptr);\r\n'
                 '      interpreter->ModifyGraphWithDelegate(delegate);\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2019-12-3'},
        {'body': 'Got the following error output from Databricks; exported the '
                 'notebook to the attached file; error occurs in cell 10.\r\n'
                 '\r\n'
                 'INFO:tensorflow:Converted call: <function read.<locals>.<lambda> at '
                 '0x7fbde97eae18>\r\n'
                 "    args: (<tf.Tensor 'args_0:0' shape=() dtype=string>,)\r\n"
                 '    kwargs: {}\r\n'
                 '\r\n'
                 "INFO:tensorflow:Not whitelisted: <method-wrapper '__call__' of "
                 'function object at 0x7fbde97eae18>: default rule\r\n'
                 'INFO:tensorflow:Not whitelisted: <function read.<locals>.<lambda> '
                 'at 0x7fbde97eae18>: default rule\r\n'
                 'INFO:tensorflow:Entity <function read.<locals>.<lambda> at '
                 '0x7fbde97eae18> is not cached for key <code object <lambda> at '
                 '0x7fbde9888f60, file "<command-608347>", line 138> subkey '
                 '(<tensorflow.python.autograph.core.converter.ConversionOptions '
                 "object at 0x7fbdddf7c0b8>, frozenset({'mean', 'var'}))\r\n"
                 'INFO:tensorflow:Converting <function read.<locals>.<lambda> at '
                 '0x7fbde97eae18>\r\n'
                 'INFO:tensorflow:Error transforming entity <function '
                 'read.<locals>.<lambda> at 0x7fbde97eae18>\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py", '
                 'line 78, in parse_entity\r\n'
                 '    return parse_str(source, preamble_len=len(future_features)), '
                 'source\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py", '
                 'line 139, in parse_str\r\n'
                 '    module_node = gast.parse(src)\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/gast/gast.py", line '
                 '240, in parse\r\n'
                 '    return ast_to_gast(_ast.parse(*args, **kwargs))\r\n'
                 '  File "/usr/lib/python3.7/ast.py", line 35, in parse\r\n'
                 '    return compile(source, filename, mode, PyCF_ONLY_AST)\r\n'
                 '  File "<unknown>", line 4\r\n'
                 '    .map(lambda x: decode(x, mean, var))\r\n'
                 '    ^\r\n'
                 'SyntaxError: invalid syntax\r\n'
                 '\r\n'
                 'During handling of the above exception, another exception '
                 'occurred:\r\n'
                 '\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py", '
                 'line 506, in converted_call\r\n'
                 '    converted_f = conversion.convert(target_entity, program_ctx)\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py", '
                 'line 322, in convert\r\n'
                 '    free_nonglobal_var_names)\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py", '
                 'line 240, in _convert_with_cache\r\n'
                 '    entity, program_ctx)\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py", '
                 'line 469, in convert_entity_to_ast\r\n'
                 '    nodes, name, entity_info = convert_func_to_ast(o, '
                 'program_ctx)\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/conversion.py", '
                 'line 630, in convert_func_to_ast\r\n'
                 '    node, source = parser.parse_entity(f, '
                 'future_features=future_features)\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py", '
                 'line 118, in parse_entity\r\n'
                 '    return parse_str(source, preamble_len=len(future_features)), '
                 'source\r\n'
                 '  File '
                 '"/databricks/python/lib/python3.7/site-packages/tensorflow_core/python/autograph/pyct/parser.py", '
                 'line 145, in parse_str\r\n'
                 "    raise ValueError('expected exactly one node node, found "
                 "{}'.format(nodes))\r\n"
                 'ValueError: expected exactly one node node, found []\r\n'
                 'WARNING:tensorflow:Entity <function read.<locals>.<lambda> at '
                 '0x7fbde97eae18> could not be transformed and will be executed '
                 'as-is. Please report this to the AutoGraph\r\n'
                 ' team. When filing the bug, set the verbosity to 10 (on Linux, '
                 '`export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: '
                 'expected exactly one node node, found []\r\n'
                 '[v2_compute_ndcg_geov4.02.ipynb.zip](https://github.com/tensorflow/tensorflow/files/4011226/v2_compute_ndcg_geov4.02.ipynb.zip)',
         'created_at': '2019-12-3'},
        {'body': 'hi, I find some bug. Code is\r\n'
                 '\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n"
                 '    def Gen():\r\n'
                 '        for i in range(10):\r\n'
                 '            yield(i,2,3,4,5,6,7,8)\r\n'
                 '\r\n'
                 '    dataset = tf.data.Dataset.from_generator(Gen, '
                 'output_types=(tf.float32,tf.float32,tf.int32,tf.int32,tf.int32,tf.float32,tf.int32,tf.int32),output_shapes=None,args=None)\r\n'
                 '    for one_batch in dataset:\r\n'
                 "        print('one batch',one_batch)\r\n"
                 '\r\n'
                 '    print("******end**********")\r\n'
                 '\r\n'
                 '    num_gpu=1\r\n'
                 "    devices = ['/device:GPU:{}'.format(i) for i in "
                 'range(num_gpu)]\r\n'
                 '    strategy = tf.distribute.MirroredStrategy(devices)\r\n'
                 '\r\n'
                 '    input_context = '
                 'tf.distribute.InputContext(num_input_pipelines=1,\r\n'
                 '            input_pipeline_id=0,\r\n'
                 '            num_replicas_in_sync=1)\r\n'
                 '\r\n'
                 '    with strategy.scope():\r\n'
                 '        def dataset_fn(input_context):\r\n'
                 '            dataset = tf.data.Dataset.from_generator(Gen, '
                 'output_types=(tf.float32,tf.float32,tf.int32,tf.int32,tf.int32,tf.float32,tf.int32,tf.int32),output_shapes=None,args=None)\r\n'
                 '            return dataset.shard(\r\n'
                 '                    input_context.num_input_pipelines, '
                 'input_context.input_pipeline_id)\r\n'
                 '\r\n'
                 '        train_dist_dataset = '
                 'strategy.experimental_distribute_datasets_from_function(dataset_fn)\r\n'
                 '\r\n'
                 '        for one_batch  in train_dist_dataset:\r\n'
                 "            print('****one batch*******',one_batch)\r\n"
                 '\r\n'
                 'The code can be run, but in distribut "for one_batch  in '
                 'train_dist_dataset:" at the end batch will be error.\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "/usr/local/python35/lib/python3.5/pdb.py", line 1665, in '
                 'main\r\n'
                 '    pdb._runscript(mainpyfile)\r\n'
                 '  File "/usr/local/python35/lib/python3.5/pdb.py", line 1546, in '
                 '_runscript\r\n'
                 '    self.run(statement)\r\n'
                 '  File "/usr/local/python35/lib/python3.5/bdb.py", line 431, in '
                 'run\r\n'
                 '    exec(cmd, globals, locals)\r\n'
                 '  File "<string>", line 1, in <module>\r\n'
                 '  File '
                 '"/search/speech/hubo/git/tf-code-acoustics/tf2.0-model/io_test.py", '
                 'line 45, in <module>\r\n'
                 '    for one_batch  in train_dist_dataset:\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py", '
                 'line 275, in __next__\r\n'
                 '    return self.get_next()\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py", '
                 'line 304, in get_next\r\n'
                 '    global_has_value, replicas = _get_next_as_optional(self, '
                 'self._strategy)\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py", '
                 'line 200, in _get_next_as_optional\r\n'
                 '    iterator._iterators[i].get_next_as_list(new_name))  # pylint: '
                 'disable=protected-access\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py", '
                 'line 878, in get_next_as_list\r\n'
                 '    lambda: _dummy_tensor_fn(data.value_structure))\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/util/deprecation.py", '
                 'line 507, in new_func\r\n'
                 '    return func(*args, **kwargs)\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/ops/control_flow_ops.py", '
                 'line 1204, in cond\r\n'
                 '    result = false_fn()\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py", '
                 'line 878, in <lambda>\r\n'
                 '    lambda: _dummy_tensor_fn(data.value_structure))\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py", '
                 'line 801, in _dummy_tensor_fn\r\n'
                 '    result.append(create_dummy_tensor(feature_shape, '
                 'feature_type))\r\n'
                 '  File '
                 '"/usr/local/python35/lib/python3.5/site-packages/tensorflow_core/python/distribute/input_lib.py", '
                 'line 784, in create_dummy_tensor\r\n'
                 '    for dim in feature_shape.dims:\r\n'
                 "TypeError: 'NoneType' object is not iterable\r\n"
                 'Uncaught exception. Entering post mortem debugging\r\n'
                 "Running 'cont' or 'step' will restart the program\r\n"
                 '\r\n'
                 'I want to know why.',
         'created_at': '2019-12-3'},
        {'body': '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 16.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (use command below): 2.0\r\n'
                 '- Python version: 3.6\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10.0 / 7.6\r\n'
                 '- GPU model and memory: GTX 1080 / 8GB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'My model is a segmentation model with DenseNet like structure. '
                 'There are many concatenate operations between the encoder and '
                 'decoder tensors. I want to recompute the gradients on these '
                 'concatenate layers during backpropagation to limit the amount of '
                 'GPU memory usage. A similar approach can be found here : '
                 'https://github.com/joeyearsley/efficient_densenet_tensorflow.  I '
                 'tried to use tf.recompute_grad() on a wrapper function which has a '
                 'concatenate layer inside but it would raise an error when the '
                 'channel dimensions of input tensors are not matched.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'The concatenate layer should not raise an error when concatenating '
                 'inputs with different number of channels.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n'
                 'https://colab.research.google.com/drive/1d7zSGbYmocnupUpDIJLnlXt-3vH5bi83#scrollTo=xcPT-MWZAuvK&uniqifier=1\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'InvalidArgumentError                      Traceback (most recent '
                 'call last)\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py '
                 'in _create_c_op(graph, node_def, inputs, control_inputs)\r\n'
                 '   1609   try:\r\n'
                 '-> 1610     c_op = c_api.TF_FinishOperation(op_desc)\r\n'
                 '   1611   except errors.InvalidArgumentError as e:\r\n'
                 '\r\n'
                 'InvalidArgumentError: Dimension 3 in both shapes must be equal, but '
                 'are 16 and 32. Shapes are [?,30,30,16] and [?,30,30,32].\r\n'
                 "\tFrom merging shape 0 with other shapes. for 'packed_7' (op: "
                 "'Pack') with input shapes: [?,30,30,16], [?,30,30,32].\r\n"
                 '\r\n'
                 'During handling of the above exception, another exception '
                 'occurred:\r\n'
                 '\r\n'
                 'ValueError                                Traceback (most recent '
                 'call last)\r\n'
                 '16 frames\r\n'
                 '<ipython-input-24-689917ca63e3> in <module>()\r\n'
                 '----> 1 model = test_model()\r\n'
                 '      2 history = model.fit(train_images, train_labels, '
                 'epochs=10, \r\n'
                 '      3                     validation_data=(test_images, '
                 'test_labels))\r\n'
                 '\r\n'
                 '<ipython-input-23-caa5b725d514> in test_model()\r\n'
                 '      4     x_list.append(layers.Conv2D(16, (3,3), '
                 "activation='relu')(x_in))\r\n"
                 '      5     x_list.append(layers.Conv2D(32, (3,3), '
                 "activation='relu')(x_in))\r\n"
                 '----> 6     x = efficient_concat(x_list)\r\n'
                 '      7     x = layers.Flatten()(x)\r\n'
                 "      8     x = layers.Dense(10, activation='softmax')(x)\r\n"
                 '\r\n'
                 '<ipython-input-3-fa43d4abcee2> in efficient_concat(input_list)\r\n'
                 '      4         return x\r\n'
                 '      5     wraper = tf.recompute_grad(wraper)\r\n'
                 '----> 6     return wraper(input_list)\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py '
                 'in decorated(*args, **kwargs)\r\n'
                 '    164     """Decorated function with custom gradient."""\r\n'
                 '    165     if context.executing_eagerly():\r\n'
                 '--> 166       return _eager_mode_decorator(f, *args, **kwargs)\r\n'
                 '    167     else:\r\n'
                 '    168       return _graph_mode_decorator(f, *args, **kwargs)\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py '
                 'in _eager_mode_decorator(f, *args, **kwargs)\r\n'
                 '    333 \r\n'
                 '    334   input_tensors = [ops.convert_to_tensor(x) for x\r\n'
                 '--> 335                    in list(args) + list(variables)]\r\n'
                 '    336   arg_count = len(args)\r\n'
                 '    337   def actual_grad_fn(*result_grads):\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/custom_gradient.py '
                 'in <listcomp>(.0)\r\n'
                 '    332   flat_result = [gen_array_ops.identity(x) for x in '
                 'flat_result]\r\n'
                 '    333 \r\n'
                 '--> 334   input_tensors = [ops.convert_to_tensor(x) for x\r\n'
                 '    335                    in list(args) + list(variables)]\r\n'
                 '    336   arg_count = len(args)\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py '
                 'in convert_to_tensor(value, dtype, name, preferred_dtype, '
                 'dtype_hint)\r\n'
                 '   1182   preferred_dtype = '
                 'deprecation.deprecated_argument_lookup(\r\n'
                 '   1183       "dtype_hint", dtype_hint, "preferred_dtype", ' \
                 'preferred_dtype)\r\n'
                 '-> 1184   return convert_to_tensor_v2(value, dtype, '
                 'preferred_dtype, name)\r\n'
                 '   1185 \r\n'
                 '   1186 \r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py '
                 'in convert_to_tensor_v2(value, dtype, dtype_hint, name)\r\n'
                 '   1240       name=name,\r\n'
                 '   1241       preferred_dtype=dtype_hint,\r\n'
                 '-> 1242       as_ref=False)\r\n' \
                 '   1243 \r\n'
                 '   1244 \r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py ' \
                 'in internal_convert_to_tensor(value, dtype, name, as_ref, '
                 'preferred_dtype, ctx, accept_composite_tensors)\r\n'
                 '   1294 \r\n'
                 '   1295     if ret is None:\r\n'
                 '-> 1296       ret = conversion_func(value, dtype=dtype, name=name, '
                 'as_ref=as_ref)\r\n' \
                 '   1297 \r\n'
                 '   1298     if ret is NotImplemented:\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py '
                 'in _autopacking_conversion_function(v, dtype, name, as_ref)\r\n'
                 '   1276   elif dtype != inferred_dtype:\r\n' \
                 '   1277     v = ' \
                 'nest.map_structure(_cast_nested_seqs_to_dtype(dtype), v)\r\n' \
                 '-> 1278   return _autopacking_helper(v, dtype, name or "packed")\r\n'
                 '   1279 \r\n'
                 '   1280 \r\n' \
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py '
                 'in _autopacking_helper(list_or_tuple, dtype, name)\r\n'
                 '   1182     # checking.\r\n'
                 '   1183     if all(ops.is_dense_tensor_like(elem) for elem in '
                 'list_or_tuple):\r\n'
                 '-> 1184       return gen_array_ops.pack(list_or_tuple, '
                 'name=name)\r\n'
                 '   1185   must_pack = False\r\n'
                 '   1186   converted_elems = []\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py '
                 'in pack(values, axis, name)\r\n' \
                 '   6302   axis = _execute.make_int(axis, "axis")\r\n'
                 '   6303   _, _, _op = _op_def_lib._apply_op_helper(\r\n'
                 '-> 6304         "Pack", values=values, axis=axis, name=name)\r\n'
                 '   6305   _result = _op.outputs[:]\r\n' \
                 '   6306   _inputs_flat = _op.inputs\r\n'
                 '\r\n' \
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py '
                 'in _apply_op_helper(self, op_type_name, name, **keywords)\r\n'
                 '    791         op = g.create_op(op_type_name, inputs, dtypes=None, '
                 'name=scope,\r\n'
                 '    792                          input_types=input_types, '
                 'attrs=attr_protos,\r\n'
                 '--> 793                          op_def=op_def)\r\n' \
                 '    794       return output_structure, op_def.is_stateful, op\r\n'
                 '    795 \r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py '
                 'in create_op(***failed resolving arguments***)\r\n'
                 '    546     return super(FuncGraph, self)._create_op_internal(  # '
                 'pylint: disable=protected-access\r\n'
                 '    547         op_type, inputs, dtypes, input_types, name, attrs, '
                 'op_def,\r\n'
                 '--> 548         compute_device)\r\n' \
                 '    549 \r\n'
                 '    550   def capture(self, tensor, name=None):\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py '
                 'in _create_op_internal(self, op_type, inputs, dtypes, input_types, '
                 'name, attrs, op_def, compute_device)\r\n'
                 '   3427           input_types=input_types,\r\n' \
                 '   3428           original_op=self._default_original_op,\r\n'
                 '-> 3429           op_def=op_def)\r\n'
                 '   3430       self._create_op_helper(ret, '
                 'compute_device=compute_device)\r\n'
                 '   3431     return ret\r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py '
                 'in __init__(self, node_def, g, inputs, output_types, '
                 'control_inputs, input_types, original_op, op_def)\r\n'
                 '   1771           op_def, inputs, node_def.attr)\r\n'
                 '   1772       self._c_op = _create_c_op(self._graph, node_def, '
                 'grouped_inputs,\r\n'
                 '-> 1773                                 control_input_ops)\r\n'
                 '   1774     # pylint: enable=protected-access\r\n'
                 '   1775 \r\n'
                 '\r\n'
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py '
                 'in _create_c_op(graph, node_def, inputs, control_inputs)\r\n'
                 '   1611   except errors.InvalidArgumentError as e:\r\n'
                 '   1612     # Convert to ValueError for backwards compatibility.\r\n'
                 '-> 1613     raise ValueError(str(e))\r\n'
                 '   1614 \r\n'
                 '   1615   return c_op\r\n'
                 '\r\n'
                 'ValueError: Dimension 3 in both shapes must be equal, but are 16 ' \
                 'and 32. Shapes are [?,30,30,16] and [?,30,30,32].\r\n'
                 "\tFrom merging shape 0 with other shapes. for 'packed_7' (op: "
                 "'Pack') with input shapes: [?,30,30,16], [?,30,30,32].",
         'created_at': '2019-12-3'},
        {'body': 'Getting below exception while following '
                 '"https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android"\r\n'
                 '\r\n' \
                 '\r\n'
                 'org.gradle.api.InvalidUserDataException: Cannot expand TAR '
                 "'D:\\WorkSpaces\\ML_WorkSpace\\TF_Android\\app\\build\\intermediates\\mobilenet_v1_1.0_224.tgz'.\r\n"
                 '\tat '
                 'org.gradle.api.internal.file.archive.TarFileTree.cannotExpand(TarFileTree.java:133)\r\n'
                 '\t... 56 more\r\n'
                 'Caused by: org.gradle.api.resources.ResourceException: Could not '
                 'read ' \
                 'D:\\WorkSpaces\\TF_Android\\app\\build\\intermediates\\mobilenet_v1_1.0_224.tgz.\r\n'
                 '\tat '
                 'org.gradle.internal.resource.ResourceExceptions.readFailed(ResourceExceptions.java:36)\r\n'
                 '\tat '
                 'org.gradle.api.internal.file.archive.compression.GzipArchiver.read(GzipArchiver.java:64)\r\n' \
                 '\tat '
                 'org.gradle.api.internal.file.MaybeCompressedFileResource.read(MaybeCompressedFileResource.java:55)\r\n'
                 '\tat '
                 'org.gradle.api.internal.file.archive.TarFileTree.visit(TarFileTree.java:78)\r\n'
                 '\t... 107 more\r\n'
                 'Caused by: java.util.zip.ZipException: Not in GZIP format\r\n'
                 '\tat '
                 'java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:165)\r\n'
                 '\tat '
                 'org.gradle.api.internal.file.archive.compression.GzipArchiver.read(GzipArchiver.java:61)\r\n'
                 '\t... 109 more\r\n'
                 '\r\n'
                 "**Don't know, what could be the problem?**",
         'created_at': '2019-12-3'},
        {'body': 'the function `files_io.get_matching_files` says it takes a '
                 '"filepath", but actually it takes a glob\r\n'
                 '\r\n'
                 'this means that if you save your checkpoints to a folder like '
                 "`x=[abc]`, then you can't load the previous checkpoint using "
                 'something like:\r\n' \
                 '\r\n'
                 '```\r\n'
                 'def load_checkpoint(sess, checkpoint_path):\r\n'
                 '  saver = tf.train.Saver(tf.global_variables())\r\n'
                 '  ckpt = tf.train.get_checkpoint_state(checkpoint_path)\r\n' \
                 "  tf.logging.info('Loading model %s.', "
                 'ckpt.model_checkpoint_path)\r\n'
                 '  saver.restore(sess, ckpt.model_checkpoint_path)\r\n'
                 '```\r\n'
                 '\r\n'
                 'where `checkpoint_path="./logs/x=[abc]"`.', \
         'created_at': '2019-12-3'},
        {'body': '**System information**\r\n'
                 'The bugs are not related to my system informations. They are caused '
                 'by a bad coding style in the '
                 'https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cu.cc '
                 'and '
                 'https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/non_max_suppression_op.cc. '
                 'Just to be sure those bugs could be easily reproduced on google '
                 'colab. \r\n'
                 '\r\n' \
                 '**Describe the current behavior**\r\n'
                 'I ll mention two major bugs.\r\n'
                 'The first one is a garbage output when the boxes coordinates are '
                 'not logical. In fact given a box(y1,x1,y2,x2)  coordinates with y1 '
                 '= y2 or x1 = x2 (the coordinates of a line) or even worst the '
                 'coordinates of a point ( x1 = x2 = y1 = y2) the algorithme will '
                 'only output the line when reaching it. \r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Here is the code to reproduce the behaviour on google colab: \r\n'
                 '\r\n'
                 '```\r\n' \
                 'import numpy as np\r\n'
                 '%tensorflow_version 2.x\r\n'
                 'import tensorflow as tf\r\n'
                 'tf.__version__\r\n'
                 'boxes = np.array([[0.1,0.1,0.2,0.2], [0.3,0.3,0.3,0.4], '
                 '[0.5,0.5,0.6,0.6], [0.7,0.7,0.8,0.8]], dtype= np.float32)\r\n'
                 'scores = np.array([0.9,0.8,0.7,0.6], dtype = np.float32)\r\n'
                 '(tf.image.non_max_suppression(boxes, scores, 8))\r\n'
                 '```\r\n'
                 '\r\n'
                 'output:\r\n'
                 '```\r\n'
                 "'2.1.0-rc1'\r\n" \
                 '<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 1, 1, 1, 1, '
                 '1, 1], dtype=int32)>\r\n' \
                 '```\r\n'
                 '\r\n'
                 'Now I ll explain brievely how the algorithm is coded in tensorflow. '
                 'Given a list of candidate boxes containing in the beginning all the '
                 'user boxes and a list of chosen boxes empty, if the box is chosen '
                 'it will not immediately delete it from the candidate box. In fact, '
                 'this box will be again processed as a candidate box in the next '
                 'iteration. But because it is already in the chosen boxes it wont be '
                 'chosen again . The reason for that is that the IOU of a box with '
                 'himself is 1 which is always above the threshold. Unfortunately, '
                 'the IOU of a line or a point with any box is 0. This is applied '
                 'even when the IOU is calculated of the line with iteself. This will '
                 'result in adding the line to the chosen boxes again and again. This '
                 "behaviour is mentioned in this issue but wasn't clearly explained. "
                 'https://github.com/tensorflow/tensorflow/issues/29628\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'The expected behaviour must be decided by the tensorflow programer. '
                 'He can chose between putting it only once in the result : \r\n'
                 '-<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 1, 2, 3], '
                 'dtype=int32)> \r\n'
                 'or deleting the line box\r\n'
                 ' -<tf.Tensor: shape=(8,), dtype=int32, numpy=array([0, 2, 3], '
                 'dtype=int32)>\r\n'
                 '\r\n' \
                 '- The second bug is really inexplainable. Why there is only the gpu '
                 'specialisation of non_max_suppression_v2 ? Did the developer forgot '
                 'about it? This was mentioned in several issues under the name : non '
                 'max suppression work only on cpu. This is completely understandable '
                 'because the default version of non_max_suppression is v3 which '
                 'dosent have a gpu specialisation. \r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'on colab u can just copy this code:\r\n'
                 '\r\n'
                 '```\r\n'
                 'import numpy as np\r\n'
                 '%tensorflow_version 2.x\r\n'
                 'import tensorflow as tf\r\n'
                 'tf.debugging.set_log_device_placement(True)\r\n' \
                 'boxes = np.array([[0.1,0.1,0.2,0.2], [0.3,0.3,0.3,0.4], '
                 '[0.5,0.5,0.6,0.6], [0.7,0.7,0.8,0.8]], dtype= np.float32)\r\n'
                 'scores = np.array([0.9,0.8,0.7,0.6], dtype = np.float32)\r\n'
                 "with tf.device('/GPU:0'):\r\n"
                 '  print(tf.image.non_max_suppression(boxes, scores, 8))\r\n'
                 '```\r\n'
                 '\r\n' \
                 'output:\r\n'
                 '\r\n'
                 '> Executing op NonMaxSuppressionV3 in device '
                 '/job:localhost/replica:0/task:0/device:CPU:0\r\n'
                 '> tf.Tensor([0 1 1 1 1 1 1 1], shape=(8,), dtype=int32)\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n' \
                 '\r\n'
                 '> Executing op NonMaxSuppressionV3 in device '
                 '/job:localhost/replica:0/task:0/device:GPU:0\r\n' \
                 '\r\n'
                 'Correcting this issue is quite simple and straightforward. Gpu '
                 'specialisation must be made for non_max_suppression_v3 at least.',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 18.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): binary (pip, '
                 'tf-nightly-gpu)\r\n'
                 '- TensorFlow version (use command below): 2.1.0-dev20191228\r\n'
                 '- Python version: 2.7\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10.0\r\n'
                 '- GPU model and memory: Tesla M60 7618MiB\r\n'
                 '\r\n'
                 'Training a model with a ConvLSTM2D layer and float16 '
                 "(K.set_floatx('float16')) results in a type mismatch. "
                 'convolutional_recurrent.py:get_initial_state initializes zeros '
                 'without specifying a type, defaulting to float32.\r\n' \
                 '\r\n'
                 'For me, specifying the dtype as that of the function inputs '
                 'resolved the issue:\r\n' \
                 '```\r\n'
                 '    initial_state = self.cell.input_conv(initial_state,\r\n'
                 '                                         '
                 'array_ops.zeros(tuple(shape)),\r\n'
                 '                                         '
                 'padding=self.cell.padding)\r\n' \
                 '```\r\n'
                 'becomes\r\n'
                 '```\r\n'
                 '    initial_state = self.cell.input_conv(initial_state,\r\n'
                 '                                         '
                 'array_ops.zeros(tuple(shape),dtype=inputs.dtype),\r\n'
                 '                                         '
                 'padding=self.cell.padding)```\r\n',
         'created_at': '2019-12-2'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): '
                 'Raspberry pi 4\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: No\r\n'
                 '- TensorFlow installed from (source or binary): binary in pip\r\n'
                 '- TensorFlow version: 1.14\r\n'
                 '- Python version: 3.7\r\n'
                 '- Installed using  pip\r\n' \
                 '- Bazel version (if compiling from source): No\r\n'
                 '- GCC/Compiler version (if compiling from source): No\r\n' \
                 '- CUDA/cuDNN version: No\r\n' \
                 '- GPU model and memory: No\r\n'
                 '\r\n'
                 '\r\n' \
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '****\r\n'
                 'pi@raspberrypi:~/examples/lite/examples/object_detection/raspberry_pi '
                 '$ python3 detect_picamera.py   --model /tmp/detect.tflite   '
                 '--labels /tmp/coco_labels.txt\r\n'
                 '2019-12-28 17:43:12.464130: E ' \
                 'tensorflow/core/platform/hadoop/hadoop_file_system.cc:132] '
                 'HadoopFileSystem load error: libhdfs.so: cannot open shared object '
                 'file: No such file or directory\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "detect_picamera.py", line 34, in <module>\r\n'
                 '    from tf.lite.interpreter import Interpreter\r\n'
                 "ModuleNotFoundError: No module named 'tf'\r\n"
                 '***\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '***\r\n' \
                 ' Tensorflow model running issue replaced the code with in original '
                 'source code\r\n'
                 '\r\n'
                 '> from_ tflite_runtime.interpreter import Interpreter\r\n'
                 '\r\n' \
                 '***\r\n'
                 '**WIth**\r\n'
                 '***\r\n'
                 '> from tf.lite import Interpreter\r\n'
                 '***\r\n'
                 '\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.',
         'created_at': '2019-12-2'},
        {'body': 'removed the todo message, and plugged in the default ' \
                 "**reference_ops**'s negate.",
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n' \
                 'example from '
                 'https://www.tensorflow.org/tutorials/text/image_captioning\r\n'
                 '\r\n'
                 '- OS Platform and Distribution:Linux Ubuntu 18\r\n'
                 '- TensorFlow installed from (source or binary): bin\r\n'
                 '- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 '
                 '2.0.0 \r\n'
                 '- Python version: 3.6.8\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '> tf.saved_model.save(encoder, "./models/1/encoder")\r\n'
                 '\r\n'
                 'model get serialized with errors \r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'encoder can be loaded without errors\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '- train the model from '
                 'https://www.tensorflow.org/tutorials/text/image_captioning '
                 'example \r\n'
                 '\r\n'
                 '```\r\n'
                 'tf.saved_model.save(encoder, "./models/1/encoder")\r\n'
                 'encoder = tf.saved_model.load("./models/1/encoder")\r\n'
                 'encoder(img_tensor_val)\r\n'
                 '\r\n' \
                 '```\r\n'
                 'raises the error > \r\n'
                 "'_UserObject' object is not callable\r\n"
                 '\r\n'
                 '\r\n'
                 '`encoder.fc(img_tensor_val) \r\n'
                 '`\r\n'
                 'ValueError: Could not find matching function to call loaded from '
                 'the SavedModel. Got:\r\n'
                 '  Positional arguments (1 total):\r\n' \
                 '    * Tensor("inputs:0", shape=(1, 120, 64), dtype=float32)\r\n' \
                 '  Keyword arguments: {}\r\n'
                 '\r\n' \
                 'Expected these arguments to match one of the following 0 ' \
                 'option(s):\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'W1227 00:41:32.831883 139767989909312 save_impl.py:77] Skipping '
                 'full serialization of Keras model <__main__.CNN_Encoder object at '
                 '0x7f1daf5354e0>, because its inputs are not defined.\r\n',
         'created_at': '2019-12-2'},
        {'body': 'Chain multiple estimators to create a single SavedModel with a '
                 'single serving file/output.\r\n'
                 '\r\n' \
                 "Suppose I have 3 estimators with me, first is BoostedTrees, who's "
                 'output I want to use as input into DNNClassifier, and the output of '
                 'which I want to use in my custom Estimator.\r\n'
                 '\r\n'
                 'Is there a way to chain output/input of each other to create a '
                 '**mega** estimator of sorts.\r\n'
                 '\r\n'
                 'Please help me out here.',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code :\r\n'
                 '```java\r\n'
                 'private List<Delegate> mDelegates = new ArrayList<>();\r\n' \
                 'mDelegates.add(new GpuDelegate());\r\n'
                 '```\r\n'
                 '\r\n'
                 '```java\r\n'
                 '            Interpreter.Options options = null;\r\n'
                 '            if (interpreterOptions != null) {\r\n'
                 '                options = new '
                 'Interpreter.Options().setNumThreads(interpreterOptions.getNumThreads());\r\n'
                 '            }\r\n'
                 '            if (!mDelegates.isEmpty()) {\r\n'
                 '                Iterator<Delegate> it = mDelegates.iterator();\r\n'
                 '                while (it.hasNext()) {\r\n' \
                 '                    Delegate delegate = it.next();\r\n'
                 '                    mLogger.debug("addDelegate: " + delegate);\r\n'
                 '                    options.addDelegate(delegate);\r\n'
                 '                }\r\n'
                 '            }\r\n' \
                 '            mInterpreter = new Interpreter(bb, options);\r\n'
                 '```\r\n'
                 '\r\n'
                 '- Mobile device :\r\n'
                 'MI 8 / MIUI 11.0.4\r\n'
                 '\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 "implementation 'org.tensorflow:tensorflow-lite:2.0.0'\r\n"
                 "implementation 'org.tensorflow:tensorflow-lite-gpu:2.0.0'\r\n"
                 '\r\n'
                 '**Describe the current behavior**\r\n' \
                 "crashed when load the attached file with 'GpuDelegate'\r\n"
                 '\r\n' \
                 '**Describe the expected behavior**\r\n'
                 'load the attached file correct\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'N/A\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '```text\r\n'
                 '    java.lang.IllegalStateException: Internal error: Unexpected '
                 'failure when preparing tensor allocations: TfLiteGpuDelegate Init: '
                 'FULLY_CONNECTED: Amount of input data should match weights width\r\n'
                 '    TfLiteGpuDelegate Prepare: delegate is not initialized\r\n'
                 '    Node number 3 (TfLiteGpuDelegateV2) failed to prepare.\r\n' \
                 '    \r\n'
                 '    Restored previous execution plan after delegate application '
                 'failure.\r\n' \
                 '        at '
                 'org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native '
                 'Method)\r\n'
                 '        at '
                 'org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:162)\r\n'
                 '        at ' \
                 'org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n'
                 '        at ' \
                 'com.didi.aoe.runtime.tensorflow.lite.TensorFlowMultipleInputsOutputsInterpreter.run(TensorFlowMultipleInputsOutputsInterpreter.java:159)\r\n'
                 '        at '
                 'com.didi.aoe.library.core.NativeProcessorWrapper.run(NativeProcessorWrapper.java:40)\r\n'
                 '```\r\n'
                 '\r\n'
                 '[mnist_cnn_keras.tflite.zip](https://github.com/tensorflow/tensorflow/files/4004963/mnist_cnn_keras.tflite.zip)\r\n',
         'created_at': '2019-12-2'},
        {'body': 'Incompatible flag --incompatible_no_implicit_file_export will be '
                 'enabled by default in a future Bazel release [1], thus breaking ' \
                 'TensorFlow.\n'
                 '\n'
                 'The flag is documented here: '
                 'https://github.com/bazelbuild/bazel/issues/10225\n'
                 '\n'
                 'Please check the following CI builds for build and test results:\n'
                 '\n'
                 '* <a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#3e0dba20-d84a-4394-bbe8-4155855f8aa5" ' \
                 'target="_blank"><img ' \
                 'src="https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/mac.png" '
                 'height="16"/>macOS, OpenJDK 8</a>\n'
                 '* <a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#8067be5e-7d21-4333-a4e1-6e2817d456d4" '
                 'target="_blank"><img '
                 'src="https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/ubuntu.png" '
                 'height="16"/>Ubuntu 18.04, OpenJDK 11</a>\n'
                 '* <a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/361#5664296d-5125-45bc-a16b-80fb8b5b94f1" '
                 'target="_blank"><img ' \
                 'src="https://raw.githubusercontent.com/buildkite/emojis/master/img-buildkite-64/windows.png" '
                 'height="16"/>Windows, OpenJDK 8</a>\n'
                 '\n'
                 'Never heard of incompatible flags before? We have ' \
                 '[documentation](https://docs.bazel.build/versions/master/backward-compatibility.html) '
                 'that explains everything.\n'
                 '\n'
                 "If you don't want to receive any future issues for TensorFlow or if "
                 'you have any questions,\n'
                 'please file an issue in '
                 'https://github.com/bazelbuild/continuous-integration\n'
                 '\n'
                 '**Important**: Please do NOT modify the issue title since that ' \
                 'might break our tools.\n'
                 '\n' \
                 "[1] The target release hasn't been determined yet. Our tool will " \
                 'update the issue title once the flag flip has been scheduled.\n',
         'created_at': '2019-12-2'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests ' \
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n' \
                 '\r\n'
                 '**System information**\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu ' \
                 '19.0\r\n' \
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the ' \
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): Source\r\n' \
                 '- TensorFlow version: 2.0\r\n'
                 '- Python version: 3.7.4\r\n' \
                 '- Installed using virtualenv? pip? conda?: pip\r\n'
                 '- Bazel version (if compiling from source): 1.1.0\r\n'
                 '- GCC/Compiler version (if compiling from source): 8.3.0\r\n'
                 '- CUDA/cuDNN version: n/a\r\n'
                 '- GPU model and memory: n/a\r\n' \
                 '\r\n'
                 '\r\n' \
                 '\r\n'
                 '**Describe the problem**\r\n' \
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed ' \
                 'before running into the problem**\r\n' \
                 'After about 20+ minutes of watching the code build the build ' \
                 'stopped abruptly\r\n' \
                 '\r\n'
                 '**Any other info / logs**\r\n' \
                 'Here is a link to an enviorment where you can run my exact dev-env '
                 'in the cloud\r\n'
                 'https://gitpod.io/#https://github.com/JesterOrNot/tensorflow/tree/JesterOrNot/gitpod-setup\r\n' \
                 '\r\n' \
                 '```\r\n'
                 './tensorflow/python/lib/core/pybind11_proto.h:40:44: warning: ' \
                 "'pybind11::str pybind11::detail::object_api<Derived>::str() const "
                 '[wi\r\n' \
                 "th Derived = pybind11::handle]' is deprecated: Use py::str(obj) " \
                 'instead [-Wdeprecated-declarations]\r\n' \
                 '       std::string(py_object.get_type().str()), " is not a valid ' \
                 'proto."));\r\n'
                 '                                            ^\r\n'
                 'In file included from '
                 'external/pybind11/include/pybind11/cast.h:13,\r\n'
                 '                 from '
                 'external/pybind11/include/pybind11/attr.h:13,\r\n' \
                 '                 from ' \
                 'external/pybind11/include/pybind11/pybind11.h:49,\r\n' \
                 '                 from '
                 'tensorflow/python/client/device_lib_wrapper.cc:18:\r\n' \
                 'external/pybind11/include/pybind11/pytypes.h:147:19: note: declared ' \
                 'here\r\n'
                 '     pybind11::str str() const;\r\n'
                 '                   ^~~\r\n'
                 'INFO: From Compiling ' \
                 'tensorflow/stream_executor/stream_executor_pimpl.cc [for host]:\r\n' \
                 'tensorflow/stream_executor/stream_executor_pimpl.cc: In member '
                 "function 'stream_executor::DeviceMemoryBase "
                 'stream_executor::StreamE\r\n' \
                 "xecutor::Allocate(tensorflow::uint64, tensorflow::int64)':\r\n" \
                 'tensorflow/stream_executor/stream_executor_pimpl.cc:462:31: '
                 'warning: comparison of integer expressions of different signedness: '
                 "'lo\r\n"
                 "ng long unsigned int' and 'tensorflow::int64' {aka 'long long int'} "
                 '[-Wsign-compare]\r\n' \
                 '       mem_alloc_bytes_ + size > memory_limit_bytes_) {\r\n'
                 '       ~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~\r\n'
                 'In file included from ' \
                 './tensorflow/core/platform/default/logging.h:29,\r\n'
                 '                 from ./tensorflow/core/platform/logging.h:27,\r\n' \
                 '                 from ./tensorflow/core/platform/status.h:24,\r\n'
                 '                 from ./tensorflow/core/platform/errors.h:22,\r\n'
                 '                 from ./tensorflow/core/lib/core/errors.h:19,\r\n'
                 '                 from ' \
                 './tensorflow/stream_executor/device_memory_allocator.h:23,\r\n' \
                 '                 from '
                 './tensorflow/stream_executor/stream_executor_pimpl.h:28,\r\n'
                 '                 from '
                 'tensorflow/stream_executor/stream_executor_pimpl.cc:20:\r\n'
                 './tensorflow/core/platform/default/logging.h: In instantiation of '
                 "'std::__cxx11::string* tensorflow::internal::Check_EQImpl(const "
                 'T1&, const T2&, const char*) [with T1 = int; T2 = long long '
                 'unsigned int; std::__cxx11::string = '
                 "std::__cxx11::basic_string<char>]':\r\n" \
                 'tensorflow/stream_executor/stream_executor_pimpl.cc:700:3:   '
                 'required from here\r\n'
                 './tensorflow/core/platform/default/logging.h:386:25: warning: '
                 "comparison of integer expressions of different signedness: 'const "
                 "int' and 'const long long unsigned int' [-Wsign-compare]\r\n"
                 '                         ==)  // Compilation error with ' \
                 'CHECK_EQ(NULL, x)?\r\n'
                 './tensorflow/core/platform/macros.h:88:49: note: in definition of '
                 "macro 'TF_PREDICT_TRUE'\r\n"
                 ' #define TF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\r\n'
                 '                                                 ^\r\n'
                 './tensorflow/core/platform/default/logging.h:385:1: note: in ' \
                 "expansion of macro 'TF_DEFINE_CHECK_OP_IMPL'\r\n"
                 ' TF_DEFINE_CHECK_OP_IMPL(Check_EQ,\r\n' \
                 ' ^~~~~~~~~~~~~~~~~~~~~~~\r\n'
                 'INFO: From Compiling '
                 'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc '
                 '[for host]:\r\n'
                 'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc: '
                 "In function 'tensorflow::Status tensorflow::{anonymous}::GetT\r\n"
                 'PUDevices(tensorflow::Devices, '
                 'llvm::ArrayRef<tensorflow::DeviceNameUtils::ParsedName>, '
                 'llvm::SmallVectorImpl<llvm::SmallVector<ten\r\n'
                 "sorflow::DeviceNameUtils::ParsedName, 8> >*)':\r\n" \
                 'tensorflow/compiler/mlir/tensorflow/utils/tpu_rewrite_device_util.cc:129:27: ' \
                 'warning: comparison of integer expressions of differen\r\n'
                 "t signedness: 'int' and 'size_t' {aka 'long unsigned int'} "
                 '[-Wsign-compare]\r\n'
                 '     if (num_tpus_per_host != host_tpu_devices.size())\r\n'
                 '         ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~\r\n'
                 'INFO: From Compiling tensorflow/core/kernels/quantization_utils.cc '
                 '[for host]:\r\n' \
                 'In file included from '
                 'external/gemmlowp/public/../internal/dispatch_gemm_shape.h:23,\r\n' \
                 '                 from external/gemmlowp/public/gemmlowp.h:19,\r\n' \
                 '                 from '
                 './tensorflow/core/kernels/quantization_utils.h:37,\r\n' \
                 '                 from '
                 'tensorflow/core/kernels/quantization_utils.cc:16:\r\n'
                 'external/gemmlowp/public/../internal/multi_thread_gemm.h: In member ' \
                 "function 'void gemmlowp::WorkersPool::LegacyExecuteAndDestroyTa\r\n" \
                 "sks(const std::vector<gemmlowp::Task*>&)':\r\n"
                 'external/gemmlowp/public/../internal/multi_thread_gemm.h:405:23: '
                 'warning: comparison of integer expressions of different '
                 'signedness\r\n' \
                 ": 'int' and 'std::size_t' {aka 'long unsigned int'} " \
                 '[-Wsign-compare]\r\n'
                 '     for (int i = 0; i < tasks_count - 1; i++) {\r\n'
                 '                     ~~^~~~~~~~~~~~~~~~~\r\n'
                 'In file included from ' \
                 'tensorflow/core/kernels/quantization_utils.cc:16:\r\n'
                 "./tensorflow/core/kernels/quantization_utils.h: In function 'void "
                 'tensorflow::RequantizeManyInNewRangeReference(const qint32*, '
                 'tensorflow::int64, float, float, float, float, '
                 "tensorflow::quint8*)':\r\n" \
                 './tensorflow/core/kernels/quantization_utils.h:271:32: warning: ' \
                 "comparison of integer expressions of different signedness: 'size_t' "
                 "{aka 'long unsigned int'} and 'tensorflow::int64' {aka 'long long " \
                 "int'} [-Wsign-compare]\r\n"
                 '   for (size_t index = 0; index < count; ++index) {\r\n' \
                 '                          ~~~~~~^~~~~~~\r\n' \
                 '[6,434 / 12,032] 16 actions running\r\n' \
                 '    Compiling tensorflow/python/tfe_wrapper.cc [for host]; 77s '
                 'local\r\n' \
                 '    Compiling tensorflow/core/kernels/rnn/lstm_ops.cc [for host]; '
                 '44s local\r\n' \
                 '    Compiling tensorflow/core/kernels/rnn/gru_ops.cc [for host]; '
                 '43s local\r\n' \
                 '    Compiling tensorflow/stream_executor/stream.cc [for host]; 41s '
                 'local\r\n'
                 '    Compiling tensorflow/core/kernels/split_lib_cpu.cc [for host]; ' \
                 '28s local\r\n'
                 '    Compiling .../core/kernels/serialize_sparse_op.cc [for host]; ' \
                 '24s local\r\n' \
                 '    //tensorflow/core/kernels:deserialize_sparse_string_op; 22s ' \
                 'local\r\n'
                 '    Compiling .../core/kernels/sparse_reorder_op.cc [for host]; 22s ' \
                 'local ...\r\n' \
                 '\r\n'
                 "Server terminated abruptly (error code: 14, error message: 'Socket " \
                 "closed', log file: "
                 "'/home/gitpod/.cache/bazel/_bazel_gitpod/2c92b5569ddded7b3a6bd5e139451b60/server/jvm.out'\r\n"
                 '```', \
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): No\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: Pixel 2 Simulator in Android Studio '
                 'with Android 7.1.1\r\n' \
                 '- TensorFlow installed from (source or binary): source\r\n' \
                 '- TensorFlow version (use command below): 1.12.2\r\n'
                 '- Python version: 3.6.9\r\n' \
                 '- Bazel version (if compiling from source): 0.15.0\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n' \
                 '- CUDA/cuDNN version: 9.0/7.5\r\n'
                 '- GPU model and memory: 1070 max-q 8 GB\r\n'
                 '\r\n' \
                 '\r\n'
                 '**Describe the current behavior**\r\n' \
                 'I fine-tuned the ssd_mobilenet_v2 pretrained model from Tensorflow '
                 'model zoo to detect two classes. Training was done with TF OD API. '
                 'After converting the model to detect.tflite and put it in the '
                 '"assets" folder of the [official Android '
                 'demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) ' \
                 'and modified labelmap.txt, upon running I got the following ' \
                 'error:\r\n' \
                 '```java\r\n'
                 'E/AndroidRuntime: FATAL EXCEPTION: inference\r\n'
                 '    Process: org.tensorflow.lite.examples.detection, PID: 4609\r\n'
                 '    java.lang.IllegalArgumentException: Internal error: Failed to ' \
                 'run on the given Interpreter: '
                 'tensorflow/lite/kernels/detection_postprocess.cc:404 ' \
                 'ValidateBoxes(decoded_boxes, num_boxes) was not true.\r\n' \
                 '    Node number 102 (TFLite_Detection_PostProcess) failed to ' \
                 'invoke.\r\n'
                 '    \r\n'
                 '        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native '
                 'Method)\r\n'
                 '        at '
                 'org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:152)\r\n'
                 '        at '
                 'org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:296)\r\n'
                 '        at ' \
                 'org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:193)\r\n'
                 '        at '
                 'org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:183)\r\n' \
                 '        at android.os.Handler.handleCallback(Handler.java:751)\r\n'
                 '        at android.os.Handler.dispatchMessage(Handler.java:95)\r\n'
                 '        at android.os.Looper.loop(Looper.java:154)\r\n'
                 '        at android.os.HandlerThread.run(HandlerThread.java:61)\r\n'
                 '```\r\n' \
                 'However, if I use model.ckpt-0 which is the initial checkpoint, the '
                 'converted .tflite works fine. So I assume that the convertion is '
                 'OK. Is it due to the training process?\r\n'
                 'Please help me figure this out.\r\n', \
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n' \
                 '* Have I written custom code (as opposed to using a stock example ' \
                 'script provided in TensorFlow): yes\r\n' \
                 '* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04.3 LTS\r\n'
                 '* TensorFlow installed from (source or binary): binary\r\n'
                 '* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece ' \
                 '2.1.0-rc2 (python3 -c "import tensorflow as tf; '
                 'print(tf.version.GIT_VERSION, tf.version.VERSION)")\r\n'
                 '* Python version: Python 3.6.8\r\n'
                 '* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: ' \
                 '10.2, cuDNN 7.6.2\r\n'
                 '* GPU model and memory: Tesla V100-SXM2-16GB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n' \
                 "I'm using Bijectors as a flexible prior for a VAE. \r\n" \
                 '\r\n'
                 'In tf2.1 autograph distributed mirrored mode \r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L1152\r\n' \
                 'I am getting\r\n'
                 '```\r\n' \
                 'google.protobuf.message.DecodeError: Error parsing message\r\n'
                 '```\r\n'
                 'when running with multiple GPUs (but not when running with single '
                 'GPU):\r\n' \
                 '```\r\n'
                 'bijectors = []\r\n'
                 'for i in range(16):\r\n' \
                 '    bijectors.append(tfb.MaskedAutoregressiveFlow(\r\n'
                 '      '
                 'shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\r\n'
                 '          code, hidden_layers=[1024, 1024], name=scope + "/maf_" + '
                 'str(i))))\r\n'
                 '\r\n'
                 '    bijectors.append(tfb.BatchNormalization(\r\n'
                 '        batchnorm_layer=tf.layers.BatchNormalization(\r\n'
                 "                            name=scope + '/batch_norm_' + "
                 'str(i)),\r\n' \
                 "        name=scope + '/batch_norm_bijector' + str(i)))\r\n"
                 '\r\n'
                 "    permutation=tf.get_variable('permutation_'+str(i), "
                 'initializer=np.random.permutation(out_channel).astype("int32"), ' \
                 'trainable=False)\r\n'
                 '    bijectors.append(tfb.Permute(permutation))\r\n'
                 '    \r\n'
                 'flow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))\r\n'
                 '```\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Should not crash\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'TF2.x code:\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n' \
                 '```\r\n' \
                 'Traceback (most recent call last):\r\n'
                 '  File "main.py", line 134, in <module>\r\n' \
                 '    main()\r\n'
                 '  File "main.py", line 121, in main\r\n'
                 '    gan.train()\r\n' \
                 '  File "/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py", line 1379, ' \
                 'in train\r\n'
                 '    train_loop()\r\n'
                 '  File "/app/home/ubuntu/SPADE-Tensorflow.tf2/SPADE.py", line 1336, '
                 'in train_loop\r\n'
                 '    counter, result_inputs, result_losses_det, result_outputs_det, '
                 'result_outputs_resample_det, result_outputs_random_det, '
                 'result_outputs_random_gen_det = train_det_grad_both(global_step, '
                 'self.train_main, *inputs)\r\n'
                 '  File ' \
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 568, in __call__\r\n'
                 '    result = self._call(*args, **kwds)\r\n'
                 '  File ' \
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", '
                 'line 615, in _call\r\n'
                 '    self._initialize(args, kwds, '
                 'add_initializers_to=initializers)\r\n'
                 '  File ' \
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py", ' \
                 'line 497, in _initialize\r\n' \
                 '    *args, **kwds))\r\n' \
                 '  File '
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", ' \
                 'line 2389, in _get_concrete_function_internal_garbage_collected\r\n'
                 '    graph_function, _, _ = self._maybe_define_function(args, '
                 'kwargs)\r\n' \
                 '  File '
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 2703, in _maybe_define_function\r\n'
                 '    graph_function = self._create_graph_function(args, kwargs)\r\n'
                 '  File ' \
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 2599, in _create_graph_function\r\n'
                 '    shared_func_graph=False)\r\n'
                 '  File '
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 1511, in __init__\r\n'
                 '    func_graph, self._attrs, self._garbage_collector)\r\n'
                 '  File '
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 601, in __init__\r\n'
                 '    self._func_graph.inputs, self._func_graph.outputs, attrs)\r\n'
                 '  File '
                 '"/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py", '
                 'line 466, in __init__\r\n' \
                 '    function_def.ParseFromString(compat.as_bytes(proto_data))\r\n'
                 'google.protobuf.message.DecodeError: Error parsing message\r\n' \
                 '```\r\n' \
                 '[train.CelebAMask-HQ.tf21.4xgpu.maf.log](https://github.com/tensorflow/tensorflow/files/4001259/train.CelebAMask-HQ.tf21.4xgpu.maf.log)\r\n',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '* Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): yes\r\n' \
                 '* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04.3 LTS\r\n'
                 '* TensorFlow installed from (source or binary): binary\r\n'
                 '* TensorFlow version (use command below): v2.1.0-rc1-58-g9837ece ' \
                 '2.1.0-rc2 (python3 -c "import tensorflow as tf; '
                 'print(tf.version.GIT_VERSION, tf.version.VERSION)")\r\n'
                 '* Python version: Python 3.6.8\r\n'
                 '* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: '
                 '10.2, cuDNN 7.6.2\r\n'
                 ' *GPU model and memory: Tesla V100-SXM2-16GB\r\n' \
                 '\r\n'
                 '**Describe the current behavior**\r\n' \
                 "I'm using Bijectors as a flexible prior for a VAE. \r\n"
                 '\r\n'
                 'This code has negligible overhead in tf1.x (for input batch size ' \
                 '18x256x256x3). In tf2.1 autograph distributed mirrored mode \r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L1152\r\n'
                 'with single GPU it increases training step duration 1 second '
                 '(tf1.x) -> 1.9 seconds (tf2.1):\r\n'
                 '```\r\n' \
                 'bijectors = []\r\n' \
                 'for i in range(16):\r\n'
                 '    bijectors.append(tfb.MaskedAutoregressiveFlow(\r\n' \
                 '      '
                 'shift_and_log_scale_fn=tfb.masked_autoregressive_default_template(\r\n'
                 '          code, hidden_layers=[1024, 1024], name=scope + "/maf_" + '
                 'str(i))))\r\n'
                 '\r\n' \
                 '    bijectors.append(tfb.BatchNormalization(\r\n' \
                 '        batchnorm_layer=tf.layers.BatchNormalization(\r\n'
                 "                            name=scope + '/batch_norm_' + "
                 'str(i)),\r\n'
                 "        name=scope + '/batch_norm_bijector' + str(i)))\r\n"
                 '\r\n' \
                 "    permutation=tf.get_variable('permutation_'+str(i), " \
                 'initializer=np.random.permutation(out_channel).astype("int32"), '
                 'trainable=False)\r\n' \
                 '    bijectors.append(tfb.Permute(permutation))\r\n' \
                 '    \r\n'
                 'flow_bijector = tfb.Chain(list(reversed(bijectors[:-1])))\r\n' \
                 '```\r\n' \
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n'
                 '\r\n' \
                 "I'm using custom masked autoregressive template\r\n" \
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py\r\n'
                 'but it is as slow with the default one:\r\n'
                 'https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/masked_autoregressive_default_template\r\n'
                 '\r\n'
                 'Possible suspects:\r\n'
                 '```\r\n' \
                 'tfb.masked_dense\r\n'
                 '```\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py#L44\r\n' \
                 '\r\n'
                 '```\r\n'
                 'tf1.make_template()\r\n'
                 '```\r\n' \
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/masked_autoregressive.py#L115\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'Performance in tf2.1 and tf1.x should be comparable.\r\n' \
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'TF2.x code:\r\n' \
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/85b5fd7943296561dc3d54557fec5346c2adea58/SPADE.py#L190\r\n' \
                 '\r\n'
                 'TF1.x code:\r\n' \
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/develop/SPADE.py#L190\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose ' \
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n' \
                 '- OS Platform and Distribution: Windows10 x64\r\n'
                 '- TensorFlow installed from: binary\r\n' \
                 '- TensorFlow version: 1.15.0\r\n'
                 '- Python version: 3.7\r\n'
                 '- Installed using: conda\r\n'
                 '- Bazel version (if compiling from source): 1.1/2.0\r\n'
                 '- CUDA/cuDNN version: 10.0\r\n'
                 '\r\n' \
                 'I was trying to inspect a model following the guide '
                 '[here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#inspecting-graphs)\r\n'
                 'When running the command\r\n' \
                 '```bash\r\n' \
                 'bazel build tensorflow/tools/graph_transforms:summarize_graph\r\n'
                 '```\r\n' \
                 'it failed with these logs\r\n'
                 '```\r\n'
                 'INFO: Writing tracer profile to '
                 "'C:/users/yy/_bazel_yy/zxtlmlwl/command.profile.gz'\r\n"
                 'INFO: Options provided by the client:\r\n'
                 "  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\n"
                 'INFO: Options provided by the client:\r\n'
                 "  'build' options: "
                 '--python_path=C:/Users/YY/Anaconda3/python.exe\r\n'
                 "INFO: Reading rc options for 'build' from "
                 'e:\\tensorflow\\tensorflow\\.bazelrc:\r\n' \
                 "  'build' options: --apple_platform_type=macos --define " \
                 'framework_shared_object=true --define open_source_build=true '
                 '--java_toolchain=//third_party/toolchains/java:tf_java_toolchain ' \
                 '--host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain ' \
                 '--define=use_fast_cpp_protos=true '
                 '--define=allow_oversize_protos=true --spawn_strategy=standalone -c ' \
                 'opt --announce_rc --define=grpc_no_ares=true '
                 '--noincompatible_remove_legacy_whole_archive ' \
                 '--enable_platform_specific_config --config=v2\r\n' \
                 'INFO: Found applicable config definition build:v2 in file '
                 'e:\\tensorflow\\tensorflow\\.bazelrc: --define=tf_api_version=2 '
                 '--action_env=TF2_BEHAVIOR=1\r\n'
                 'INFO: Found applicable config definition build:windows in file ' \
                 'e:\\tensorflow\\tensorflow\\.bazelrc: --copt=/w '
                 '--copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 '
                 '--host_cxxopt=/std:c++14 --config=monolithic '
                 '--copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN '
                 '--copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG '
                 '--host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF ' \
                 '--linkopt=/OPT:ICF --host_linkopt=/OPT:ICF '
                 '--experimental_strict_action_env=true '
                 '--incompatible_windows_native_test_wrapper --verbose_failures ' \
                 '--distinct_host_configuration=false\r\n'
                 'INFO: Found applicable config definition build:monolithic in file ' \
                 'e:\\tensorflow\\tensorflow\\.bazelrc: --define ' \
                 'framework_shared_object=false\r\n'
                 'INFO: Call stack for the definition of repository '
                 "'com_google_protobuf' which is a tf_http_archive (rule definition " \
                 'at E:/tensorflow/tensorflow/third_party/repo.bzl:121:19):\r\n'
                 ' - E:/tensorflow/tensorflow/tensorflow/workspace.bzl:457:5\r\n'
                 ' - E:/tensorflow/tensorflow/WORKSPACE:19:1\r\n'
                 "INFO: Repository 'com_google_protobuf' used the following cache " \
                 'hits instead of downloading the corresponding file.\r\n'
                 ' * Hash '
                 "'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' " \
                 'for '
                 'https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\r\n'
                 "If the definition of 'com_google_protobuf' was updated, verify that "
                 'the hashes were also updated.\r\n' \
                 'ERROR: An error occurred during the fetch of repository ' \
                 "'com_google_protobuf':\r\n"
                 '   Traceback (most recent call last):\r\n' \
                 '        File "E:/tensorflow/tensorflow/third_party/repo.bzl", line '
                 '101\r\n'
                 '                _apply_patch(ctx, <1 more arguments>)\r\n'
                 '        File "E:/tensorflow/tensorflow/third_party/repo.bzl", line ' \
                 '68, in _apply_patch\r\n'
                 '                _execute_and_check_ret_code(ctx, <1 more '
                 'arguments>)\r\n' \
                 '        File "E:/tensorflow/tensorflow/third_party/repo.bzl", line '
                 '52, in _execute_and_check_ret_code\r\n'
                 '                fail(<1 more arguments>)\r\n' \
                 'Non-zero return code(2) when executing ' \
                 '\'C:\\Windows\\system32\\bash.exe -l -c "patch" "-p1" "-d" '
                 '"C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf" "-i" '
                 '"E:/tensorflow/tensorflow/third_party/protobuf/protobuf.patch"\':\r\n'
                 'Stdout:\r\n'
                 "Stderr: patch: **** Can't change to directory "
                 'C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No '
                 'such file or directory\r\n'
                 'ERROR: Analysis of target '
                 "'//tensorflow/tools/graph_transforms:summarize_graph' failed; build "
                 "aborted: no such package '@com_google_protobuf//': Traceback (most "
                 'recent call last):\r\n'
                 '        File "E:/tensorflow/tensorflow/third_party/repo.bzl", line '
                 '101\r\n'
                 '                _apply_patch(ctx, <1 more arguments>)\r\n'
                 '        File "E:/tensorflow/tensorflow/third_party/repo.bzl", line '
                 '68, in _apply_patch\r\n' \
                 '                _execute_and_check_ret_code(ctx, <1 more '
                 'arguments>)\r\n'
                 '        File "E:/tensorflow/tensorflow/third_party/repo.bzl", line ' \
                 '52, in _execute_and_check_ret_code\r\n'
                 '                fail(<1 more arguments>)\r\n'
                 'Non-zero return code(2) when executing '
                 '\'C:\\Windows\\system32\\bash.exe -l -c "patch" "-p1" "-d" '
                 '"C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf" "-i" '
                 '"E:/tensorflow/tensorflow/third_party/protobuf/protobuf.patch"\':\r\n'
                 'Stdout:\r\n'
                 "Stderr: patch: **** Can't change to directory "
                 'C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No '
                 'such file or directory\r\n'
                 'INFO: Elapsed time: 3.975s\r\n'
                 'INFO: 0 processes.\r\n'
                 'FAILED: Build did NOT complete successfully (0 packages loaded, 0 '
                 'targets configured)\r\n'
                 '    currently loading: tensorflow\r\n' \
                 '```\r\n'
                 '\r\n'
                 "I've tried diferent versions of bazel, 0.20, 1.1.0, and 2.0.0，and "
                 '```bazel clean```, the error is still there.\r\n'
                 'What makes me confused is that the " Can\'t change to directory '
                 'C:/users/yy/_bazel_yy/zxtlmlwl/external/com_google_protobuf : No ' \
                 'such file or directory", which in fact I can find the dir at that ' \
                 'path\r\n'
                 '\r\n'
                 "I've checked other similar issue, but none can fix this error\r\n", \
         'created_at': '2019-12-2'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), ' \
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n' \
                 '\r\n'
                 '**System information**\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ' \
                 'Ubuntu18.04.3 LTS, 64bits\r\n' \
                 '- TensorFlow installed from: ' \
                 'https://github.com/tensorflow/tensorflow, \r\n'
                 '- TensorFlow version: 8e8fabfee3\r\n' \
                 '- Python version: Python 3.6.9 :: Anaconda, Inc.\r\n'
                 '- Installed using virtualenv? pip? conda?: conda\r\n'
                 '- Bazel version (if compiling from source): 1.1.0\r\n'
                 '- GCC/Compiler version (if compiling from source): Not compiled '
                 'from source\r\n'
                 '- CUDA/cuDNN version: None\r\n'
                 '- GPU model and memory: None\r\n'
                 '\r\n' \
                 '\r\n' \
                 '\r\n'
                 '**Describe the problem**\r\n' \
                 '\r\n'
                 'I can build the libtensorflow-lite.a. by \r\n'
                 '\r\n'
                 '`./tensorflow/lite/tools/make/build_aarch64_lib.sh`\r\n' \
                 '\r\n' \
                 'But cannot build the gl_delegate by\r\n' \
                 '\r\n' \
                 '`bazel build -c opt --config android_arm64 ' \
                 'tensorflow/lite/delegates/gpu:gl_delegate`\r\n' \
                 '\r\n' \
                 '**Provide the exact sequence of commands / steps that you executed ' \
                 'before running into the problem**\r\n'
                 '\r\n' \
                 '1. Install tensorflow and build the libtensorflow-lite.a by the ' \
                 'instructions from '
                 '[here](https://www.tensorflow.org/lite/guide/build_arm64)\r\n' \
                 '2. Install bazel by the instructions from ' \
                 '[here](https://docs.bazel.build/versions/master/install-ubuntu.html) \r\n' \
                 '3. Install anaconda, create environment\r\n' \
                 '`conta create --name tensorflow python=3.6`\r\n' \
                 '4. Install tensorflow and others tools by conda\r\n'
                 '5. Build gl_delegate by `bazel build -c opt --config android_arm64 ' \
                 'tensorflow/lite/delegates/gpu:gl_delegate`\r\n'
                 '\r\n' \
                 '**Any other info / logs**\r\n' \
                 '\r\n' \
                 '> NFO: Writing tracer profile to ' \
                 "'/home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/command.profile.gz'\r\n" \
                 '> INFO: Options provided by the client:\r\n'
                 ">   Inherited 'common' options: --isatty=1 "
                 '--terminal_columns=111\r\n' \
                 "> INFO: Reading rc options for 'build' from " \
                 '/home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc:\r\n'
                 ">   'build' options: --apple_platform_type=macos --define " \
                 'framework_shared_object=true --define open_source_build=true ' \
                 '--java_toolchain=//third_party/toolchains/java:tf_java_toolchain ' \
                 '--host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain ' \
                 '--define=use_fast_cpp_protos=true ' \
                 '--define=allow_oversize_protos=true --spawn_strategy=standalone -c ' \
                 'opt --announce_rc --define=grpc_no_ares=true ' \
                 '--noincompatible_remove_legacy_whole_archive ' \
                 '--enable_platform_specific_config --config=v2\r\n' \
                 '> INFO: Found applicable config definition build:v2 in file ' \
                 '/home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: ' \
                 '--define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n' \
                 '> INFO: Found applicable config definition build:android_arm64 in ' \
                 'file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --config=android ' \
                 '--cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n'
                 '> INFO: Found applicable config definition build:android in file ' \
                 '/home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: '
                 '--crosstool_top=//external:android/crosstool '
                 '--host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n' \
                 '> INFO: Found applicable config definition build:linux in file '
                 '/home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: --copt=-w '
                 '--define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib '
                 '--define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 ' \
                 '--host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n' \
                 '> INFO: Found applicable config definition build:dynamic_kernels in '
                 'file /home/yyyy/Qt/3rdLibs/tensorflow/.bazelrc: '
                 '--define=dynamic_loaded_kernels=true '
                 '--copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n' \
                 "> DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical " \
                 'reproducible form can be obtained by modifying arguments ' \
                 'shallow_since = "1556410077 -0400"\r\n' \
                 '> DEBUG: Call stack for the definition of repository '
                 "'io_bazel_rules_docker' which is a git_repository (rule definition " \
                 'at ' \
                 '/home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n' \
                 '>  - ' \
                 '/home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n'
                 '>  - /home/yyyy/Qt/3rdLibs/tensorflow/WORKSPACE:37:1\r\n'
                 '> ERROR: '
                 '/home/yyyy/.cache/bazel/_bazel_yyyy/9f4180a9f81bdaca56a8020b37cc35bc/external/local_config_cc/BUILD:47:1: ' \
                 'in cc_toolchain_suite rule @local_config_cc//:toolchain: ' \
                 "cc_toolchain_suite '@local_config_cc//:toolchain' does not contain " \
                 "a toolchain for cpu 'arm64-v8a'\r\n" \
                 '> ERROR: Analysis of target '
                 "'//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so' " \
                 'failed; build aborted: Analysis of target ' \
                 "'@local_config_cc//:toolchain' failed; build aborted\r\n" \
                 '> INFO: Elapsed time: 0.450s\r\n' \
                 '> INFO: 0 processes.\r\n' \
                 '> FAILED: Build did NOT complete successfully (1 packages loaded, 1 ' \
                 'target configured)', \
         'created_at': '2019-12-2'},
        {'body': 'I can see many users over the last 2 weeks struggling with getting ' \
                 'the build to work with Windows 10. I also see a few **recent ' \
                 'commits** to improve the build in response to the above ' \
                 'feedback.\r\n' \
                 '\r\n' \
                 'As [stated here](https://www.tensorflow.org/install/source_windows) ' \
                 'that the Windows tensorflow 2.0 builds with following ' \
                 'configurations are successful.\r\n' \
                 '\r\n'
                 '![image](https://user-images.githubusercontent.com/59223977/71443261-34f74d00-270a-11ea-8cc5-ee2902f336b8.png)\r\n'
                 '![image](https://user-images.githubusercontent.com/59223977/71443269-44769600-270a-11ea-8857-750f7f3598ee.png)\r\n' \
                 '\r\n' \
                 'Please **update** [the ' \
                 'links](https://www.tensorflow.org/install/lang_c) here with the '
                 'successful windows tensorflow 2.0 builds provided above \r\n' \
                 '\r\n'
                 'For example: we need\r\n'
                 'https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.0.0.zip\r\n'
                 'https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.0.0.zip\r\n' \
                 '\r\n' \
                 '_Although these tentative builds are old_, but it will provide a '
                 'starting point **to start TESTING the Tensorflow 2.0 features '
                 'through bindings.**',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- windows\r\n'
                 '- TensorFlow installed from conda\r\n' \
                 '- TensorFlow version:2.0\r\n'
                 '- Python version:3.7.4\r\n'
                 '- CUDA/cuDNN version: 10.2/7.6\r\n'
                 '- GPU model and memory: 2 nvidia rtx 2070s 8GB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'i follow the distributed training tutorials to change my '
                 'code(custom model) for cumtom training loop. but when i run the '
                 'script, it shows mistake "ValueError: Flattening a PerReplica to '
                 'components is not supported in replica context.". i don\'t ' \
                 'understande why it is happens.  it can run in the train step,  but ' \
                 "it can't run in the test step\r\n"
                 '**Describe the expected behavior**\r\n' \
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary ' \
                 'to generate the problem.\r\n'
                 '```\r\n' \
                 '# i use this strategy before,but it make the same mistake\r\n'
                 '# strategy = tf.distribute.MirroredStrategy(\r\n'
                 '#     '
                 'cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\r\n'
                 '\r\n'
                 'strategy = tf.distribute.MirroredStrategy(\r\n'
                 '        '
                 'cross_device_ops=tf.distribute.ReductionToOneDevice("/device:CPU:0"))\r\n'
                 '\r\n'
                 'def train_step_fn(rgb, spec):\r\n' \
                 '            with tf.GradientTape() as tape:\r\n' \
                 '                fake_spec = model(rgb, training=True)\r\n'
                 '                loss = compute_loss(spec, fake_spec)\r\n'
                 '\r\n'
                 '            gradients = tape.gradient(loss, '
                 'model.trainable_variables)\r\n'
                 '            opt.apply_gradients(zip(gradients, '
                 'model.trainable_variables))\r\n' \
                 '            # update metrics\r\n' \
                 '            rmse1.update_state(spec, fake_spec)\r\n'
                 '            rmse2.update_state(spec, fake_spec)\r\n' \
                 '            rrmse1.update_state(spec, fake_spec)\r\n'
                 '            rrmse2.update_state(spec, fake_spec)\r\n'
                 '            sam.update_state(spec, fake_spec)\r\n'
                 '            return loss\r\n'
                 '\r\n'
                 'def test_step_fn(rgb, sepc):\r\n'
                 '            fake_spec = model(rgb, training=False)\r\n'
                 '            loss = loss_object(spec, fake_spec)\r\n' \
                 '            # update metrics\r\n'
                 '            rmse1.update_state(spec, fake_spec)\r\n' \
                 '            rmse2.update_state(spec, fake_spec)\r\n'
                 '            rrmse1.update_state(spec, fake_spec)\r\n'
                 '            rrmse2.update_state(spec, fake_spec)\r\n'
                 '            sam.update_state(spec, fake_spec)\r\n'
                 '            return loss\r\n' \
                 '\r\n'
                 '@tf.function\r\n'
                 'def distributed_train_step(rgb, spec):\r\n' \
                 '            per_replica_losses = '
                 'strategy.experimental_run_v2(train_step_fn,\r\n' \
                 '                                                              '
                 'args=(rgb, spec))\r\n' \
                 '            return strategy.reduce(tf.distribute.ReduceOp.SUM,\r\n'
                 '                                   per_replica_losses,\r\n'
                 '                                   axis=None)\r\n' \
                 '\r\n'
                 '@tf.function\r\n'
                 'def distributed_test_step(rgb, spec):\r\n' \
                 '            return strategy.experimental_run_v2(test_step_fn, ' \
                 'args=(rgb, spec))\r\n'
                 '\r\n' \
                 'for epoch in range(parser.epochs):\r\n'
                 '            # train\r\n'
                 '            for step, (rgb, spec) in enumerate(datas[0]):\r\n'
                 '                train_mean_loss = distributed_train_step(rgb, '
                 'spec)\r\n' \
                 '                steps += 1\r\n'
                 '                ckpt.steps.assign(steps)\r\n'
                 '                if step == 50:\r\n'
                 '                    break\r\n'
                 '            # val\r\n' \
                 '             rmse1.reset_state()\r\n'
                 '             rmse2.reset_state()\r\n' \
                 '             rrmse1.reset_state()\r\n' \
                 '             rrmse2.reset_state()\r\n'
                 '             sam.reset_state()\r\n'
                 '            for step, (rgb, spec) in enumerate(datas[1]):\r\n'
                 '\r\n'
                 '                test_mean_loss = distributed_test_step(rgb, ' \
                 'spec)\r\n'
                 '```\r\n' \
                 '**Other info / logs**\r\n'
                 '```\r\n'
                 'ValueError: in converted code:\r\n'
                 '    mytrainT.py:163 distributed_test_step  *\r\n' \
                 '        return strategy.experimental_run_v2(test_step_fn, '
                 'args=(rgb, spec))\r\n' \
                 '    '
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py:760 ' \
                 'experimental_run_v2\r\n'
                 '        return self._extended.call_for_each_replica(fn, args=args, ' \
                 'kwargs=kwargs)\r\n' \
                 '    mytrainT.py:144 test_step_fn  *\r\n' \
                 '        loss = loss_object(spec, fake_spec)\r\n' \
                 '    ' \
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py:125 '
                 '__call__\r\n'
                 '        with K.name_scope(scope_name or self.__class__.__name__), '
                 'graph_ctx:\r\n' \
                 '    ' \
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\contextlib.py:112 '
                 '__enter__\r\n' \
                 '        return next(self.gen)\r\n'
                 '    ' \
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:435 ' \
                 'graph_context_for_symbolic_tensors\r\n'
                 '        if any(is_symbolic_tensor(v) for v in list(args) + '
                 'list(kwargs.values())):\r\n'
                 '    ' \
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:435 '
                 '<genexpr>\r\n' \
                 '        if any(is_symbolic_tensor(v) for v in list(args) + ' \
                 'list(kwargs.values())):\r\n'
                 '    ' \
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:345 '
                 'is_symbolic_tensor\r\n' \
                 '        return tensor._is_graph_tensor  # pylint: ' \
                 'disable=protected-access\r\n'
                 '    ' \
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\composite_tensor.py:119 '
                 '_is_graph_tensor\r\n'
                 '        components = self._type_spec._to_components(self)  # ' \
                 'pylint: disable=protected-access\r\n'
                 '    '
                 'C:\\Users\\zhangstation\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\distribute\\values.py:500 '
                 '_to_components\r\n' \
                 '        "Flattening a PerReplica to components is not supported in '
                 'replica "\r\n' \
                 '\r\n' \
                 '    ValueError: Flattening a PerReplica to components is not ' \
                 'supported in replica context.\r\n'
                 '```',
         'created_at': '2019-12-2'}, \
        {'body': '**System information**\r\n' \
                 '- TensorFlow version (you are using): 1.15 and >=2.0\r\n'
                 '- Are you willing to contribute it (Yes/No): Yes\r\n'
                 '\r\n' \
                 '**Describe the feature and the current behavior/state.**\r\n' \
                 'Add SO_REUSEPORT option when starting tensorflow training server. ' \
                 'It will enable to scan ports to build TF_CONFIG env variable. It is ' \
                 'necessary to use distributed tensorflow with resource managers that ' \
                 'do not reserve ports (such as Yarn).\r\n'
                 '\r\n' \
                 'It already has been discussed in ticket '
                 'https://github.com/tensorflow/tensorflow/issues/21492. It is '
                 'unclear why the option has been disabled in '
                 'https://github.com/tensorflow/tensorflow/commit/8cf38e81e638db173238a8f95d6ea613c24d3d9f\r\n'
                 '\r\n'
                 '**Will this change the current api? How?**\r\n'
                 'No\r\n'
                 '\r\n' \
                 '**Who will benefit with this feature?**\r\n'
                 '\r\n'
                 'Projects like https://github.com/criteo/tf-yarn (tensorflow on ' \
                 'yarn) will use it to implement the recommended way to create the '
                 'cluster configuration. (from ' \
                 'https://www.tensorflow.org/guide/distributed_training): The ' \
                 'procedure will be: \r\n'
                 '* Launch on all executors a process that will scan ports and ' \
                 'reserve a free one\r\n' \
                 '* A master gathers ports numbers. \r\n'
                 '* Master creates configuration and broadcasts TF_CONFIG variable to ' \
                 'all executors\r\n'
                 '* Launch tensorflow servers\r\n' \
                 '\r\n'
                 '**Any Other info.**\r\n',
         'created_at': '2019-12-2'},
        {'body': 'i converted object detection model into tensorflow lite and i am '
                 'trying to run it in raspberry pi but the performance of tensorflow '
                 'lite is slower in desktop. How can i increase the performance speed '
                 'of the tensorflow lite? ',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n' \
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):Yes\r\n'
                 '- OS Platform and Distribution (Android7.1 Snapdragon 625):\r\n' \
                 '- Mobile device (HUAWEI Nova CAZ-AL 10) if the issue happens on '
                 'mobile device:\r\n'
                 '- TensorFlow installed from (source):\r\n' \
                 '- TensorFlow version ' \
                 '(org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly):\r\n'
                 '\r\n' \
                 '\r\n'
                 '**Describe the current behavior**\r\n' \
                 'I use **yolov3** model on android platform to do object detection. '
                 'When I did object detection on CPU the results are right but when I ' \
                 'add GPU module the results are totally different.\r\n' \
                 '\r\n' \
                 '**Code to reproduce the issue**\r\n' \
                 '\r\n'
                 '\r\n'
                 '/**\r\n'
                 ' * Wrapper for frozen detection models trained using the Tensorflow '
                 'Object Detection API:\r\n' \
                 ' * '
                 'github.com/tensorflow/models/tree/master/research/object_detection\r\n'
                 ' */\r\n' \
                 'public class TFLitePanoObjectDetectionAPIModel implements ' \
                 'Classifier {\r\n'
                 '  private static final Logger LOGGER = new Logger();\r\n'
                 '\r\n' \
                 '  private static final int NUM_of_classes = 19;\r\n' \
                 '  private static final int NUM_THREADS = 4;\r\n'
                 '  private boolean isModelQuantized;\r\n'
                 '  private int inputSize;\r\n' \
                 '  private Vector<String> labels = new Vector<String>();\r\n' \
                 '  private int[] intValues;\r\n' \
                 '  private float[][][][] output_1;\r\n' \
                 '  private float[][][][] output_2;\r\n'
                 '  private float[][][][] output_3;\r\n'
                 '  private ByteBuffer imgData;\r\n' \
                 '  private Interpreter tflite;\r\n'
                 '  private MappedByteBuffer tfliteModel;\r\n' \
                 '  private final Interpreter.Options tfliteOptions = new '
                 'Interpreter.Options();\r\n'
                 '  private GpuDelegate gpuDelegate = null;\r\n'
                 '  private NnApiDelegate nnapiDelegate = null;\r\n'
                 '  private int gridNum;\r\n' \
                 '\r\n'
                 '  private int BoxNum_each_gird=3;\r\n'
                 '  private float[][][][] floatValues;\r\n'
                 '\r\n'
                 '\r\n' \
                 '  private TFLitePanoObjectDetectionAPIModel() {}\r\n'
                 '  private float scoreThreshold = 0.3f;\r\n' \
                 '  private int blockSize=32;\r\n'
                 '\r\n'
                 '  public static Classifier create(\r\n'
                 '      final AssetManager assetManager,\r\n' \
                 '      final String modelFilename,\r\n' \
                 '      final String labelFilename,\r\n' \
                 '      final int inputSize,\r\n'
                 '      final boolean isQuantized)\r\n'
                 '      throws IOException {\r\n'
                 '    final TFLitePanoObjectDetectionAPIModel d = new ' \
                 'TFLitePanoObjectDetectionAPIModel();\r\n'
                 '    try {\r\n' \
                 '\r\n' \
                 '      d.tfliteModel=loadModelFile(assetManager, modelFilename);\r\n'
                 '      if (d.gpuDelegate == null)\r\n' \
                 '      {\r\n' \
                 '        d.gpuDelegate = new GpuDelegate();\r\n' \
                 '        d.tfliteOptions.addDelegate(d.gpuDelegate);\r\n'
                 '      }\r\n'
                 '      d.tflite = new Interpreter(d.tfliteModel,d.tfliteOptions);\r\n'
                 '    } catch (Exception e) {\r\n' \
                 '      throw new RuntimeException(e);\r\n'
                 '    }\r\n'
                 '\r\n'
                 '    d.isModelQuantized = isQuantized;\r\n'
                 '    // Pre-allocate buffers.\r\n'
                 '    int numBytesPerChannel;\r\n' \
                 '    if (isQuantized) {\r\n'
                 '      numBytesPerChannel = 1; // Quantized\r\n'
                 '    } else {\r\n' \
                 '      numBytesPerChannel = 4; // Floating point\r\n' \
                 '    }\r\n'
                 '    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * ' \
                 '2*d.inputSize * 3 * numBytesPerChannel);//\r\n'
                 '    d.imgData.order(ByteOrder.nativeOrder());\r\n' \
                 '    d.intValues = new int[d.inputSize * 2*d.inputSize];\r\n'
                 '\r\n'
                 '    d.gridNum=d.inputSize/32;\r\n'
                 '    d.floatValues=new float[1][d.inputSize][2 * d.inputSize ' \
                 '][3];\r\n'
                 '    d.output_1 = new float[1][][][];\r\n'
                 '    d.output_2 = new float[1][][][];\r\n'
                 '    d.output_3 = new float[1][][][];\r\n'
                 '\r\n'
                 '\r\n'
                 '    return d;\r\n'
                 '  }\r\n'
                 '  }\r\n' \
                 '\r\n'
                 '  @Override\r\n' \
                 '  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n'
                 '\r\n' \
                 '    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, '
                 'bitmap.getWidth(), bitmap.getHeight());\r\n'
                 '    imgData.rewind();\r\n'
                 '\r\n'
                 '    for (int i = 0; i < inputSize; ++i)\r\n' \
                 '    {\r\n'
                 '      for (int j = 0; j < 2*inputSize; ++j)\r\n'
                 '      {\r\n'
                 '        int pixelValue = intValues[i * 2*inputSize + j];\r\n'
                 '          // Float model\r\n' \
                 '          floatValues[0][i][j][0]=((pixelValue >> 16) & '
                 '0xFF)/255.0f ;\r\n'
                 '          floatValues[0][i][j][1]=((pixelValue >> 8) & 0xFF)/255.0f '
                 ';\r\n'
                 '          floatValues[0][i][j][2]=(pixelValue& 0xFF) /255.0f;\r\n'
                 '      }\r\n'
                 '    }\r\n' \
                 '\r\n' \
                 '     int channelNum=BoxNum_each_gird*(NUM_of_classes+5);\r\n'
                 '\r\n'
                 '    output_1 = new float[1][gridNum][2*gridNum][channelNum];\r\n' \
                 '    output_2 = new float[1][2*gridNum][4*gridNum][channelNum];\r\n'
                 '    output_3 = new float[1][4*gridNum][8*gridNum][channelNum];\r\n'
                 '\r\n' \
                 '    Object[] inputArray = {floatValues};\r\n'
                 '    Map<Integer, Object> outputMap = new HashMap<>();\r\n'
                 '    outputMap.put(0, output_1);\r\n' \
                 '    outputMap.put(1, output_2);\r\n' \
                 '    outputMap.put(2, output_3);\r\n' \
                 '    Trace.endSection();\r\n'
                 '\r\n' \
                 '    // Run the inference call.\r\n'
                 '    Trace.beginSection("run");\r\n' \
                 '    long startTime = SystemClock.uptimeMillis();\r\n' \
                 '    tflite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n'
                 '    long lastingTime=SystemClock.uptimeMillis()-startTime;\r\n'
                 '\r\n'
                 '    LOGGER.i("runForMultipleInputsOutputs time of each image: " + '
                 'lastingTime + "ms");\r\n' \
                 '    Trace.endSection(); \r\n' \
                 '    }\r\n'
                 '\r\n'
                 '}`\r\n' \
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary '
                 'to generate the problem.\r\n' \
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n', \
         'created_at': '2019-12-2'}, \
        {'body': 'I am using tensorflow 2.1.0rc1-2 on Arch Linux.\r\n'
                 '\r\n'
                 'A Python project I work with contains some machinery to detect if ' \
                 'the same .py file is imported under multiple different names, '
                 'resulting in different module objects. This [can lead to subtle '
                 'breakage](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-double-import-trap) '
                 'as classes defined in the two copies of the module are officially ' \
                 'different classes, so instances of one of them return False when '
                 'using `isinstance()` with the class from the other copy of the '
                 "module. Basically, it's a bad idea, so the project I work with " \
                 "raises a big error to tell us when we've accidentally done it.\r\n" \
                 '\r\n' \
                 'Tensorflow has a large number of modules imported twice under ' \
                 'different names, which causes my project to raise an error about ' \
                 'it. Of course I can just whitelist tensorflow in my project, but I '
                 'think this is a bad idea for tensorflow to do. If it is going to ' \
                 'have the same module under multiple names, the import machinery '
                 'hacks in tensorflow should ensure the multiple names at least point ' \
                 'to the same module object, rather than there being multiple module ' \
                 'objects in the interpreter object corresponding to the same .py ' \
                 'file.\r\n' \
                 '\r\n'
                 'The issue can be demonstrated with the following:\r\n' \
                 '\r\n' \
                 '```python\r\n'
                 'import sys\r\n'
                 'import tensorflow\r\n' \
                 '\r\n'
                 'for name1, module1 in sys.modules.copy().items():\r\n'
                 '    for name2, module2 in sys.modules.copy().items():\r\n' \
                 "        file1 = getattr(module1, '__file__', None)\r\n"
                 "        file2 = getattr(module2, '__file__', None)\r\n"
                 '        if file1 is not None and file2 is not None:\r\n'
                 '            if file1 == file2 and name1 != name2 and module1 is not '
                 'module2:\r\n'
                 '                print(name1, name2)\r\n' \
                 '```\r\n'
                 'This prints\r\n'
                 '```\r\n'
                 'tensorflow._api tensorflow_core._api\r\n' \
                 'tensorflow.python tensorflow_core.python\r\n' \
                 'tensorflow.tools tensorflow_core.tools\r\n'
                 'tensorflow.core tensorflow_core.core\r\n'
                 'tensorflow.compiler tensorflow_core.compiler\r\n'
                 'tensorflow.lite tensorflow_core.lite\r\n'
                 'tensorflow.keras tensorflow.python.keras.api._v2.keras\r\n'
                 'tensorflow.keras tensorflow_core.python.keras.api._v2.keras\r\n' \
                 'tensorflow.keras tensorflow_core.keras\r\n' \
                 'tensorflow.compat tensorflow._api.v2.compat\r\n'
                 'tensorflow.compat tensorflow_core._api.v2.compat\r\n'
                 '\r\n'
                 '<snip>\r\n'
                 '```\r\n'
                 '\r\n'
                 'Basically there are copies of many modules imported both as ' \
                 '`tensorflow.<blah>` and `tensorflow_core.<blah>`.\r\n' \
                 '\r\n'
                 'Tensorflow should consider changing its import machinery to avoid '
                 'this, for the reasons outlined in the above linked post on the ' \
                 'double import gotcha in Python.',
         'created_at': '2019-12-2'},
        {'body': 'Add support for the SparkFun Edge board in the Arduino library '
                 'build. Allows individuals who are less comfortable with command '
                 'line tools to get started with TFLu examples. \r\n'
                 '\r\n'
                 '**dependencies**\r\n'
                 '[Update TFLu (micro) to use AmbiqSuite SDK Release 2.2.0 for '
                 'Apollo3](https://github.com/tensorflow/tensorflow/pull/35236)\r\n'
                 '[Upgrade Edge Board Support Package in TFLu '
                 '(micro)](https://github.com/tensorflow/tensorflow/pull/35290)',
         'created_at': '2019-12-2'},
        {'body': 'I noticed a high demand in training big NNs/batches is to create a '
                 'multistep optimizer. This means the optimizer accumulates gradients '
                 'from batches until it reaches a desire batch and update model '
                 'parameters. \r\n'
                 '\r\n'
                 'This is similar to ' \
                 '[this](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/multistep_optimizer.py), '
                 'but for file '
                 '[keras.optimizer_v2](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py) '
                 'instead.\r\n' \
                 '\r\n'
                 'I can contribute to this. However, I noticed by doing this, the '
                 'code optimizer_v2.py will need to change a lot. I can create a new ' \
                 'similar python file (say, multistep_optimizer_v2.py) to avoid '
                 'changing in optimizer_v2.py, but the optimizer_v2.py code it self '
                 'is indeed very long.\r\n'
                 '\r\n'
                 'I thus create a discussion here to see whether you (Tensorflow Team '
                 'and community) feel it is worthy to do so, as well as how to do '
                 'that, or any better solution for doing that, if there is.\r\n'
                 '\r\n'
                 'Best,',
         'created_at': '2019-12-2'},
        {'body': 'when saved the model as tf by api  '
                 '**tf.keras.models.save_model(testmodel, '
                 '"./1/",save_format=\'tf\')**\r\n' \
                 "then,I load it by tf.keras.models.load_model('1'),however，I got the "
                 'issue as title.\r\n'
                 'It is worth mentioning that it succed if I saved as .h5\r\n' \
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu '
                 '16.04):centos7.4\r\n'
                 '- TensorFlow version (use command below):tensorflow 2.0\r\n'
                 '- Python version:3.7\r\n'
                 '-NVIDIA-SMI 418.88\r\n'
                 '-Driver Version: 418.88\r\n'
                 '-CUDA Version: 10.1 \r\n'
                 '- GPU model and memory:12\r\n'
                 '\r\n',
         'created_at': '2019-12-2'}, \
        {'body': '**System information**\r\n'
                 '* Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): yes\r\n'
                 '* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '18.04.3 LTS\r\n'
                 '* TensorFlow installed from (source or binary): binary\r\n'
                 '* TensorFlow version (use command below): v2.1.0-rc0-47-g064e153 ' \
                 '2.1.0-rc1 (`python3 -c "import tensorflow as tf; ' \
                 'print(tf.version.GIT_VERSION, tf.version.VERSION)"`)\r\n' \
                 '* Python version: Python 3.6.8\r\n' \
                 '* CUDA/cuDNN version: Driver Version: 440.33.01, CUDA Version: '
                 '10.2, cuDNN 7.6.2\r\n' \
                 '* GPU model and memory: Tesla V100-SXM2-16GB\r\n' \
                 '\r\n' \
                 '**Describe the current behavior**\r\n'
                 'Compiling autograph function is 4-5x slower in Distributed Mirrored '
                 'Strategy:\r\n'
                 '* Single-GPU Distributed Mirrored Strategy: under 5 minutes\r\n' \
                 '* Multi-GPU Distributed Mirrored Strategy: about 30 minutes\r\n'
                 'The tensorflow code is identical in both setups.\r\n' \
                 '\r\n' \
                 '**Describe the expected behavior**\r\n' \
                 'Autograph compilation should take roughly the same time in single '
                 'and multi-GPU Distributed Mode with Mirrored Strategy.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Training loop (hierarchical VAE in the current configuration):\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1151\r\n'
                 '\r\n'
                 'The code is adapted from TF1.x repository:\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/8866a0b1457cbd4be5d6f549f9bf4075d49b2486/SPADE.py#L1045\r\n' \
                 'and is compiled using TF2.x @tf.function annotation.\r\n' \
                 '\r\n'
                 'It uses a dry-run of the model to pre-create variables using '
                 'tf.compat.v1.variable_scope(scope, ' \
                 'reuse=tf.compat.v1.AUTO_REUSE):\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1174\r\n'
                 '(it takes about 10 minutes to compile and run it in multi-gpu ' \
                 'mode)\r\n' \
                 '\r\n' \
                 'Then it runs the actual training step(s)\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1257\r\n'
                 '(it takes about 20 minutes to compile and run it for the first time '
                 'in multi-gpu mode)\r\n' \
                 '\r\n' \
                 "It looks that just disabling lines with 'optim.apply_gradients'\r\n"
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1229\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1231\r\n'
                 'https://github.com/olegmyrk/SPADE-Tensorflow/blob/4681ac60f2078144d55178f23fa885d3ed191450/SPADE.py#L1233\r\n'
                 'slashes about 10 out 20 minutes needed for initial run in multi-gpu '
                 'mode. Which is essentially compiling back-propagation?\r\n'
                 '\r\n'
                 'After the first training step that takes 20 minutes the following ' \
                 'training steps run at normal speed.\r\n'
                 '\r\n' \
                 'The total number of mirrored parameters is around 500MB.\r\n'
                 '\r\n'
                 'With 4 V100 GPUs training step is around 5x slower than with single '
                 'V100 GPU.\r\n'
                 '\r\n'
                 'Command:\r\n'
                 'NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=ALL nohup python3 main.py '
                 '--dataset CelebAMask-HQ --img_height 256 --img_width 256 --ch 16 '
                 '--img_ch 3 --phase train --save_freq 10000 --batch_size 12 '
                 '--gan_type hinge --code_gan_type gan --n_critic 1 '
                 '--code_num_layers=4 --code_dist_num_layers=0 --augment_flag false '
                 '--sn=False --train_main=true --train_nondet=true --lr 0.0002 '
                 '--epoch=50 --decay_epoch=25 --print_freq 100 &> '
                 'train.CelebAMask-HQ.log&\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '[train.CelebAMask-HQ.slow_compile.log](https://github.com/tensorflow/tensorflow/files/3993026/train.CelebAMask-HQ.slow_compile.log)\r\n',
         'created_at': '2019-12-2'},
        {'body': '### System information\r\n'
                 '- **Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow)**: YES\r\n' \
                 '- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ' \
                 'macOS 10.15.2 (19C57)\r\n' \
                 '- **TensorFlow installed from (source or binary)**: binary\r\n'
                 '- **TensorFlow version (use command '
                 'below)**:v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n'
                 '- **Python version**:3.7.5 (default, Oct 25 2019, 18:18:54) '
                 '\\n[Clang 11.0.0 (clang-1100.0.33.8)]\r\n' \
                 '\r\n'
                 '### Describe the problem\r\n'
                 '\r\n'
                 '`tf.data.Dataset` supports RaggedTensor and SparseTensor but '
                 '`tf.data.Dataset.from_generator` is limited to Tensors only. Please '
                 'support other types of Tensors. \r\n'
                 '\r\n'
                 '### Source code / logs\r\n'
                 '\r\n'
                 '```\r\n'
                 '    def data_get():\r\n'
                 '        for i in range(5):\r\n'
                 '            yield tf.ragged.constant([[i, i], [i]])\r\n'
                 '\r\n'
                 '    ds = tf.data.Dataset.from_generator(data_get, tf.int32)\r\n'
                 '\r\n'
                 '    for sample in ds:\r\n'
                 '        print(sample)\r\n' \
                 '```\r\n'
                 '\r\n'
                 'produces:\r\n'
                 '\r\n'
                 'Traceback (most recent call last):\r\n'
                 '\r\n'
                 '```\r\n'
                 '  File ' \
                 '"/Users/peak/IdeaProjects/TFmodels/venv-tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py", ' \
                 'line 221, in __call__\r\n'
                 '    ret = func(*args)\r\n'
                 '\r\n'
                 '  File '
                 '"/Users/peak/IdeaProjects/TFmodels/venv-tf2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py", ' \
                 'line 606, in generator_py_func\r\n'
                 '    "element was %s." % (dtype.name, ret))\r\n' \
                 '\r\n' \
                 'TypeError: `generator` yielded an element that could not be '
                 'converted to the expected type. The expected type was int32, but '
                 'the yielded element was <tf.RaggedTensor [[0, 0], [0]]>.\r\n'
                 '```\r\n'
                 '\r\n'
                 '\r\n',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example ' \
                 'script provided in TensorFlow): No\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android '
                 'version:9, Ubuntu 16.04\r\n' \
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the ' \
                 'issue happens on mobile device: OnePlus3, GPU:Adreno 530\r\n'
                 '- TensorFlow installed from (source or binary): tensorflow-lite-gpu '
                 '1.15, tensorflow-lite-gpu:0.0.0-nightly from ' \
                 "'https://mvnrepository.com'\r\n"
                 '- TensorFlow version (use command below): 1.14, 1.15\r\n'
                 '\r\n' \
                 '\r\n'
                 '**Describe the current behavior**\r\n' \
                 "I'm using tflite-gpu in my android application for semantic "
                 "segmentation. Using my tflite model(all gpu supported ops) i'am "
                 'able to get proper output with CPU version and tensorflow-lite-gpu ' \
                 '1.14 ;but when i use nightly or 1.15 it loads up opencl backend and '
                 'gives corrupted output. This backend seems to take longer time to ' \
                 'start up(5-10s); however it seems to be faster than corresponding ' \
                 'opengl version. When i run the model to get a image output '
                 '(float:0-1) there seems to be random rectangular blanks within the '
                 'output. The input is 256x256x3(float) and output os '
                 "256x256x1(float). However.  i 'am not facing this issue using a " \
                 'different model with input size 128; even though i use the same '
                 'back-end.\r\n' \
                 '**Describe the expected behavior**\r\n' \
                 'The tflite model should produce correct output with opencl backend '
                 'like the  opengl version, regardless of input size.\r\n'
                 '\r\n' \
                 '**Other info / logs**\r\n'
                 "I'am getting correct output for model with 128 input size "
                 'regardless of backends and devices; but for the model with 256 '
                 "output size i'am not getting proper output with opencl backend "
                 '(gpu-nightly and gpu-1.15)\r\n'
                 '[Models.zip](https://github.com/tensorflow/tensorflow/files/3992313/Models.zip)\r\n'
                 '\r\n'
                 "Only the '**opencl gpu delegate with this 256 input sized-model**' " \
                 'produces this corrupted output; other versions (CPU, Opengl-GPU, ' \
                 '128 input-model with Opencl etc.) seems to produce correct result '
                 'without the rectangular blanks.\r\n'
                 '![tmap54](https://user-images.githubusercontent.com/1130185/71320296-57515680-24cf-11ea-8c26-66f62c948e94.PNG)\r\n',
         'created_at': '2019-12-2'}, \
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub ' \
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests ' \
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example ' \
                 'script provided in TensorFlow): yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '7\r\n' \
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n' \
                 '- TensorFlow installed from (source or binary): \r\n' \
                 '- TensorFlow version (use command below): 2.0.0\r\n'
                 '- Python version: 3.7\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10.1\r\n'
                 '- GPU model and memory: \r\n' \
                 '\r\n'
                 'You can collect some of this information using our environment ' \
                 'capture\r\n'
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n' \
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python ' \
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n' \
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, ' \
                 'tf.version.VERSION)"`\r\n' \
                 '\r\n' \
                 'Hi,\r\n' \
                 '\r\n' \
                 "I tried to follow tf's doc to build up a model as follows:\r\n" \
                 'https://www.tensorflow.org/guide/keras/custom_layers_and_models\r\n'
                 '\r\n'
                 '`\r\n'
                 '\t\t\t\t\t\r\n'
                 '\tclass CBR(layers.Layer):\r\n'
                 "\t\t'''\r\n"
                 '\t\tConvolution + Batch normalisation + Relu\r\n'
                 "\t\t'''\r\n"
                 '\t\tdef __int__(self, filterNum, kSize, strSize, padMode, '
                 "name='cbr', **kwargs):\r\n" \
                 '\t\t\tsuper(CBR, self).__init__(name=name, **kwargs)\r\n'
                 '\t\t\tself.conv3D = layers.Conv3D(filters=filterNum, '
                 'kernel_size=kSize, strides=strSize, padding=padMode, '
                 "data_format='channels_first')\r\n" \
                 '\t\t\tself.BN = layers.BatchNormalization(axis=1)\r\n'
                 '\t\tdef call(self, inputs):\r\n'
                 '\t\t\tx = self.conv3D(inputs)\r\n'
                 '\t\t\tx=self.BN(x)\r\n' \
                 '\t\t\treturn layers.Relu(x)\r\n' \
                 '\t\t\t\r\n' \
                 '\tclass SimpleUNet(tf.keras.Model, layers.Layer):\r\n' \
                 "\t\t'''\r\n" \
                 '\t\tSerialise basic units so as to build up a double-layered '
                 'encoder-decoder U-Net\r\n' \
                 '\t\tInput:\r\n' \
                 '\t\t\tinDim: [mbSize, modaility/channel, tensor dimensions]\r\n'
                 '\t\t\tclassNum: background included\r\n' \
                 '\t\t\tname: name for the net\r\n' \
                 '\t\t\tinputs: 5D tf tensor of [mbSize, modaility/channel, tensor '
                 'dimensions]. Inputs must be organised into channel first\r\n'
                 '\t\tReturns:\r\n' \
                 '\t\t\toutputs: 5D tf tensor of [mbSize, classNum, tensor ' \
                 'dimensions]\r\n' \
                 "\t\t'''\r\n" \
                 "\t\tdef __init__(self, inDim, classNum, name='SimpleUNet', "
                 '**kwargs):\r\n'
                 '\t\t\tsuper(SimpleUNet, self).__init__(name=name, **kwargs)\r\n'
                 '\t\t\tself.inDim = inDim\r\n'
                 '\t\t\tself.classNum = classNum\r\n'
                 '\t\t\tdimEnSt1End = np.array(inDim[1:])-2-2\r\n'
                 '\t\t\tdimEnSt2Ed = dimEnSt1End/2-2-2\r\n'
                 '\t\t\tdimBridgeEnd = (dimEnSt2Ed/2-2-2)*2\r\n'
                 '\t\t\tdimDEStd1End = (dimBridgeEnd-2-2)*2\r\n'
                 '\t\t\toutDim = dimDEStd1End-2-2-2\r\n'
                 "\t\t\ttemp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')\r\n"
                 '\t\t\tcrop3d1 = tuple(np.tile(temp, (2, 1)).T)\r\n' \
                 "\t\t\ttemp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')\r\n"
                 '\t\t\tcrop3d2 = tuple(np.tile(temp, (2, 1)).T)\r\n' \
                 '\t\t\t# list of basic units used in the model\r\n'
                 "\t\t\tself.en_st1_cbr1 = CBR(32, 3, 1, 'valid')\r\n" \
                 "\t\t\tself.en_st1_cbr2 = CBR(64, 3, 1, 'valid')\r\n"
                 '\t\t\tself.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), '
                 "strides=(2, 2, 2), padding='valid', "
                 "data_format='channels_first')\r\n"
                 "\t\t\tself.en_st2_cbr1 = CBR(64, 3, 1, 'valid')\r\n"
                 "\t\t\tself.en_st2_cbr2 = CBR(128, 3, 1, 'valid')\r\n" \
                 '\t\t\tself.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), ' \
                 "strides=(2, 2, 2), padding='valid', " \
                 "data_format='channels_first')\r\n"
                 "\t\t\tself.bridge_cbr1 = CBR(128, 3, 1, 'valid')\r\n"
                 "\t\t\tself.bridge_cbr2 = CBR(256, 3, 1, 'valid')    \r\n"
                 '\t\t\tself.bridge_tconv1 = layers.Conv3DTranspose(256, 2, ' \
                 "strides=2, padding='valid', data_format='channels_first')\r\n"
                 '\t\t\tself.de_3dcrop1 = layers.Cropping3D(crop3d1, '
                 "data_format='channels_first')\r\n"
                 "\t\t\tself.de_st1_cbr1 = CBR(256, 3, 1, 'valid')\r\n"
                 "\t\t\tself.de_st1_cbr2 = CBR(128, 3, 1, 'valid')    \r\n" \
                 '\t\t\tself.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, '
                 "strides=2, padding='valid', data_format='channels_first')\r\n" \
                 '\t\t\tself.de_3dcrop2 = layers.Cropping3D(crop3d2, ' \
                 "data_format='channels_first')\r\n" \
                 "\t\t\tself.de_st2_cbr1 = CBR(64, 3, 1, 'valid')\r\n" \
                 "\t\t\tself.de_st2_cbr2 = CBR(64, 3, 1, 'valid') \r\n" \
                 '\t\t\tself.final_conv3D = layers.Conv3D(filters=self.classNum, ' \
                 "kernel_size=3, strides=1, padding='valid', " \
                 "data_format='channels_first')\r\n" \
                 '\t\t\t\t\t\r\n'
                 '\t\tdef call(self, inputs):\r\n' \
                 '\t\t\tx = self.en_st1_cbr1(inputs)\r\n'
                 '\t\t\txEnSt1End = self.en_st1_cbr2(x)\r\n'
                 '\t\t\tx= self.en_st2_mp(xEnSt1End)\r\n' \
                 '\t\t\tx= self.en_st2_cbr1(x)\r\n'
                 '\t\t\txEnSt2Ed = self.en_st2_cbr2(x)\r\n'
                 '\t\t\tx = self.bridge_mp(xEnSt2Ed)\r\n' \
                 '\t\t\tx = self.bridge_cbr1(x)\r\n'
                 '\t\t\tx = self.bridge_cbr2(x)\r\n' \
                 '\t\t\txBridgeEnd = self.bridge_tconv1(x)\r\n'
                 '\t\t\txCrop1 = self.de_3dcrop1(xEnSt2Ed)\r\n'
                 '\t\t\tx = layers.Concatenate([xBridgeEnd, xCrop1], axis=1)\r\n'
                 '\t\t\tx = self.de_st1_cbr1(x)\r\n'
                 '\t\t\tx = self.de_st1_cbr2(x)\r\n'
                 '\t\t\txDeSt1End = self.de_st1_tconv1(x)\r\n' \
                 '\t\t\txCrop2 = self.de_3dcrop2(xEnSt1End)\r\n'
                 '\t\t\tx = layers.Concatenate([xDeSt1End, xCrop2], axis=1)\r\n'
                 '\t\t\tx = self.de_st2_cbr1(x)\r\n'
                 '\t\t\tx = self.de_st2_cbr2(x)\r\n'
                 '\t\t\tx = self.final_conv3D(x)\r\n'
                 '\t\t\toutputs = activations.softmax(x, axis=1)\r\n'
                 '\t\t\t\r\n'
                 '\t\t\treturn outputs\r\n' \
                 '\r\n' \
                 '`\r\n'
                 '\r\n'
                 'Then I initialised it, and tried to build it by calling '
                 'SUNet.build\r\n'
                 '`\r\n' \
                 '\tclassNum = 3 \r\n'
                 '\tmbSize = 16 \r\n' \
                 '\tinDim = [4, 64, 64, 64] \r\n'
                 '\tSUNet = SimpleUNet(inDim, classNum) \r\n'
                 '\tSUNet.build(input_shape=inDim)\r\n' \
                 '`\r\n'
                 "I strictly followed the example given in tf's doc, but an error was " \
                 'raised when building up it\r\n'
                 'ValueError: name for name_scope must be a string.\r\n' \
                 'It occurred here when CBR is called for the first time:\r\n' \
                 '\r\n'
                 '`\r\n'
                 "def __int__(self, filterNum, kSize, strSize, padMode, name='cbr', "
                 '**kwargs): \r\n'
                 '\tsuper(CBR, self).__init__(name=name, **kwargs)\r\n'
                 '`\r\n'
                 '\r\n'
                 'I cannot figure out any syntactic mistake. Could anyone give me a '
                 'hand? Thanks a lot.\r\n'
                 'Or, is the model cannot be built at this moment until it is '
                 'actually used when being called in the training?',
         'created_at': '2019-12-2'}, \
        {'body': 'Added usage examples for tf.math (subtract, scalar_mul, truediv, '
                 'divide_no_nan, multiply_no_nan, floordiv, reduce_prod, reduce_min, '
                 'reduce_max, sigmoid, log_sigmoid, unsorted_segmentation, '
                 'unsorted_segment_sqrt_n, polyval)',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code: Yes\r\n'
                 '- OS Platform and Distribution: 4.4.0-18362-Microsoft\r\n'
                 '- TensorFlow installed from: Anaconda default source\r\n' \
                 '- TensorFlow version: 1.15\r\n'
                 '- Python version: 3.7.5\r\n'
                 '- CUDA/cuDNN version: 10.0\r\n'
                 '- GPU model and memory: GeForce RTX 2060\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 "I'm trying to implement a custom loss function based on a custom "
                 "accuracy function that I'm already using to evaluate my model "
                 "predictions on the test dataset. The conversion can't be 1:1 "
                 'because I use numpy "greater" and "equal" functions that are not '
                 'differentiable. I created thefore custom functions that approximate '
                 'the latters but their behavior has some problems\r\n' \
                 '\r\n' \
                 '**Describe the expected behavior**\r\n' \
                 '\r\n'
                 'In particular I can test if everything is fine by comparing the '
                 'results obtained by my original custom accuracy f. and the new loss '
                 'f. given the same input (my input are tensorflow predictions, I '
                 'just inglobe them inside K.constant to convert them in tensors). '
                 'What I noticed is that this line of code\r\n'
                 '\r\n' \
                 '```\r\n'
                 'eps = sys.float_info.epsilon\r\n'
                 'return 0.5*(y + 5 + K.sqrt(K.pow(y-5,2) + eps))\r\n' \
                 '```\r\n' \
                 '\r\n'
                 'is problematic. In particular y is an array of float32 values in '
                 "[1:10] range and the returned array, let's call it 'ret', should " \
                 'have ret[i]=max(5,y[i]) but sometimes the value of **5** becomes '
                 '**4.9999995** instead. The next portion of my code is based on how '
                 "many **5** are present and therefore I can't ignore this " \
                 'problem.\r\n'
                 '\r\n' \
                 "The fact is that, let's say a problematic index is 'w' so that " \
                 'ret[w]=4.9999995 instead of 5, if I use the same code with y now '
                 'equal to only y[w] the returned array is correctly 5. This means '
                 'that somehow if y is a batch of predictions and not just one '
                 "something isn't working. This should not be the case because both "
                 'K.sqrt and K.pow works element wise, it should not matter if y is '
                 'an array of 1 or multiple values \r\n'
                 '\r\n'
                 'Out of almost 20k predictions, around 1k have this same problem and ' \
                 'it is deterministic (always the same are problematic). I also tried '
                 'to use:\r\n'
                 '\r\n' \
                 '```\r\n'
                 'eps = sys.float_info.epsilon\r\n' \
                 'return 0.5*(y + 5 + np.sqrt(np.pow(y-5,2) + eps))\r\n'
                 '```\r\n'
                 '\r\n'
                 'and the problem is gone thefore it is related to Keras backend.\r\n'
                 '\r\n'
                 'Last info, I tried to use also:\r\n'
                 '\r\n'
                 '```\r\n'
                 'eps = sys.float_info.epsilon\r\n' \
                 'return tf.math.ceil(0.5*(y + 5 + K.sqrt(K.pow(y-5,2) + eps)))\r\n'
                 '```\r\n' \
                 '\r\n' \
                 'but this completely ruins the returned value, sometimes real ' \
                 'numbers such as 4.5 are rounded to 6 instead of 5\r\n'
                 '\r\n'
                 'If more informations are needed I can provide them',
         'created_at': '2019-12-2'},
        {'body': 'Thank you for submitting a TensorFlow documentation issue. Per our ' \
                 'GitHub\r\n'
                 'policy, we only address code/doc bugs, performance issues, feature '
                 'requests, and\r\n' \
                 'build/installation issues on GitHub.\r\n' \
                 '\r\n'
                 'The TensorFlow docs are open source! To get involved, read the '
                 'documentation\r\n'
                 'contributor guide: ' \
                 'https://www.tensorflow.org/community/contribute/docs\r\n'
                 '\r\n'
                 '## URL(s) with the issue:\r\n'
                 '\r\n' \
                 'Please provide a link to the documentation entry, for example:\r\n'
                 'https://www.tensorflow.org/api_docs/python/tf/keras/losses/Reduction?version=stable\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 '\r\n'
                 'I intend to build up a custom loss function as follows:\r\n'
                 '\r\n'
                 '\r\n'
                 '`\tfrom __future__ import absolute_import, division, ' \
                 'print_function, unicode_literals\r\n'
                 '\timport functools\r\n'
                 '\r\n'
                 '\timport numpy as np\r\n'
                 '\timport tensorflow as tf\r\n'
                 '\r\n'
                 '\r\n' \
                 '\tclass GeneralDiceLoss(tf.keras.losses.Loss):\r\n'
                 '\t\tdef __init__(self, reduction=tf.keras.losses.Reduction.AUTO, '
                 "name='GeneralDiceLoss'):\r\n"
                 '\t\t\tsuper().__init__(reduction=reduction, name=name)\r\n'
                 '\t\t\tself.epsilon = 1e-16 \r\n' \
                 '\t\t\r\n'
                 '\t\t\r\n'
                 '\t\tdef get_config(self):\r\n'
                 '\t\t\tconfig = super(GeneralDiceLoss, self).get_config()\r\n' \
                 '\t\t\treturn config\r\n'
                 '\t\t\r\n'
                 '\t\tdef call(self, yPred, yTrue):\r\n'
                 '\t\t\t#yTrue =tf.dtypes.cast(yTrue, dtype=yPred.dtype)\r\n'
                 '\t\t\t# Dot product yPred and yTrue and sum them up for each datum '
                 'and class\r\n'
                 '\t\t\tcrossProd=tf.multiply(yPred, yTrue)\r\n'
                 '\t\t\tcrossProdSum=tf.math.reduce_sum(crossProd, axis=np.arange(2, ' \
                 'yTrue.ndim))\r\n'
                 '\t\t\t# Calculate weight for each datum and class \r\n' \
                 '\t\t\tweight = tf.math.reduce_sum(yTrue, axis=np.arange(2, '
                 'yTrue.ndim))\r\n'
                 '\t\t\tweight = tf.math.divide(1, '
                 'tf.math.square(weight)+self.epsilon)\r\n'
                 '\t\t\t# Weighted sum over classes\r\n'
                 '\t\t\tnumerator = 2*tf.math.reduce_sum(tf.multiply(crossProdSum, '
                 'weight), axis=1)\r\n'
                 '\t\t\t# Saquared summation \r\n'
                 '\t\t\tyySum = tf.math.reduce_sum(tf.math.square(yPred) + ' \
                 'tf.math.square(yTrue), axis=np.arange(2, yTrue.ndim))\r\n'
                 '\t\t\t# Weighted sum over classes\r\n'
                 '\t\t\tdenominator = tf.math.reduce_sum(tf.multiply(weight, yySum), '
                 'axis=1)\r\n'
                 '\t\t\tloss = 1 - tf.math.divide(numerator, ' \
                 'denominator+self.epsilon)\r\n' \
                 '\t\t\t#loss = tf.math.reduce_mean(1 - tf.math.divide(numerator, '
                 'denominator+self.epsilon))\r\n'
                 '\t\t\t\r\n'
                 '\t\t\treturn loss\r\n' \
                 '`\r\n'
                 '\r\n' \
                 'Then I create variables to have it test\r\n'
                 '`\r\n'
                 '\r\n'
                 '\tGeneralDiceLoss()\r\n'
                 '\tyPred = tf.random.uniform(shape=(16, 3, 4, 4, 4))\r\n'
                 '\tyTrue = tf.round(tf.random.uniform(shape=(16, 3, 4, 4, 4)))\r\n'
                 '\r\n'
                 '\tloss=GeneralDiceLoss(yPred, yTrue)\r\n'
                 '`\r\n'
                 'But I got an error\r\n'
                 '`\r\n'
                 '\r\n' \
                 '\t  File '
                 '"...\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py", '
                 'line 96, in convert_to_eager_tensor\r\n' \
                 '\t\treturn ops.EagerTensor(value, ctx.device_name, dtype)\r\n'
                 '\r\n'
                 "\tTypeError: Cannot convert 'auto' to EagerTensor of dtype float\r\n"
                 '`\r\n'
                 '\r\n'
                 'In the doc above, \r\n' \
                 '1) there is NO clear indication or warning about conversion issue, '
                 'not to mention there is NO dtype conversion in my code at all. \r\n' \
                 '2) there is NO clear example indicating which option, AUTO or '
                 "SUM_OVER_BATCH_SIZE, should be adopted in one's minbatch size is "
                 'greater than 1. In my case, assume my batch is 16 as exhibted in ' \
                 'yPred and yTrue above, shall I use\r\n'
                 '\r\n'
                 '`\r\n'
                 '\t\t\tloss = 1 - tf.math.divide(numerator, '
                 'denominator+self.epsilon)\r\n'
                 '`\r\n'
                 'or \r\n'
                 '`\r\n'
                 '\t\t\tloss = tf.math.reduce_mean(1 - tf.math.divide(numerator, '
                 'denominator+self.epsilon))\r\n'
                 '`\r\n' \
                 'And for which option?\r\n' \
                 '\r\n'
                 'Building up a custom layer/loss function is already a tough task '
                 'for many practitioners, so could the doc provide more detailed '
                 "explanations and examples so as to make users' life a little bit " \
                 'easier? Many thanks.',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows '
                 '10\r\n'
                 '- TensorFlow installed from (source or binary): installed TF binary '
                 '2.0 with Conda\r\n'
                 '- TensorFlow version (or github SHA if from source): TF 2.0\r\n' \
                 '\r\n'
                 '\r\n'
                 '**Command used to run the converter or code if you’re using the '
                 'Python API**\r\n'
                 '\r\n'
                 '```\r\n'
                 'converter = ' \
                 "tf.lite.TFLiteConverter.from_saved_model('model_mnist.hd5')\r\n"
                 'converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n'
                 'converter.target_spec.supported_ops = '
                 '[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n'
                 'converter.inference_input_type = tf.uint8\r\n' \
                 'converter.inference_output_type = tf.uint8\r\n'
                 '\r\n' \
                 'images = tf.cast(X_train, tf.float32)\r\n'
                 'mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\r\n'
                 'def representative_data_gen():\r\n' \
                 '    for input_value in mnist_ds.take(100):\r\n'
                 '        yield[input_value]\r\n' \
                 'converter.representative_dataset = representative_data_gen\r\n' \
                 '\r\n'
                 'tflite_quant_model = converter.convert()\r\n'
                 "with open('model_mnist_quant_uint8.tflite', 'wb') as f:\r\n"
                 '    f.write(tflite_quant_model)\r\n'
                 '\r\n' \
                 '\r\n' \
                 'interpreter = ' \
                 "tf.lite.Interpreter(model_path='model_mnist_quant_uint8.tflite')\r\n"
                 'interpreter.allocate_tensors()\r\n' \
                 '\r\n'
                 'img = X_train[0] * 255\r\n' \
                 "img = img.astype('uint8')\r\n"
                 'print(interpreter.get_input_details())\r\n'
                 "interpreter.set_tensor(interpreter.get_input_details()[0]['index'], "
                 'np.expand_dims(img, axis=0))\r\n' \
                 '```\r\n' \
                 '\r\n'
                 '**The output from the converter invocation**\r\n'
                 '\r\n' \
                 '```\r\n' \
                 "[{'name': 'flatten_input', 'index': 11, 'shape': array([ 1, 28, "
                 "28]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, " \
                 '0)}]\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'ValueError                                Traceback (most recent '
                 'call last)\r\n'
                 '<ipython-input-12-5a2fa86c9de2> in <module>\r\n'
                 "      5 img = img.astype('uint8')\r\n" \
                 '      6 print(interpreter.get_input_details())\r\n' \
                 '----> 7 '
                 "interpreter.set_tensor(interpreter.get_input_details()[0]['index'], " \
                 'np.expand_dims(img, axis=0))\r\n'
                 '\r\n' \
                 '~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter.py '
                 'in set_tensor(self, tensor_index, value)\r\n'
                 '    344       ValueError: If the interpreter could not set the '
                 'tensor.\r\n'
                 '    345     """\r\n' \
                 '--> 346     self._interpreter.SetTensor(tensor_index, value)\r\n'
                 '    347 \r\n' \
                 '    348   def resize_tensor_input(self, input_index, ' \
                 'tensor_size):\r\n'
                 '\r\n' \
                 '~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow_core\\lite\\python\\interpreter_wrapper\\tensorflow_wrap_interpreter_wrapper.py ' \
                 'in SetTensor(self, i, value)\r\n' \
                 '    134 \r\n'
                 '    135     def SetTensor(self, i, value):\r\n'
                 '--> 136         return '
                 '_tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_SetTensor(self, '
                 'i, value)\r\n' \
                 '    137 \r\n'
                 '    138     def GetTensor(self, i):\r\n'
                 '\r\n'
                 'ValueError: Cannot set tensor: Got tensor of type UINT8 but '
                 'expected type FLOAT32 for input 11, name: flatten_input \r\n'
                 '```\r\n' \
                 '\r\n'
                 '**Also, please include a link to the saved model or GraphDef**\r\n'
                 '\r\n'
                 '```\r\n'
                 '# Put link here or attach to the issue.\r\n' \
                 '```\r\n'
                 '\r\n'
                 '**Failure details**\r\n'
                 'The conversion is successful, but the generated model is wrong. The '
                 'input tensor dtype should be uint8, but is still float32.\r\n'
                 '\r\n'
                 'I tried the same thing with TF 1.15.0. In this case, every things ' \
                 'works as expected. Here is the result with TF 1.15.0\r\n'
                 '```\r\n' \
                 "[{'name': 'flatten_input', 'index': 11, 'shape': array([ 1, 28, "
                 "28], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': "
                 '(0.003921568859368563, 0)}]\r\n'
                 '```\r\n' \
                 '\r\n'
                 '\r\n' \
                 '**Any other info / logs**\r\n' \
                 '\r\n'
                 'Include any logs or source code that would be helpful to diagnose ' \
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2019-12-2'},
        {'body': 'An external developer pointed out that the test for the quantized '
                 'fully connected operation passes in a non-zero weight offset to the '
                 'kernel for int8 tests:\r\n'
                 'https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/fully_connected_test.cc#L118-L119\r\n'
                 '\r\n'
                 'The quantization specification promises that int8 kernels will '
                 'always receive zero weight offsets:\r\n' \
                 'https://www.tensorflow.org/lite/performance/quantization_spec\r\n'
                 '\r\n' \
                 'This failing test is preventing an optimized kernel for a hardware '
                 'platform from being accepted.',
         'created_at': '2019-12-2'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):  Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu ' \
                 '18.04\r\n'
                 '- TensorFlow installed from (source or binary):  binary\r\n'
                 '- TensorFlow version (use command below):  2.0.0\r\n'
                 '- Python version:  3.6.8\r\n'
                 '- CUDA/cuDNN version:  10.2 / 7.6.5.32-1+cuda10.2\r\n'
                 '- GPU model and memory:  NVidia Titan RTX 24218 MiB\r\n'
                 '\r\n' \
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'When I attempt to slice a tensor inside a keras.util.Sequence from '
                 'model.fit_generator with multiprocessing=True, TensorFlow hangs '
                 'forever without reporting any error or using any CPU or GPU '
                 'cycles.  It works as expected when multiprocessing=False.\r\n' \
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'TensorFlow should correctly fit the model just as it does with '
                 'multiprocessing=False\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 'In order to reproduce, substitute my_jpeg for some jpeg on your '
                 'computer (hopefully with dimension greater than 224px).  Note that '
                 'if you set use_multiprocessing=False in the example below, then '
                 'this will correctly train the model.\r\n'
                 '\r\n'
                 '```\r\n'
                 'from __future__ import absolute_import, division, print_function, '
                 'unicode_literals\r\n' \
                 '\r\n'
                 'import tensorflow as tf\r\n'
                 'from tensorflow import keras\r\n'
                 '\r\n'
                 '# In order to reproduce, just use whatever random JPEG you have '
                 'handy here.\r\n'
                 '# It should be larger than my_crop in the x and y dimension.\r\n'
                 'my_jpeg = "/home/ben/my_jpeg.jpg"\r\n'
                 'my_crop = 224\r\n' \
                 '\r\n'
                 '# Generates a single crop for TensorFlow.\r\n'
                 'class DataGenerator(keras.utils.Sequence):\r\n'
                 '  def __init__(\r\n'
                 '      self,\r\n'
                 '      image_location,\r\n'
                 '      crop_size=224):\r\n'
                 '    self._image_location = image_location\r\n'
                 '    self._crop_size = crop_size\r\n' \
                 '\r\n'
                 '  # Just one single batch will be returned, of just one single '
                 'image.\r\n'
                 '  def __len__(self):\r\n' \
                 '    return 1\r\n'
                 '\r\n'
                 '  # Generate one batch of data.\r\n'
                 '  def __getitem__(self, index):\r\n'
                 '    # Where the tensors will be stored.\r\n'
                 '    X = []\r\n'
                 '    y = [1]\r\n'
                 '\r\n'
                 '    # Read it.\r\n'
                 '    image = tf.io.read_file(self._image_location)\r\n'
                 '\r\n'
                 '    # Load it.\r\n'
                 '    image = tf.image.decode_jpeg(image, channels=3)\r\n' \
                 '\r\n'
                 '    assert image.shape[2] == 3  # MUST be RGB.\r\n'
                 '    height = image.shape[0]\r\n'
                 '    width = image.shape[1]\r\n'
                 '\r\n' \
                 '    # Just take a trivial crop of the image.\r\n'
                 '    # This is the offending line operation which hangs forever.\r\n'
                 '    image = image[0:self._crop_size, 0:self._crop_size, :]\r\n'
                 '\r\n'
                 '    # This line is equivalent to above, and it also hangs with '
                 'multiprocessing enabled.\r\n'
                 '    # image = tf.slice(image, [0, 0, 0], [self._crop_size, '
                 'self._crop_size, 3])\r\n'
                 '\r\n'
                 '    X.append(tf.dtypes.cast(image, tf.float32))\r\n' \
                 '\r\n'
                 '    # Tensors are not generally assignable, but we can create them '
                 'from a number of existing ones.\r\n'
                 '    X = tf.stack(X)\r\n'
                 '    y = tf.stack(y)\r\n' \
                 '\r\n'
                 '    # Preprocess it.\r\n'
                 '    X /= 255.0  # Normalize to [0, 1] range.\r\n'
                 '\r\n'
                 '    return X, y\r\n' \
                 '\r\n'
                 'generator = DataGenerator(my_jpeg, my_crop)\r\n'
                 '\r\n'
                 'model = tf.keras.applications.ResNet50(input_shape=(my_crop, ' \
                 'my_crop, 3))\r\n'
                 '\r\n' \
                 "model.compile(loss='mse')\r\n"
                 '\r\n'
                 '# use_multiprocessing=False works.\r\n'
                 '# use_multiprocessing=True hangs.\r\n'
                 'model.fit_generator(generator, use_multiprocessing=True, '
                 'workers=2)\r\n'
                 '```\r\n'
                 '\r\n',
         'created_at': '2019-12-2'},
        {'body': "I'm currently using tf.1.14 on Windows 10, but it doesn't seem to "
                 'matter too much.\r\n'
                 '\r\n'
                 'I have a high-dimensional tensor object which I try to process with '
                 'a Keras Dense layer. On the GPU everything works perfectly fine, '
                 'but if I want to run the network on the CPU I get the following '
                 'error "Only ranks up to 5 supported:.... ".  A quick look into the '
                 'C code shows that the dimensionality handling is, indeed, '
                 'hardcoded:\r\n'
                 'https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/bias_op.cc#L102\r\n' \
                 '\r\n'
                 'My question is only why? It seems like it might have something to '
                 'do with the way `GetBiasValueDims()` is written, but not '
                 'necessarily.',
         'created_at': '2019-12-2'},
        {'body': '@tensorflow/micro\r\n' \
                 '\r\n'
                 '**System information**\r\n'
                 '- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): '
                 'Ubuntu 18.04 ( Windows SL)\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- Tensorflow version (commit SHA if source):\r\n'
                 '- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): target '
                 ': efr32/efm32 (tried to compile with mbed) \r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 "I'm trying to run one of the provided projects on efr/efm32 " \
                 'microcontroller, (i choose magic_wanda example because i have an ' \
                 "accelero & it seem's good example to start with). \r\n"
                 'I wanted just to generate a binary file first, then make changes to '
                 'adapt code with external accelerometer, itried this command:  _make '
                 '-f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed '
                 'TAGS="CMSIS efm32pg_stk3401" magic_wand_bin_ but errors appear.\r\n'
                 'Should i do some big modifications on project to run it on '
                 'efr32/efm32 micro ?\r\n'
                 "Thank's for your answers\r\n"
                 '\r\n'
                 '**Please provide the exact sequence of commands/steps when you ran '
                 'into the problem**\r\n'
                 '\r\n'
                 ' _make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed '
                 'TAGS="CMSIS efm32pg_stk3401" magic_wand_bin\r\n' \
                 '\r\n',
         'created_at': '2019-12-2'},
        {'body': 'This was discovered in debugging of '
                 'https://github.com/tensorflow/tensorboard/issues/766 by a '
                 'combination of @psybuzz, @wchargin, and myself.  From empirical '
                 'evidence from TensorBoard users it appears that this grows without '
                 'bound, so in practical usage it only takes a day or so to consume '
                 'dozens of GB of memory.\r\n'
                 '\r\n' \
                 'Calling `tf.io.gfile.isdir()` leaks memory at a rate of '
                 'approximately 1 MB per 20,000 calls, and this is reproducible at TF '
                 '2.0.0 and latest tf-nightly (`tf-nightly-2.1.0.dev20191219`), on '
                 'macOS, Ubuntu 16.04, and Linux Debian (a Google workstation), and '
                 'with python 2.7, 3.5, and 3.7.\r\n'
                 '\r\n' \
                 "Here's our repro script:\r\n"
                 '```python\r\n'
                 'import gc\r\n'
                 'import os\r\n'
                 'import resource\r\n'
                 'import time\r\n'
                 '\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n' \
                 'print("PID: %d\\n" % (os.getpid(),))\r\n'
                 'prev = 0\r\n'
                 'while True:\r\n' \
                 '  peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n'
                 '  print("peak memory = %d (+%d) in kb (Linux) or b (macOS)" % ' \
                 '(peak, peak-prev))\r\n' \
                 '  prev = peak\r\n'
                 '  for _ in range(20000):\r\n'
                 '    '
                 'tf.io.gfile.isdir(b"/tmp/nonexistent-file-for-tf-memory-leak")\r\n' \
                 '  gc.collect()\r\n'
                 '  time.sleep(1.0)\r\n'
                 '```\r\n'
                 '\r\n'
                 'Sample output of `python repro.py`:\r\n'
                 '```\r\n' \
                 'PID: 153611\r\n'
                 '\r\n'
                 'peak memory = 226796 (+226796) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 228108 (+1312) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 229132 (+1024) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 229900 (+768) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 230924 (+1024) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 231948 (+1024) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 232716 (+768) in kb (Linux) or b (macOS)\r\n' \
                 'peak memory = 233740 (+1024) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 234764 (+1024) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 235788 (+1024) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 236556 (+768) in kb (Linux) or b (macOS)\r\n'
                 'peak memory = 237580 (+1024) in kb (Linux) or b (macOS)\r\n'
                 '...\r\n' \
                 '```\r\n'
                 '\r\n'
                 'Our initial attempt to find a root cause led us to suspect the fact ' \
                 'that `is_directory_v2` uses ScopedTFStatus while the rest of the ' \
                 '`gfile` API does not (we spot-checked a few other APIs, including '
                 '`tf.io.gfile.stat()`, and did not see the same issue).\r\n'
                 '\r\n'
                 "Here's the code from v2.0.0 (file_io.py was just converted to "
                 "PyBind11 today so it's possible this actually fixes the issue, but "
                 'there is not yet a nightly with the change to check):\r\n' \
                 'https://github.com/tensorflow/tensorflow/blob/v2.0.0/tensorflow/python/lib/io/file_io.py#L585-L596\r\n'
                 '\r\n'
                 'We attempted to debug further by deconstructing the calls to '
                 '`isdir()` into the two lines, one that creates `ScopedTFStatus` and '
                 'one that calls `pywrap_tensorflow.IsDirectory()`, and it seemed to ' \
                 'be the case that the memory leak is proportional to the number of '
                 'times `IsDirectory()` is called with a *distinct* `ScopedTFStatus` '
                 "pointer (calling it over and over with the same status doesn't seem " \
                 'to leak at a proportional rate; reusing the status here seemed fine '
                 'for testing this because `IsDirectory()` does not actually touch '
                 'the status in the codepath for a nonexistent file).  So we suspect '
                 "maybe there's a weird interaction at the SWIG boundary that results "
                 'in the leak.\r\n'
                 '\r\n'
                 'Furthermore, it also seems to leak when the argument is an existing '
                 'filename; the repro uses a nonexistent one for simplicity and '
                 'because that makes the codepath slightly simpler (since then ' \
                 '`IsDirectory()` exits early on file nonexistence via the `access()` '
                 'syscall and never even calls `stat()`).  Also, the leak still '
                 "occurs when the `gc.collect()` is omitted; it's also just there to "
                 'isolate possible causes of the leak.',
         'created_at': '2019-12-2'},
        {'body': 'This PR addresses minor spelling tweaks under `tensorflow/lite` ' \
                 'directory.\r\n'
                 'follow-on of #34958', \
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android '
                 '9.0 and 10.0\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: Pixel 3a (Android 10), tested on '
                 'Android 9 as well with same bad behaviour\r\n'
                 '- TensorFlow installed from (source or binary):  tflite .so built '
                 'from source\r\n' \
                 '- TensorFlow version (use command below): 1.13, 1.14, 1.15, 2.0, '
                 '2.1\r\n'
                 '- Bazel version (if compiling from source): various; 0.21 to '
                 '0.29\r\n'
                 '\r\n'
                 'Problem:\r\n'
                 'We have an audio processing application that runs in real time, '
                 "including when the phone's screen is off. Using the built shared "
                 'objects (tensorflowlite.so) from r1.13 and r1.14 branches, our '
                 'processing time stays consistent. Locking the phone or minimizing '
                 'the app does not affect the tflite inference times. Our app also '
                 'uses a foreground service to make Android give us optimal process '
                 'scheduling.\r\n'
                 '\r\n'
                 'However, when using newer tflite shared objects (.so) built from ' \
                 'r1.15, r2.0, and r2.1 the inference performance drops when the app '
                 'is not in focus (either screen was locked, or our app minimized). ' \
                 'The behaviour is especially bad when using more threads, e.g: '
                 '`interpreter->SetNumThreads(2);`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'Android tflite model inference times are lower when app is not in '
                 'focus with tflite versions r1.15, r2.0, r2.1. \r\n'
                 '**Describe the expected behavior**\r\n'
                 'Exactly the same inference processing performance when the app is '
                 'in focus or not.\r\n'
                 '\r\n'
                 'Is there something in the API of newer tflite versions (r1.15 and '
                 'newer) that I could play around with to fix this? Any help is ' \
                 'greatly appreciated, thank you!\r\n',
         'created_at': '2019-12-1'},
        {'body': 'My application only requires kTfLiteInt8 kernels.\r\n'
                 'However, kTfLiteFloat32 and kTfLiteUInt8 kernels are also built '
                 'into the application.\r\n' \
                 '\r\n'
                 'It would save a considerable amount of code space if there was a '
                 'way to disable building in the unused data types, e.g.:\r\n'
                 '```\r\n'
                 '  switch (input->type) {  // Already know in/out types are same.\r\n' \
                 '#ifndef TFLITE_FLOAT32_DISABLED\r\n'
                 '    case kTfLiteFloat32:\r\n'
                 '      return EvalFloat(context, node, params, &data, input, filter, '
                 'bias,\r\n'
                 '                       nullptr, nullptr, output);\r\n' \
                 '      break;\r\n' \
                 '#endif \r\n'
                 '#infdef TFLITE_INT8_DISABLED\r\n'
                 '    case kTfLiteInt8:\r\n'
                 '      return EvalQuantizedPerChannel(context, node, params, &data, '
                 'input,\r\n'
                 '                                     filter, bias, output, '
                 'nullptr);\r\n'
                 '      break; \r\n'
                 '#endif \r\n'
                 '#ifndef TFLITE_UINT8_DISABLED\r\n'
                 '    case kTfLiteUInt8:\r\n'
                 '      return EvalQuantized(context, node, params, &data, input, '
                 'filter, bias,\r\n' \
                 '                           nullptr, nullptr, output);\r\n'
                 '      break;\r\n'
                 '#endif\r\n'
                 '    default:\r\n'
                 '      context->ReportError(context, "Type %s (%d) not '
                 'supported.",\r\n'
                 '                           TfLiteTypeGetName(input->type), ' \
                 'input->type);\r\n'
                 '      return kTfLiteError\r\n'
                 '```\r\n'
                 '\r\n'
                 'Or something more elegant ;) \r\n',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n' \
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes, see minimal example.\r\n'
                 '- OS Platform and Distribution: Ubuntu 18.04.3 LTS\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:  N/A\r\n' \
                 '- TensorFlow installed from (source or binary): binary '
                 '(specifically, `tensorflow/tensorflow:nightly-py3` Docker image)\r\n'
                 '- TensorFlow version (use command below): 2.1.0-dev20191216\r\n'
                 '- Python version: 3.6.9\r\n'
                 '- Bazel version (if compiling from source): N/A\r\n'
                 '- GCC/Compiler version (if compiling from source): N/A\r\n' \
                 '- CUDA/cuDNN version: N/A\r\n' \
                 '- GPU model and memory: N/A\r\n'
                 '\r\n' \
                 '**Describe the current behavior**\r\n'
                 '\r\n' \
                 "A multi-output Keras model compiled so that one output doesn't have "
                 'a loss function raises an exception when calling `.fit`.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'Training should minimise the losses defined for the other '
                 'output(s).\r\n'
                 '\r\n' \
                 '**Code to reproduce the issue**\r\n'
                 '\r\n' \
                 '```\r\n'
                 'import numpy as np\r\n'
                 'import tensorflow as tf\r\n'
                 'import tensorflow.keras as keras\r\n'
                 '\r\n'
                 'input_a = keras.layers.Input(shape=(10,), name="input_a")\r\n'
                 'input_b = keras.layers.Input(shape=(20,), name="input_b")\r\n'
                 'output_a = keras.layers.Dense(1, name="output_a")(input_a)\r\n'
                 'output_b = keras.layers.Dense(1, name="output_b")(input_b)\r\n'
                 'model = keras.Model(inputs=[input_a, input_b], outputs=[output_a, '
                 'output_b])\r\n'
                 'model.compile(optimizer="sgd", loss={"output_a": None, "output_b": '
                 '"mse"})\r\n'
                 '\r\n'
                 'n = 128\r\n' \
                 'input_a = np.ones((n, 10))\r\n'
                 'input_b = np.ones((n, 20))\r\n'
                 'output_a = np.ones((n, 1))\r\n'
                 'output_b = np.ones((n, 1))\r\n' \
                 '\r\n'
                 'dataset = tf.data.Dataset.from_tensor_slices(\r\n'
                 '    ((input_a, input_b), (output_a, output_b))\r\n' \
                 ').batch(64)\r\n'
                 '\r\n'
                 'model.fit(dataset)\r\n'
                 '```\r\n'
                 '\r\n'
                 'Raises:\r\n'
                 '\r\n'
                 '```\r\n'
                 'ValueError: Error when checking model target: the list of Numpy '
                 'arrays that you are passing to your model is not the size the model '
                 "expected. Expected to see 1 array(s), for inputs ['output_b'] but " \
                 "instead got the following list of 2 arrays: [<tf.Tensor 'args_2:0' "
                 "shape=(None, 1) dtype=float64>, <tf.Tensor 'args_3:0' shape=(None, "
                 '1) dtype=float64>]...\r\n' \
                 '```',
         'created_at': '2019-12-1'},
        {'body': '', 'created_at': '2019-12-1'},
        {'body': '### tf.linalg.normalize(np.zeros([10, 4]), ord=1, axis=-1) yields '
                 'nan as below\r\n'
                 '\r\n'
                 '(<tf.Tensor: id=58, shape=(10, 4), dtype=float64, numpy=\r\n' \
                 ' array([[nan, nan, nan, nan],\r\n'
                 '        [nan, nan, nan, nan],\r\n'
                 '        [nan, nan, nan, nan],\r\n'
                 '        [nan, nan, nan, nan],\r\n' \
                 '        [nan, nan, nan, nan],\r\n'
                 '        [nan, nan, nan, nan],\r\n'
                 '        [nan, nan, nan, nan],\r\n'
                 '        [nan, nan, nan, nan],\r\n' \
                 '        [nan, nan, nan, nan],\r\n'
                 '        [nan, nan, nan, nan]])>,\r\n'
                 ' <tf.Tensor: id=57, shape=(10, 1), dtype=float64, numpy=\r\n' \
                 ' array([[0.],\r\n'
                 '        [0.],\r\n'
                 '        [0.],\r\n'
                 '        [0.],\r\n'
                 '        [0.],\r\n'
                 '        [0.],\r\n'
                 '        [0.],\r\n' \
                 '        [0.],\r\n'
                 '        [0.],\r\n' \
                 '        [0.]])>)\r\n'
                 '\r\n' \
                 '\r\n'
                 '* I know this is caused by divide by 0, so in the future tensorflow '
                 'should make this operation more numerically stable. ',
         'created_at': '2019-12-1'},
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n' \
                 '\r\n' \
                 '**System information**\r\n'
                 '- OS Platform Windows 10 Pro\r\n'
                 '- desktop computer:\r\n' \
                 '- TensorFlow installed from source\r\n' \
                 '- TensorFlow version: 2.0\r\n'
                 '- Python version: 3.7\r\n'
                 '- Installed using virtualenv? pip? conda?: installed via command '
                 'line / git clone etc.\r\n'
                 '- Bazel version (if compiling from source): 1.1.0\r\n'
                 '- GCC/Compiler version (if compiling from source): Visual Studio '
                 '2017 Redistributables\r\n'
                 '- CUDA/cuDNN version: --- (ROCm selected)\r\n'
                 '- GPU model and memory:  AMD FX-8800P R7\r\n'
                 '\r\n'
                 'Build fails, see listing below.\r\n'
                 '\r\n'
                 'C:\\Users\\Bludorf\\tensorflow>python ./configure.py\r\n'
                 'WARNING: Running Bazel server needs to be killed, because the '
                 'startup options are different.\r\n'
                 'WARNING: Waiting for server process to terminate (waited 5 seconds, '
                 'waiting at most 60)\r\n'
                 'WARNING: Waiting for server process to terminate (waited 10 '
                 'seconds, waiting at most 60)\r\n'
                 'WARNING: --batch mode is deprecated. Please instead explicitly shut '
                 'down your Bazel server using the command "bazel shutdown".\r\n'
                 'You have bazel 1.1.0 installed.\r\n'
                 'Please specify the location of python. [Default is '
                 'C:\\Users\\Bludorf\\AppData\\Local\\Programs\\Python\\Python37\\python.exe]: \r\n'
                 '\r\n' \
                 '\r\n'
                 'Found possible Python library paths:\r\n'
                 '  '
                 'C:\\Users\\Bludorf\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\r\n'
                 'Please input the desired Python library path to use.  Default is '
                 '[C:\\Users\\Bludorf\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages]\r\n'
                 '\r\n' \
                 'Do you wish to build TensorFlow with XLA JIT support? [y/N]: y\r\n'
                 'XLA JIT support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with ROCm support? [y/N]: y\r\n'
                 'ROCm support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Do you wish to build TensorFlow with CUDA support? [y/N]: N\r\n' \
                 'No CUDA support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Please specify optimization flags to use during compilation when '
                 'bazel option "--config=opt" is specified [Default is /arch:AVX]:\r\n'
                 '\r\n'
                 '\r\n'
                 'Would you like to override eigen strong inline for some C++ '
                 'compilation to reduce the compilation time? [Y/n]: Y\r\n'
                 'Eigen strong inline overridden.\r\n' \
                 '\r\n'
                 'Preconfigured Bazel build configs. You can use any of the below by '
                 'adding "--config=<>" to your build command. See .bazelrc for more '
                 'details.\r\n'
                 '        --config=mkl            # Build with MKL support.\r\n'
                 '        --config=monolithic     # Config for mostly static '
                 'monolithic build.\r\n'
                 '        --config=ngraph         # Build with Intel nGraph '
                 'support.\r\n' \
                 '        --config=numa           # Build with NUMA support.\r\n'
                 '        --config=dynamic_kernels        # (Experimental) Build '
                 'kernels into separate shared objects.\r\n'
                 '        --config=v2             # Build TensorFlow 2.x instead of '
                 '1.x.\r\n'
                 'Preconfigured Bazel build configs to DISABLE default on '
                 'features:\r\n'
                 '        --config=noaws          # Disable AWS S3 filesystem ' \
                 'support.\r\n' \
                 '        --config=nogcp          # Disable GCP support.\r\n' \
                 '        --config=nohdfs         # Disable HDFS support.\r\n'
                 '        --config=nonccl         # Disable NVIDIA NCCL support.\r\n'
                 '\r\n'
                 'C:\\Users\\Bludorf\\tensorflow>bazel build --config=v2 ' \
                 '//tensorflow/tools/pip_package:build_pip_package\r\n'
                 'Starting local Bazel server and connecting to it...\r\n'
                 'WARNING: The following configs were expanded more than once: [v2]. '
                 'For repeatable flags, repeats are counted twice and may lead to '
                 'unexpected behavior.\r\n'
                 'INFO: Writing tracer profile to '
                 "'C:/users/bludorf/_bazel_bludorf/jkbqqwso/command.profile.gz'\r\n" \
                 'INFO: Options provided by the client:\r\n'
                 "  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\n"
                 'INFO: Options provided by the client:\r\n'
                 "  'build' options: "
                 '--python_path=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe\r\n'
                 "INFO: Reading rc options for 'build' from "
                 'c:\\users\\bludorf\\tensorflow\\.bazelrc:\r\n'
                 "  'build' options: --apple_platform_type=macos --define "
                 'framework_shared_object=true --define open_source_build=true ' \
                 '--java_toolchain=//third_party/toolchains/java:tf_java_toolchain '
                 '--host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain '
                 '--define=use_fast_cpp_protos=true '
                 '--define=allow_oversize_protos=true --spawn_strategy=standalone -c '
                 'opt --announce_rc --define=grpc_no_ares=true '
                 '--noincompatible_remove_legacy_whole_archive '
                 '--enable_platform_specific_config --config=v2\r\n'
                 "INFO: Reading rc options for 'build' from "
                 'c:\\users\\bludorf\\tensorflow\\.tf_configure.bazelrc:\r\n'
                 "  'build' options: --action_env "
                 'PYTHON_BIN_PATH=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe '
                 '--action_env '
                 'PYTHON_LIB_PATH=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/lib/site-packages '
                 '--python_path=C:/Users/Bludorf/AppData/Local/Programs/Python/Python37/python.exe ' \
                 '--config=xla --config=rocm '
                 '--define=override_eigen_strong_inline=true --action_env '
                 'TF_CONFIGURE_IOS=0\r\n'
                 'INFO: Found applicable config definition build:v2 in file '
                 'c:\\users\\bludorf\\tensorflow\\.bazelrc: --define=tf_api_version=2 ' \
                 '--action_env=TF2_BEHAVIOR=1\r\n'
                 'INFO: Found applicable config definition build:xla in file '
                 'c:\\users\\bludorf\\tensorflow\\.bazelrc: '
                 '--action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\n'
                 'INFO: Found applicable config definition build:xla in file '
                 'c:\\users\\bludorf\\tensorflow\\.tf_configure.bazelrc: --define '
                 'with_xla_support=true\r\n' \
                 'INFO: Found applicable config definition build:rocm in file '
                 'c:\\users\\bludorf\\tensorflow\\.bazelrc: '
                 '--crosstool_top=@local_config_rocm//crosstool:toolchain '
                 '--define=using_rocm=true --define=using_rocm_hipcc=true '
                 '--action_env TF_NEED_ROCM=1\r\n' \
                 'INFO: Found applicable config definition build:v2 in file ' \
                 'c:\\users\\bludorf\\tensorflow\\.bazelrc: --define=tf_api_version=2 '
                 '--action_env=TF2_BEHAVIOR=1\r\n' \
                 'INFO: Found applicable config definition build:windows in file '
                 'c:\\users\\bludorf\\tensorflow\\.bazelrc: --copt=/w '
                 '--cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic '
                 '--copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN ' \
                 '--copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG '
                 '--host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF '
                 '--linkopt=/OPT:ICF --host_linkopt=/OPT:ICF '
                 '--experimental_strict_action_env=true '
                 '--incompatible_windows_native_test_wrapper --verbose_failures '
                 '--distinct_host_configuration=false\r\n'
                 'INFO: Found applicable config definition build:monolithic in file '
                 'c:\\users\\bludorf\\tensorflow\\.bazelrc: --define '
                 'framework_shared_object=false\r\n'
                 'INFO: Call stack for the definition of repository '
                 "'io_bazel_rules_docker' which is a git_repository (rule definition "
                 'at '
                 'C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n' \
                 ' - '
                 'C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n'
                 ' - C:/users/bludorf/tensorflow/WORKSPACE:37:1\r\n'
                 'ERROR: An error occurred during the fetch of repository '
                 "'io_bazel_rules_docker':\r\n" \
                 '   Traceback (most recent call last):\r\n'
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl", '
                 'line 177\r\n'
                 '                _clone_or_update(ctx)\r\n'
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl", '
                 'line 36, in _clone_or_update\r\n'
                 '                git_repo(ctx, directory)\r\n'
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 91, in git_repo\r\n'
                 '                _update(ctx, git_repo)\r\n'
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 101, in _update\r\n' \
                 '                init(ctx, git_repo)\r\n'
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 115, in init\r\n'
                 '                _error(ctx.name, cl, st.stderr)\r\n' \
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 181, in _error\r\n'
                 '                fail(<1 more arguments>)\r\n'
                 "error running 'git init " \
                 "C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' "
                 'while working with @io_bazel_rules_docker:\r\n'
                 'java.io.IOException: ERROR: '
                 'src/main/native/windows/process.cc(199): CreateProcessW("git" init '
                 'C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): '
                 'Das System kann die angegebene Datei nicht finden.\r\n'
                 ' (error: 2)\r\n'
                 "ERROR: no such package '@io_bazel_rules_docker//repositories': " \
                 'Traceback (most recent call last):\r\n'
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl", '
                 'line 177\r\n' \
                 '                _clone_or_update(ctx)\r\n' \
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl", '
                 'line 36, in _clone_or_update\r\n' \
                 '                git_repo(ctx, directory)\r\n' \
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 91, in git_repo\r\n'
                 '                _update(ctx, git_repo)\r\n'
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 101, in _update\r\n'
                 '                init(ctx, git_repo)\r\n' \
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 115, in init\r\n' \
                 '                _error(ctx.name, cl, st.stderr)\r\n' \
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", ' \
                 'line 181, in _error\r\n'
                 '                fail(<1 more arguments>)\r\n'
                 "error running 'git init " \
                 "C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' "
                 'while working with @io_bazel_rules_docker:\r\n'
                 'java.io.IOException: ERROR: '
                 'src/main/native/windows/process.cc(199): CreateProcessW("git" init '
                 'C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): ' \
                 'Das System kann die angegebene Datei nicht finden.\r\n' \
                 ' (error: 2)\r\n'
                 "ERROR: no such package '@io_bazel_rules_docker//repositories': "
                 'Traceback (most recent call last):\r\n'
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl", '
                 'line 177\r\n'
                 '                _clone_or_update(ctx)\r\n'
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git.bzl", '
                 'line 36, in _clone_or_update\r\n'
                 '                git_repo(ctx, directory)\r\n'
                 '        File ' \
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", ' \
                 'line 91, in git_repo\r\n' \
                 '                _update(ctx, git_repo)\r\n'
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 101, in _update\r\n'
                 '                init(ctx, git_repo)\r\n'
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", '
                 'line 115, in init\r\n'
                 '                _error(ctx.name, cl, st.stderr)\r\n'
                 '        File '
                 '"C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/bazel_tools/tools/build_defs/repo/git_worker.bzl", ' \
                 'line 181, in _error\r\n'
                 '                fail(<1 more arguments>)\r\n'
                 "error running 'git init " \
                 "C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker' " \
                 'while working with @io_bazel_rules_docker:\r\n'
                 'java.io.IOException: ERROR: '
                 'src/main/native/windows/process.cc(199): CreateProcessW("git" init '
                 'C:/users/bludorf/_bazel_bludorf/jkbqqwso/external/io_bazel_rules_docker): '
                 'Das System kann die angegebene Datei nicht finden.\r\n'
                 ' (error: 2)\r\n'
                 'INFO: Elapsed time: 7.563s\r\n' \
                 'INFO: 0 processes.\r\n' \
                 'FAILED: Build did NOT complete successfully (0 packages loaded)\r\n'
                 '\r\n',
         'created_at': '2019-12-1'}, \
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu ' \
                 '18.04\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (or github SHA if from source): 1.15.0-gpu\r\n' \
                 '\r\n'
                 '\r\n' \
                 '**Command used to run the converter or code if you’re using the '
                 'Python API**\r\n' \
                 '\r\n'
                 '```\r\n'
                 'import os\r\n'
                 '\r\n'
                 'import tensorflow as tf\r\n'
                 'import numpy as np\r\n' \
                 'from PIL import Image\r\n'
                 '\r\n' \
                 '\r\n'
                 'dataset = []\r\n'
                 "directory_images = './data_test'\r\n" \
                 "directory_saved = './models'\r\n"
                 'for img in os.listdir(directory_images):\r\n'
                 '\tdata = Image.open(os.path.join(directory_images,img)))\r\n' \
                 '\tdata = np.asarray(data, dtype=np.float32)[np.newaxis, :]\r\n' \
                 '\tdataset.append(data)\r\n'
                 '\r\n'
                 'def representative_dataset_gen():\r\n' \
                 '    for input_value in dataset:\r\n'
                 '        yield [input_value]\r\n'
                 '\r\n'
                 'converter = '
                 'tf.lite.TFLiteConverter.from_saved_model(directory_saved + '
                 '"/saved")\r\n' \
                 'converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n'
                 'converter.representative_dataset = representative_dataset_gen\r\n' \
                 'converter.target_spec.supported_ops = ' \
                 '[tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n'
                 'converter.inference_input_type = tf.uint8\r\n' \
                 'converter.inference_output_type = tf.uint8\r\n' \
                 '\r\n'
                 'tflite_model = converter.convert()\r\n' \
                 '\r\n' \
                 'name = directory_saved + "/tflite_model"\r\n'
                 'open(name + ".tflite", "wb").write(tflite_model)\r\n'
                 '```\r\n'
                 '\r\n'
                 '**The output from the converter invocation**\r\n' \
                 '\r\n' \
                 '```\r\n'
                 'Traceback (most recent call last):\r\n' \
                 '  File "saved2lite.py", line 34, in <module>\r\n' \
                 '    tflite_model = converter.convert()\r\n'
                 '  File '
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", '
                 'line 993, in convert\r\n'
                 '    inference_output_type)\r\n'
                 '  File ' \
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", '
                 'line 239, in _calibrate_quantize_model\r\n' \
                 '    inference_output_type, allow_float)\r\n'
                 '  File '
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py", '
                 'line 78, in calibrate_and_quantize\r\n' \
                 '    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n'
                 '  File '
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py", '
                 'line 115, in QuantizeModel\r\n' \
                 '    return ' \
                 '_tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, '
                 'input_py_type, output_py_type, allow_float)\r\n'
                 'RuntimeError: Invalid quantization params for op MAXIMUM at index 4 ' \
                 'in subgraph 0\r\n'
                 '```\r\n'
                 '\r\n' \
                 '**Also, please include a link to the saved model or GraphDef**\r\n'
                 '\r\n' \
                 '```\r\n' \
                 'https://www.dropbox.com/s/tj96fsm6t6rq8ye/model-r100-arcface-ms1m-refine-v2.zip?dl=0\r\n'
                 '```\r\n' \
                 '\r\n'
                 '**Failure details**\r\n' \
                 'Conversion fails.\r\n' \
                 '\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Full log:\r\n'
                 '```\r\n'
                 '2019-12-19 11:49:10.789835: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcuda.so.1\r\n'
                 '2019-12-19 11:49:10.810583: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:10.811068: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device '
                 '0 with properties: \r\n'
                 'name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): ' \
                 '1.392\r\n' \
                 'pciBusID: 0000:01:00.0\r\n'
                 '2019-12-19 11:49:10.811245: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcudart.so.10.0\r\n'
                 '2019-12-19 11:49:10.812226: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcublas.so.10.0\r\n'
                 '2019-12-19 11:49:10.813094: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcufft.so.10.0\r\n' \
                 '2019-12-19 11:49:10.813351: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcurand.so.10.0\r\n'
                 '2019-12-19 11:49:10.814559: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcusolver.so.10.0\r\n'
                 '2019-12-19 11:49:10.815596: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcusparse.so.10.0\r\n'
                 '2019-12-19 11:49:10.817880: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcudnn.so.7\r\n'
                 '2019-12-19 11:49:10.818002: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:10.818618: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:10.819072: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding ' \
                 'visible gpu devices: 0\r\n'
                 '2019-12-19 11:49:10.819465: I ' \
                 'tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU '
                 'supports instructions that this TensorFlow binary was not compiled '
                 'to use: AVX2 FMA\r\n'
                 '2019-12-19 11:49:10.843162: I '
                 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU ' \
                 'Frequency: 3408000000 Hz\r\n'
                 '2019-12-19 11:49:10.843971: I '
                 'tensorflow/compiler/xla/service/service.cc:168] XLA service '
                 '0x55724cd4c080 initialized for platform Host (this does not '
                 'guarantee that XLA will be used). Devices:\r\n'
                 '2019-12-19 11:49:10.844031: I '
                 'tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor '
                 'device (0): Host, Default Version\r\n'
                 '2019-12-19 11:49:10.926515: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n' \
                 '2019-12-19 11:49:10.927095: I ' \
                 'tensorflow/compiler/xla/service/service.cc:168] XLA service '
                 '0x55724cdae1a0 initialized for platform CUDA (this does not ' \
                 'guarantee that XLA will be used). Devices:\r\n'
                 '2019-12-19 11:49:10.927112: I '
                 'tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor ' \
                 'device (0): GeForce GTX 1050 Ti, Compute Capability 6.1\r\n'
                 '2019-12-19 11:49:10.927282: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n' \
                 '2019-12-19 11:49:10.927748: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device ' \
                 '0 with properties: \r\n'
                 'name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): ' \
                 '1.392\r\n'
                 'pciBusID: 0000:01:00.0\r\n'
                 '2019-12-19 11:49:10.927774: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcudart.so.10.0\r\n' \
                 '2019-12-19 11:49:10.927782: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcublas.so.10.0\r\n'
                 '2019-12-19 11:49:10.927790: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcufft.so.10.0\r\n'
                 '2019-12-19 11:49:10.927797: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcurand.so.10.0\r\n'
                 '2019-12-19 11:49:10.927804: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcusolver.so.10.0\r\n' \
                 '2019-12-19 11:49:10.927811: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcusparse.so.10.0\r\n'
                 '2019-12-19 11:49:10.927819: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcudnn.so.7\r\n'
                 '2019-12-19 11:49:10.927857: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:10.928283: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:10.928683: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding '
                 'visible gpu devices: 0\r\n'
                 '2019-12-19 11:49:10.928703: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcudart.so.10.0\r\n'
                 '2019-12-19 11:49:10.929442: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device '
                 'interconnect StreamExecutor with strength 1 edge matrix:\r\n'
                 '2019-12-19 11:49:10.929452: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n'
                 '2019-12-19 11:49:10.929458: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n'
                 '2019-12-19 11:49:10.929674: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:10.930285: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:10.930708: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created '
                 'TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 '
                 'with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX '
                 '1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n'
                 'WARNING:tensorflow:From '
                 '/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/convert_saved_model.py:60: '
                 'load (from tensorflow.python.saved_model.loader_impl) is deprecated '
                 'and will be removed in a future version.\r\n'
                 'Instructions for updating:\r\n'
                 'This function will only be available through the v1 compatibility '
                 'library as tf.compat.v1.saved_model.loader.load or ' \
                 'tf.compat.v1.saved_model.load. There will be a new function for '
                 'importing SavedModels in Tensorflow 2.0.\r\n'
                 '2019-12-19 11:49:13.849255: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:13.849669: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device '
                 '0 with properties: \r\n'
                 'name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): '
                 '1.392\r\n'
                 'pciBusID: 0000:01:00.0\r\n'
                 '2019-12-19 11:49:13.849711: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcudart.so.10.0\r\n'
                 '2019-12-19 11:49:13.849721: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcublas.so.10.0\r\n'
                 '2019-12-19 11:49:13.849729: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcufft.so.10.0\r\n' \
                 '2019-12-19 11:49:13.849737: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcurand.so.10.0\r\n'
                 '2019-12-19 11:49:13.849744: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcusolver.so.10.0\r\n'
                 '2019-12-19 11:49:13.849752: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcusparse.so.10.0\r\n'
                 '2019-12-19 11:49:13.849759: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcudnn.so.7\r\n'
                 '2019-12-19 11:49:13.849796: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:13.850140: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:13.850456: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding '
                 'visible gpu devices: 0\r\n'
                 '2019-12-19 11:49:13.850478: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device ' \
                 'interconnect StreamExecutor with strength 1 edge matrix:\r\n'
                 '2019-12-19 11:49:13.850484: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n'
                 '2019-12-19 11:49:13.850488: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n'
                 '2019-12-19 11:49:13.850595: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:13.850985: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but ' \
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:13.851332: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created '
                 'TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 '
                 'with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX '
                 '1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n' \
                 '2019-12-19 11:49:16.749682: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] ' \
                 'successful NUMA node read from SysFS had negative value (-1), but ' \
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:16.750076: I '
                 'tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs ' \
                 '(core count >= 8, compute capability >= 0.0): 0\r\n' \
                 '2019-12-19 11:49:16.750135: I '
                 'tensorflow/core/grappler/clusters/single_machine.cc:356] Starting '
                 'new session\r\n' \
                 '2019-12-19 11:49:16.750521: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:16.750902: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device '
                 '0 with properties: \r\n'
                 'name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): '
                 '1.392\r\n'
                 'pciBusID: 0000:01:00.0\r\n'
                 '2019-12-19 11:49:16.750927: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcudart.so.10.0\r\n'
                 '2019-12-19 11:49:16.750936: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcublas.so.10.0\r\n' \
                 '2019-12-19 11:49:16.750945: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcufft.so.10.0\r\n' \
                 '2019-12-19 11:49:16.750952: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcurand.so.10.0\r\n'
                 '2019-12-19 11:49:16.750959: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcusolver.so.10.0\r\n' \
                 '2019-12-19 11:49:16.750966: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcusparse.so.10.0\r\n' \
                 '2019-12-19 11:49:16.750974: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcudnn.so.7\r\n' \
                 '2019-12-19 11:49:16.751007: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] ' \
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:16.751348: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] ' \
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n' \
                 '2019-12-19 11:49:16.751663: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding '
                 'visible gpu devices: 0\r\n' \
                 '2019-12-19 11:49:16.751682: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device '
                 'interconnect StreamExecutor with strength 1 edge matrix:\r\n'
                 '2019-12-19 11:49:16.751688: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n'
                 '2019-12-19 11:49:16.751692: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n' \
                 '2019-12-19 11:49:16.751831: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:16.752173: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n' \
                 '2019-12-19 11:49:16.752495: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created '
                 'TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 '
                 'with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX '
                 '1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n' \
                 '2019-12-19 11:49:17.182263: I ' \
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] ' \
                 'Optimization results for grappler item: graph_to_optimize\r\n' \
                 '2019-12-19 11:49:17.182289: I ' \
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   ' \
                 'function_optimizer: function_optimizer did nothing. time = '
                 '0.003ms.\r\n' \
                 '2019-12-19 11:49:17.182737: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   '
                 'function_optimizer: function_optimizer did nothing. time = 0ms.\r\n' \
                 'WARNING:tensorflow:From '
                 '/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/util.py:249: '
                 'convert_variables_to_constants (from ' \
                 'tensorflow.python.framework.graph_util_impl) is deprecated and will ' \
                 'be removed in a future version.\r\n'
                 'Instructions for updating:\r\n'
                 'Use `tf.compat.v1.graph_util.convert_variables_to_constants`\r\n'
                 'WARNING:tensorflow:From '
                 '/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: '
                 'extract_sub_graph (from '
                 'tensorflow.python.framework.graph_util_impl) is deprecated and will ' \
                 'be removed in a future version.\r\n'
                 'Instructions for updating:\r\n'
                 'Use `tf.compat.v1.graph_util.extract_sub_graph`\r\n'
                 '2019-12-19 11:49:18.713005: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but ' \
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:18.713562: I '
                 'tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs '
                 '(core count >= 8, compute capability >= 0.0): 0\r\n'
                 '2019-12-19 11:49:18.713618: I '
                 'tensorflow/core/grappler/clusters/single_machine.cc:356] Starting '
                 'new session\r\n'
                 '2019-12-19 11:49:18.714027: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] ' \
                 'successful NUMA node read from SysFS had negative value (-1), but ' \
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:18.714386: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device '
                 '0 with properties: \r\n'
                 'name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): '
                 '1.392\r\n'
                 'pciBusID: 0000:01:00.0\r\n' \
                 '2019-12-19 11:49:18.714424: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcudart.so.10.0\r\n' \
                 '2019-12-19 11:49:18.714433: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcublas.so.10.0\r\n'
                 '2019-12-19 11:49:18.714441: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcufft.so.10.0\r\n'
                 '2019-12-19 11:49:18.714448: I ' \
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcurand.so.10.0\r\n'
                 '2019-12-19 11:49:18.714455: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcusolver.so.10.0\r\n'
                 '2019-12-19 11:49:18.714462: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcusparse.so.10.0\r\n'
                 '2019-12-19 11:49:18.714470: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcudnn.so.7\r\n'
                 '2019-12-19 11:49:18.714501: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] ' \
                 'successful NUMA node read from SysFS had negative value (-1), but ' \
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n' \
                 '2019-12-19 11:49:18.714882: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n' \
                 '2019-12-19 11:49:18.715214: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding ' \
                 'visible gpu devices: 0\r\n' \
                 '2019-12-19 11:49:18.715235: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device '
                 'interconnect StreamExecutor with strength 1 edge matrix:\r\n'
                 '2019-12-19 11:49:18.715242: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \r\n'
                 '2019-12-19 11:49:18.715261: I ' \
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \r\n'
                 '2019-12-19 11:49:18.715411: I '
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node ' \
                 'zero\r\n'
                 '2019-12-19 11:49:18.715760: I ' \
                 'tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] '
                 'successful NUMA node read from SysFS had negative value (-1), but '
                 'there must be at least one NUMA node, so returning NUMA node '
                 'zero\r\n'
                 '2019-12-19 11:49:18.716119: I '
                 'tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created '
                 'TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 '
                 'with 2934 MB memory) -> physical GPU (device: 0, name: GeForce GTX '
                 '1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n' \
                 '2019-12-19 11:49:20.750755: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] '
                 'Optimization results for grappler item: graph_to_optimize\r\n' \
                 '2019-12-19 11:49:20.750831: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   ' \
                 'constant_folding: Graph size after: 1230 nodes (-2206), 1329 edges ' \
                 '(-2360), time = 1332.40198ms.\r\n'
                 '2019-12-19 11:49:20.750850: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   ' \
                 'constant_folding: Graph size after: 1230 nodes (0), 1329 edges (0), '
                 'time = 370.552ms.\r\n'
                 'Traceback (most recent call last):\r\n'
                 '  File "saved2lite.py", line 34, in <module>\r\n'
                 '    tflite_model = converter.convert()\r\n'
                 '  File '
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", '
                 'line 993, in convert\r\n' \
                 '    inference_output_type)\r\n'
                 '  File ' \
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py", ' \
                 'line 239, in _calibrate_quantize_model\r\n'
                 '    inference_output_type, allow_float)\r\n'
                 '  File '
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py", ' \
                 'line 78, in calibrate_and_quantize\r\n' \
                 '    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n'
                 '  File '
                 '"/home/ds017/.pyenv/versions/takehome/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py", '
                 'line 115, in QuantizeModel\r\n' \
                 '    return '
                 '_tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, '
                 'input_py_type, output_py_type, allow_float)\r\n'
                 'RuntimeError: Invalid quantization params for op MAXIMUM at index 4 ' \
                 'in subgraph 0\r\n' \
                 '```',
         'created_at': '2019-12-1'},
        {'body': 'Incompatible flag --incompatible_restrict_string_escapes will break ' \
                 'TensorFlow once Bazel 1.2.1 is released.\n'
                 '\n' \
                 'Please see the following CI builds for more information:\n' \
                 '\n'
                 '* [:darwin: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7" '
                 'target="_blank">:darwin: (OpenJDK 8)</a>)\n' \
                 '* [:windows: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa" ' \
                 'target="_blank">:windows: (OpenJDK 8)</a>)\n'
                 '* [:ubuntu: 18.04 (OpenJDK 11)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1" ' \
                 'target="_blank">:ubuntu: 18.04 (OpenJDK 11)</a>)\n'
                 '\n'
                 'Questions? Please file an issue in ' \
                 'https://github.com/bazelbuild/continuous-integration\n'
                 '\n'
                 '**Important**: Please do NOT modify the issue title since that '
                 'might break our tools.\n',
         'created_at': '2019-12-1'},
        {'body': 'Incompatible flag --incompatible_use_platforms_repo_for_constraints '
                 'will break TensorFlow once Bazel 1.2.1 is released.\n'
                 '\n'
                 'Please see the following CI builds for more information:\n'
                 '\n'
                 '* [:darwin: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7" '
                 'target="_blank">:darwin: (OpenJDK 8)</a>)\n'
                 '* [:windows: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa" '
                 'target="_blank">:windows: (OpenJDK 8)</a>)\n'
                 '* [:ubuntu: 18.04 (OpenJDK 11)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1" ' \
                 'target="_blank">:ubuntu: 18.04 (OpenJDK 11)</a>)\n'
                 '\n'
                 'Questions? Please file an issue in '
                 'https://github.com/bazelbuild/continuous-integration\n' \
                 '\n' \
                 '**Important**: Please do NOT modify the issue title since that '
                 'might break our tools.\n', \
         'created_at': '2019-12-1'},
        {'body': 'Incompatible flag --incompatible_load_cc_rules_from_bzl will break '
                 'TensorFlow once Bazel 1.2.1 is released.\n' \
                 '\n'
                 'Please see the following CI builds for more information:\n'
                 '\n' \
                 '* [:darwin: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7" '
                 'target="_blank">:darwin: (OpenJDK 8)</a>)\n'
                 '* [:windows: (OpenJDK 8)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa" '
                 'target="_blank">:windows: (OpenJDK 8)</a>)\n' \
                 '* [:ubuntu: 18.04 (OpenJDK 11)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1" '
                 'target="_blank">:ubuntu: 18.04 (OpenJDK 11)</a>)\n'
                 '\n'
                 'Questions? Please file an issue in '
                 'https://github.com/bazelbuild/continuous-integration\n' \
                 '\n'
                 '**Important**: Please do NOT modify the issue title since that '
                 'might break our tools.\n', \
         'created_at': '2019-12-1'},
        {'body': 'Incompatible flag --incompatible_disable_target_provider_fields '
                 'will break TensorFlow once Bazel 1.2.1 is released.\n'
                 '\n'
                 'Please see the following CI builds for more information:\n' \
                 '\n'
                 '* [:darwin: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7" '
                 'target="_blank">:darwin: (OpenJDK 8)</a>)\n' \
                 '* [:windows: (OpenJDK 8)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa" '
                 'target="_blank">:windows: (OpenJDK 8)</a>)\n' \
                 '* [:ubuntu: 18.04 (OpenJDK 11)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1" '
                 'target="_blank">:ubuntu: 18.04 (OpenJDK 11)</a>)\n' \
                 '\n'
                 'Questions? Please file an issue in ' \
                 'https://github.com/bazelbuild/continuous-integration\n' \
                 '\n' \
                 '**Important**: Please do NOT modify the issue title since that ' \
                 'might break our tools.\n',
         'created_at': '2019-12-1'}, \
        {'body': 'Incompatible flag --incompatible_no_implicit_file_export will break '
                 'TensorFlow once Bazel 1.2.1 is released.\n'
                 '\n' \
                 'Please see the following CI builds for more information:\n' \
                 '\n'
                 '* [:darwin: (OpenJDK 8)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7" '
                 'target="_blank">:darwin: (OpenJDK 8)</a>)\n'
                 '* [:windows: (OpenJDK 8)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa" '
                 'target="_blank">:windows: (OpenJDK 8)</a>)\n' \
                 '* [:ubuntu: 18.04 (OpenJDK 11)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1" ' \
                 'target="_blank">:ubuntu: 18.04 (OpenJDK 11)</a>)\n'
                 '\n'
                 'Questions? Please file an issue in '
                 'https://github.com/bazelbuild/continuous-integration\n' \
                 '\n'
                 '**Important**: Please do NOT modify the issue title since that '
                 'might break our tools.\n', \
         'created_at': '2019-12-1'},
        {'body': 'Incompatible flag --incompatible_load_python_rules_from_bzl will '
                 'break TensorFlow once Bazel 1.2.1 is released.\n'
                 '\n' \
                 'Please see the following CI builds for more information:\n' \
                 '\n' \
                 '* [:darwin: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7" '
                 'target="_blank">:darwin: (OpenJDK 8)</a>)\n'
                 '* [:windows: (OpenJDK 8)](<a '
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa" '
                 'target="_blank">:windows: (OpenJDK 8)</a>)\n' \
                 '* [:ubuntu: 18.04 (OpenJDK 11)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1" ' \
                 'target="_blank">:ubuntu: 18.04 (OpenJDK 11)</a>)\n' \
                 '\n'
                 'Questions? Please file an issue in '
                 'https://github.com/bazelbuild/continuous-integration\n' \
                 '\n'
                 '**Important**: Please do NOT modify the issue title since that '
                 'might break our tools.\n', \
         'created_at': '2019-12-1'}, \
        {'body': 'Incompatible flag --incompatible_disallow_empty_glob will break ' \
                 'TensorFlow once Bazel 1.2.1 is released.\n' \
                 '\n' \
                 'Please see the following CI builds for more information:\n' \
                 '\n'
                 '* [:darwin: (OpenJDK 8)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#042f4c72-2a23-4d7b-9d3b-d78619ea3fc7" '
                 'target="_blank">:darwin: (OpenJDK 8)</a>)\n'
                 '* [:windows: (OpenJDK 8)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#65246ccf-c676-40de-b693-3e1408052faa" ' \
                 'target="_blank">:windows: (OpenJDK 8)</a>)\n' \
                 '* [:ubuntu: 18.04 (OpenJDK 11)](<a ' \
                 'href="https://buildkite.com/bazel/bazelisk-plus-incompatible-flags/builds/342#42574ec2-097a-4fa6-9c82-5de0093a2ed1" '
                 'target="_blank">:ubuntu: 18.04 (OpenJDK 11)</a>)\n' \
                 '\n' \
                 'Questions? Please file an issue in '
                 'https://github.com/bazelbuild/continuous-integration\n'
                 '\n'
                 '**Important**: Please do NOT modify the issue title since that '
                 'might break our tools.\n',
         'created_at': '2019-12-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub ' \
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), ' \
                 'we only address code/doc bugs, performance issues, feature requests ' \
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n' \
                 '\r\n' \
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '16.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): pip install\r\n' \
                 '- TensorFlow version (use command below): 2.0.0\r\n'
                 '- Python version: 3.7.5\r\n'
                 '- Bazel version (if compiling from source):\r\n' \
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: 10.0\r\n'
                 '- GPU model and memory: Tesla V100-SXM2-16GB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'When creating a custom keras.layers.Layer, it is possible to add '
                 'weight in a lazy fashion through the add_weight() API. One of the '
                 'possible arguments is the variable name.\r\n'
                 '\r\n'
                 'According the documentation the name can be omitted, however if we '
                 'have a Variable with None name and we try to save the model using ' \
                 "the CheckpointManager API we can't generate a proper graph "
                 'taxonomy. Specifically, it is impossible to obtain the path prefix '
                 'for the custom variable since the ``_escape_local_name()`` function '
                 'in graph_view.py line 51 would rise a NoneType error.\r\n'
                 ' \r\n'
                 '**Describe the expected behavior**\r\n'
                 'The expected behaviour would rise an error when the custom layer is '
                 'build. In my opinion I would make the Variable name a mandatory '
                 'field instead of having a None value by default.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n'
                 'class CustomEmbedding(tf.keras.layers.Layer):\r\n' \
                 '    """Construct the embeddings from word, position and token_type '
                 'embeddings.\r\n'
                 '    """\r\n'
                 '\r\n'
                 '    def __init__(self, vocab_size, hidden_size,  '
                 'max_position_embeddings, **kwargs):\r\n'
                 '        super(CustomEmbedding, self).__init__(**kwargs)\r\n'
                 '        self.vocab_size = vocab_size\r\n' \
                 '        self.hidden_size = hidden_size\r\n' \
                 '        self.max_position_embeddings = max_position_embeddings\r\n' \
                 '        self.pad_idx = 0\r\n' \
                 '        self.max_position_embeddings += 1\r\n' \
                 '\r\n' \
                 '\r\n' \
                 '    def build(self, input_shape):\r\n'
                 '        """Build shared word embedding layer """\r\n' \
                 '        with tf.name_scope("position_embeddings"):\r\n'
                 '            self.position_embeddings = self.add_weight(\r\n' \
                 '                # Note that the name is missing\r\n'
                 '                shape=(self.max_position_embeddings, '
                 'self.hidden_size),\r\n'
                 '                initializer=positional_encoding,\r\n'
                 '                trainable=False,\r\n'
                 '                dtype=self.dtype)\r\n'
                 '\r\n'
                 '        with tf.name_scope("word_embeddings"):\r\n'
                 '            # Create and initialize weights. The random normal '
                 'initializer was chosen\r\n'
                 '            # arbitrarily, and works well.\r\n'
                 '            self.word_embeddings = self.add_weight(\r\n'
                 '                "weight",\r\n' \
                 '                shape=[self.vocab_size, self.hidden_size],\r\n'
                 '                ' \
                 'initializer=get_initializer(self.initializer_range),\r\n'
                 '                trainable=True,\r\n' \
                 '                dtype=self.dtype\r\n'
                 '            )\r\n'
                 '        super(CustomEmbedding, self).build(input_shape)\r\n'
                 '\r\n'
                 '    def call(self, inputs):\r\n' \
                 '        input_ids, position_ids = inputs\r\n'
                 '\r\n'
                 '        inputs_embeds = '
                 'tf.nn.embedding_lookup(self.word_embeddings, input_ids)\r\n'
                 '        position_embeddings = '
                 'tf.nn.embedding_lookup(self.position_embeddings, position_ids)\r\n' \
                 '\r\n'
                 '        embeddings = inputs_embeds + position_embeddings\r\n'
                 '        return embeddings\r\n' \
                 '```\r\n'
                 'Assuming to have a model with the over-defined CustomLayer. If we '
                 'try to save the model after some training epochs\r\n'
                 '```\r\n' \
                 'ckpt = tf.train.Checkpoint(model=model, optimizer=optim)\r\n'
                 'ckpt_manager = tf.train.CheckpointManager(ckpt,\r\n'
                 '    directory=path.join(args.ckpt_path, args.version),\r\n'
                 '    max_to_keep=args.max_ckp_to_keep)\r\n'
                 '\r\n'
                 'if ckpt_manager.latest_checkpoint:\r\n'
                 '    ckpt.restore(ckpt_manager.latest_checkpoint)\r\n'
                 "    print('Latest checkpoint restored!!')\r\n"
                 '\r\n'
                 '# training code\r\n'
                 '\r\n' \
                 'if self.val_accuracy.result() > best_model:\r\n'
                 '    best_model = self.test_accuracy.result()\r\n'
                 '    ckpt_save_path = ckpt_manager.save() # expected errors\r\n'
                 "    print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, "
                 'ckpt_save_path))\r\n'
                 '``` \r\n',
         'created_at': '2019-12-1'},
        {'body': '<em>Please make sure that this is a feature request. As per our '
                 '[GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), ' \
                 'we only address code/doc bugs, performance issues, feature requests ' \
                 'and build/installation issues on GitHub. '
                 'tag:feature_template</em>\r\n' \
                 '\r\n'
                 '\r\n' \
                 '**System information**\r\n'
                 '- TensorFlow version (you are using): 2.0.0\r\n' \
                 '- Are you willing to contribute it (Yes/No): Yes\r\n' \
                 '\r\n'
                 '\r\n' \
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 'Currently, whenever user loads keras models using HDF5, user gets '
                 'no confirmation that models were loaded successfully ' \
                 '([\\[1\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#load_weights), '
                 '[\\[2\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable#load_weights) ' \
                 'return nothing). This confirmation could be useful for verifying '
                 'that weights are properly loaded. In case of transfer learning, '
                 'users typically only want to load specific weights from the ' \
                 'HDF5file. This can be currently achieved by using '
                 "`load_weights(by_name=True)`, however, users don't get any " \
                 'confirmation about which layers were actually loaded.\r\n'
                 '\r\n'
                 'Also, in case there are no matching layers in between the original '
                 'model and source model, the `model.load_weights(by_name=True)` ' \
                 'fails without raising any exception so there is no way to actually ' \
                 "debug what went wrong with the model loading. (Note here that I'm "
                 'talking about name mismatch not weight mismatch) This significantly ' \
                 'affects the ability to write unit tests for models since the tester '
                 'code cannot actually verify what layers were loaded from HDF5 '
                 'file.\r\n'
                 '\r\n' \
                 '**Will this change the current api? How?**\r\n'
                 'Yes, this changes the current API for '
                 '[\\[1\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable#load_weights) '
                 'and '
                 '[\\[2\\]](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#load_weights) ' \
                 'by adding a return type to them. Specifically, we will be returning '
                 'a particular data structure when loading HDF5 files '
                 '[here](https://github.com/tensorflow/tensorflow/blob/c49396cf71dacc32195033507b3bbd985b12c255/tensorflow/python/keras/engine/network.py#L1131). '
                 'My current idea is to return the list of layers that were loaded '
                 "from HDF5 although I'm open to discussion about the return type.\r\n" \
                 '\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 'Users who are loading their weights from h5py files and want to '
                 'verify/check what layers were loaded from HDF5. \r\n'
                 '\r\n' \
                 '**Any Other info.**\r\n'
                 'N/A\r\n'
                 '\r\n' \
                 '\r\n'
                 "Let me know if any more clarification/information is needed. I'm "
                 'willing to contribute by working on this issue.',
         'created_at': '2019-12-1'}, \
        {'body': 'Updating tf.multiply with usage example, and description',
         'created_at': '2019-12-1'},

        {'body': 'I am also facing a similar issue.\r\n'
                 'The demo API gives error at tflite.load_delegate.\r\n' \
                 '\r\n'
                 '```\r\n'
                 'pi@bpi-iot-ros-ai:~/coral/tflite/python/examples/classification$ ' \
                 'python3 classify_image.py --model ' \
                 'models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite --labels ' \
                 'models/inat_bird_labels.txt --input images/parrot.jpg\r\n'
                 'E :248] HIB Error. hib_error_status = 0000000000000001, '
                 'hib_first_error_status = 0000000000000001\r\n'
                 'E :248] HIB Error. hib_error_status = 0000000000000001, '
                 'hib_first_error_status = 0000000000000001\r\n'
                 'INFO: Initialized TensorFlow Lite runtime.\r\n'
                 '----INFERENCE TIME----\r\n' \
                 'Note: The first inference on Edge TPU is slow because it includes '
                 'loading the model into Edge TPU memory.\r\n' \
                 '\r\n'
                 '\r\n' \
                 '\r\n' \
                 'pi@bpi-iot-ros-ai:~$ uname -a\r\n'
                 'Linux bpi-iot-ros-ai 5.4.0-bpi-r64 #1 SMP PREEMPT Mon Dec 16 '
                 '16:00:08 IST 2019 aarch64 aarch64 aarch64 GNU/Linux\r\n'
                 'pi@bpi-iot-ros-ai:~$ lscpu\r\n'
                 'Architecture:          aarch64\r\n' \
                 'Byte Order:            Little Endian\r\n'
                 'CPU(s):                2\r\n' \
                 'On-line CPU(s) list:   0,1\r\n' \
                 'Thread(s) per core:    1\r\n' \
                 'Core(s) per socket:    2\r\n'
                 'Socket(s):             1\r\n'
                 'CPU max MHz:           1350.0000\r\n'
                 'CPU min MHz:           30.0000\r\n'
                 'pi@bpi-iot-ros-ai:~$ ls -l /usr/lib/aarch64-linux-gnu/libedge*\r\n'
                 'lrwxrwxrwx 1 root root     17 Sep 17 04:27 '
                 '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1 -> libedgetpu.so.1.0\r\n'
                 '-rwxrwxrwx 1 root root 792376 Sep 17 04:27 ' \
                 '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\r\n'
                 'pi@bpi-iot-ros-ai:~$ lspci\r\n' \
                 '00:00.0 PCI bridge: MEDIATEK Corp. Device 3258\r\n'
                 '01:00.0 System peripheral: Device 1ac1:089a\r\n'
                 'pi@bpi-iot-ros-ai:~$ ls /dev/apex_0 \r\n'
                 '/dev/apex_0\r\n'
                 'pi@bpi-iot-ros-ai:~$ sudo sh -c "echo \'SUBSYSTEM==\\"apex\\", '
                 'MODE=\\"0660\\", GROUP=\\"apex\\"\' >> ' \
                 '/etc/udev/rules.d/65-apex.rules"\r\n'
                 'pi@bpi-iot-ros-ai:~$ sudo groupadd apex\r\n'
                 "groupadd: group 'apex' already exists\r\n" \
                 'pi@bpi-iot-ros-ai:~$ sudo adduser $USER apex\r\n'
                 "The user `pi' is already a member of `apex'.\r\n" \
                 '```\r\n',
         'created_at': '2019-12-1'},
        {'body': '@tensorflow/micro\r\n'
                 '\r\n' \
                 '**System information**\r\n' \
                 '- macOS 10.15 Catalina\r\n' \
                 '- TensorFlow installed from source \r\n'
                 '- Tensorflow version (commit SHA if '
                 'source):2ba0b2ef68d4259e8b02fa8be77a9372020b81b7 (Dec. 16)\r\n'
                 '- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ' \
                 'bluepill\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n' \
                 '```\r\n'
                 'arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g '
                 '-DTF_LITE_STATIC_MEMORY -fno-rtti '
                 '-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK -DTF_LITE_STATIC_MEMORY '
                 '-DTF_LITE_MCU_DEBUG_LOG -fno-rtti -fmessage-length=0 ' \
                 '-fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections ' \
                 '-fdata-sections -funsigned-char -MMD -mcpu=cortex-m3 -mthumb '
                 '-std=gnu++11 -Wvla -Wall -Wextra -Wno-unused-parameter '
                 '-Wno-missing-field-initializers -Wno-write-strings ' \
                 '-Wno-sign-compare -fno-delete-null-pointer-checks ' \
                 '-fomit-frame-pointer -fpermissive -nostdlib -g -Os -I. '
                 '-Itensorflow/lite/micro/tools/make/downloads/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/gemmlowp '
                 '-Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include '
                 '-isystemtensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/Core/Include/ '
                 '-Itensorflow/lite/micro/tools/make/downloads/stm32_bare_lib/include '
                 '-Itensorflow/lite/micro/tools/make/downloads/kissfft -o ' \
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/hello_world '
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/main.o ' \
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o '
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/sine_model_data.o ' \
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/output_handler.o '
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/constants.o  ' \
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a '
                 '-T tensorflow/lite/micro/tools/make/targets/bluepill/bluepill.lds '
                 '-Wl,-Map=tensorflow/lite/micro/tools/make/gen/bluepill.map,--cref '
                 '-Wl,--gc-sections -lm\r\n'
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o: '
                 "In function `setup()':\r\n" \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:47: '
                 "undefined reference to `__cxa_guard_acquire'\r\n" \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:47: ' \
                 "undefined reference to `__cxa_guard_release'\r\n"
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:63: '
                 "undefined reference to `__cxa_guard_acquire'\r\n" \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:63: ' \
                 "undefined reference to `__cxa_guard_release'\r\n" \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:67: '
                 "undefined reference to `__cxa_guard_acquire'\r\n"
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:67: ' \
                 "undefined reference to `__cxa_guard_release'\r\n" \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/examples/hello_world/main_functions.cc:83: '
                 "undefined reference to `__dso_handle'\r\n" \
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/lib/libtensorflow-microlite.a(greedy_memory_planner.o): ' \
                 'In function '
                 "`tflite::GreedyMemoryPlanner::~GreedyMemoryPlanner()':\r\n" \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/memory_planner/greedy_memory_planner.cc:70: ' \
                 "undefined reference to `operator delete(void*)'\r\n"
                 "`__lock___atexit_recursive_mutex' referenced in section "
                 "`.data.__atexit_recursive_mutex' of "
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-__call_atexit.o): '
                 "defined in discarded section `COMMON' of " \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/lib/thumb/v7-m/libc.a(lib_a-lock.o)\r\n'
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: ' \
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/hello_world: ' \
                 "hidden symbol `__dso_handle' isn't defined\r\n" \
                 '/Users/pinxue/projects/rt-thread/ai/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/../../../../arm-none-eabi/bin/ld: '
                 'final link failed: Bad value\r\n' \
                 'collect2: error: ld returned 1 exit status\r\n'
                 'gmake: *** '
                 '[tensorflow/lite/micro/examples/hello_world/Makefile.inc:42: ' \
                 'tensorflow/lite/micro/tools/make/gen/bluepill_cortex-m3/bin/hello_world] '
                 'Error 1\r\n' \
                 '```\r\n'
                 '\r\n'
                 '**Please provide the exact sequence of commands/steps when you ran '
                 'into the problem**\r\n' \
                 '\r\n'
                 '```\r\n' \
                 '$ /opt/local/bin/gmake -j1 -f ' \
                 'tensorflow/lite/micro/tools/make/Makefile TARGET=bluepill '
                 'hello_world\r\n'
                 '```\r\n',
         'created_at': '2019-12-1'},
        {'body': '<em>Please make sure that this is a bug. As per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), ' \
                 'we only address code/doc bugs, performance issues, feature requests ' \
                 'and build/installation issues on GitHub. tag:bug_template</em>\r\n'
                 '\r\n' \
                 '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): ' \
                 'https://github.com/cogaplex-bts/bts\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu '
                 '1604\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): pip\r\n'
                 '- TensorFlow version (use command below):1.13.2\r\n' \
                 '- Python version: 3.7\r\n' \
                 '- Bazel version (if compiling from source):\r\n' \
                 '- GCC/Compiler version (if compiling from source): pip \r\n' \
                 '- CUDA/cuDNN version: 10.0 \r\n'
                 '- GPU model and memory: 1060, 6G\r\n'
                 '\r\n' \
                 'You can collect some of this information using our environment '
                 'capture\r\n' \
                 '[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n'
                 'You can also obtain the TensorFlow version with: 1. TF 1.0: `python '
                 '-c "import\r\n'
                 'tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"` 2. TF 2.0: '
                 '`python -c\r\n' \
                 '"import tensorflow as tf; print(tf.version.GIT_VERSION, '
                 'tf.version.VERSION)"`\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'in a process, use full path , twice load:\r\n'
                 'tf.load_op_library(os.path.join(dname, '
                 "'build/libcompute_depth.so'))\r\n"
                 'the second time, OP_LIST is empty, \r\n'
                 'but with relative path, twice  tf.load_op_library not empty\r\n'
                 '**Describe the expected behavior**\r\n'
                 'use full path, twice  tf.load_op_library, OP_LIST should not '
                 'empty\r\n' \
                 '**Code to reproduce the issue**\r\n'
                 'Provide a reproducible test case that is the bare minimum necessary ' \
                 'to generate the problem.\r\n'
                 'I compile https://github.com/cogaplex-bts/bts, met this problem. ' \
                 'But I think this is a general problem, reproduce: just in a python '
                 'file, load the library twice, with full path and relative '
                 'path   \r\n'
                 '**Other info / logs**\r\n' \
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n'
                 '\r\n'
                 '```\r\n' \
                 "compute_depth_grad_module dict: {'__name__': "
                 "'a11935c229913616b7b14d8da52f01ac', '__doc__': 'Python wrappers " \
                 'around TensorFlow ops.\\n\\nThis file is MACHINE GENERATED! Do not ' \
                 "edit.\\n', ...  , 'LIB_HANDLE': <Swig Object of type 'TF_Library *' "
                 "at 0x7ff4ad011f60>, 'OP_LIST': op {\r\n"
                 '  name: "ComputeDepth"\r\n' \
                 '  input_arg {\r\n' \
                 '    name: "input"\r\n' \
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n'
                 '  input_arg {\r\n'
                 '    name: "focal"\r\n'
                 '    type: DT_FLOAT\r\n'
                 '  }\r\n'
                 '  output_arg {\r\n' \
                 '    name: "depth"\r\n' \
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n'
                 '  attr {\r\n'
                 '    name: "upratio"\r\n'
                 '    type: "int"\r\n'
                 '  }\r\n' \
                 '}\r\n'
                 'op {\r\n' \
                 '  name: "ComputeDepthGrad"\r\n' \
                 '  input_arg {\r\n' \
                 '    name: "depth_grad"\r\n'
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n'
                 '  input_arg {\r\n' \
                 '    name: "input"\r\n' \
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n' \
                 '  input_arg {\r\n' \
                 '    name: "focal"\r\n' \
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n'
                 '  output_arg {\r\n' \
                 '    name: "grad_input"\r\n' \
                 '    type: DT_FLOAT\r\n'
                 '  }\r\n'
                 '  output_arg {\r\n'
                 '    name: "grad_focal"\r\n'
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n' \
                 '}\r\n'
                 '}\r\n'
                 "{'__name__': 'a11935c229913616b7b14d8da52f01ac', '__doc__': 'Python "
                 'wrappers around TensorFlow ops.\\n\\nThis file is MACHINE '
                 "GENERATED! Do not edit.\\n', '__package__': None,  ... , "
                 "'LIB_HANDLE': <Swig Object of type 'TF_Library *' at "
                 "0x7ff4ad011f60>, 'OP_LIST': op {\r\n"
                 '  name: "ComputeDepth"\r\n' \
                 '  input_arg {\r\n'
                 '    name: "input"\r\n'
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n'
                 '  input_arg {\r\n'
                 '    name: "focal"\r\n'
                 '    type: DT_FLOAT\r\n'
                 '  }\r\n'
                 '  output_arg {\r\n' \
                 '    name: "depth"\r\n'
                 '    type: DT_FLOAT\r\n'
                 '  }\r\n'
                 '  attr {\r\n'
                 '    name: "upratio"\r\n'
                 '    type: "int"\r\n'
                 '  }\r\n'
                 '}\r\n' \
                 'op {\r\n' \
                 '  name: "ComputeDepthGrad"\r\n'
                 '  input_arg {\r\n' \
                 '    name: "depth_grad"\r\n'
                 '    type: DT_FLOAT\r\n'
                 '  }\r\n' \
                 '  input_arg {\r\n'
                 '    name: "input"\r\n'
                 '    type: DT_FLOAT\r\n'
                 '  }\r\n'
                 '  input_arg {\r\n'
                 '    name: "focal"\r\n'
                 '    type: DT_FLOAT\r\n'
                 '  }\r\n'
                 '  output_arg {\r\n'
                 '    name: "grad_input"\r\n'
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n'
                 '  output_arg {\r\n'
                 '    name: "grad_focal"\r\n'
                 '    type: DT_FLOAT\r\n' \
                 '  }\r\n'
                 '}\r\n'
                 '}\r\n'
                 '```\r\n'
                 '\r\n'
                 'but with full name, second will print:\r\n'
                 '```\r\n'
                 "compute_depth_grad_module dict: {'__name__': "
                 "'670cc8cfec5b6d3b8635f39bd583d769', '__doc__': 'Python wrappers " \
                 'around TensorFlow ops.\\n\\nThis file is MACHINE GENERATED! Do not '
                 "edit.\\n', '__package__': None, ... , 'LIB_HANDLE': <Swig Object of "
                 "type 'TF_Library *' at 0x7f6b2a8f3e70>, 'OP_LIST': }\r\n"
                 '```\r\n',
         'created_at': '2019-12-1'},
        {'body': '### System information\r\n'
                 '- **Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow)**: No\r\n'
                 '- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ' \
                 'Ubuntu 18.04\r\n'
                 '- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device**: Huawei\r\n'
                 '- **TensorFlow installed from (source or binary)**: Source\r\n' \
                 '- **TensorFlow version (use command below)**: 2.0\r\n'
                 '- **Python version**: 3.6\r\n'
                 '- **Bazel version (if compiling from source)**: \r\n'
                 '- **GCC/Compiler version (if compiling from source)**: \r\n'
                 '- **CUDA/cuDNN version**: 10.2\r\n'
                 '- **GPU model and memory**: GTX2080TI\r\n'
                 '- **Exact command to reproduce**:\r\n'
                 '\r\n'
                 '### Describe the problem\r\n'
                 'This is a feature request. I am struggling to make a TFLITE model '
                 'that works perfectly in Python to  also work in and Android apk. It '
                 'seems I have problems related with the conversion from Bitmap to a ' \
                 "bytebuffer, std, mean and that stuff (I've been using old Android "
                 'examples that made the conversion Bitmap -> Bytebuffer '
                 'manually). \r\n' \
                 '\r\n'
                 "Now I've seen this new TensorImage feature in "
                 'tensorflow-lite-support and tried it, but it seems it only supports '
                 'loading RGB images. My model is MNIST-based, so the input shape of '
                 "the images are (28, 28, 1). I don't see how I can use TensorImage "
                 'to feed my model in this scenario.\r\n'
                 '\r\n' \
                 'Can you please support grayscale (1 channel) images also '
                 'please? \r\n' \
                 '\r\n'
                 'Thanks,\r\n'
                 '\r\n'
                 '### Source code / logs\r\n'
                 '\r\n',
         'created_at': '2019-12-1'},
        {'body': 'Object detection SSD trained and tested to produce expected results '
                 'in PC. \r\n' \
                 'http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz\r\n'
                 'https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_inception_v2_pets.config\r\n'
                 '\r\n'
                 'Model works well and produces accurate results on PC. \r\n'
                 'After converting to tflite and loading in mobile, the camera view '
                 'shows incorrect detection (several detection of same class and in ' \
                 'fixed place even when moving the camera).', \
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS '
                 '10.15.1\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n' \
                 '- TensorFlow version (or github SHA if from source): '
                 'tf-nightly==2.1.0.dev20191203\r\n'
                 '\r\n' \
                 '**Command used to run the converter or code if you’re using the ' \
                 'Python API**\r\n'
                 '\r\n'
                 '```\r\n' \
                 'import pathlib\r\n'
                 '\r\n'
                 'inpt = tf.keras.layers.Input(shape=[256, 256, 3])\r\n'
                 'out = tf.keras.layers.Lambda(lambda x: '
                 'tf.keras.activations.softmax(x))(inpt)\r\n' \
                 'out = tf.keras.layers.Lambda(lambda x: tf.nn.softmax(x))(out)\r\n' \
                 'model = tf.keras.Model(inpt, out)\r\n'
                 '\r\n'
                 'converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n'
                 'tflite_model = converter.convert()\r\n'
                 "pathlib.Path('out.tflite').write_bytes(tflite_model)\r\n"
                 '```\r\n' \
                 '\r\n'
                 '**Failure details**\r\n' \
                 '![image](https://user-images.githubusercontent.com/1422280/71025023-3904e880-20d4-11ea-95fb-29cfea49a44d.png)\r\n'
                 'This graph shows the difference between the different softmax ' \
                 'methods.  When using `tf.keras.activations.softmax`, there is '
                 '[code](https://github.com/tensorflow/tensorflow/blob/v2.1.0-rc1/tensorflow/python/keras/activations.py#L43-L79) '
                 'with a workaround for multiple dimensions.  It looks like this was ' \
                 'written before the tensorflow op had multi-dimension support. ',
         'created_at': '2019-12-1'}, \
        {'body': 'This PR is trying to resolve the issue in #35179\r\n'
                 'where boringssl was not the latest version. The old version\r\n'
                 '7f63442 was released one and half year ago so it is time to '
                 'update.\r\n'
                 '\r\n'
                 'This PR updates boringssl to the latest 80ca9f9.\r\n'
                 '\r\n'
                 'This PR fixes #35179.\r\n'
                 '\r\n'
                 'Signed-off-by: Yong Tang <yong.tang.github@outlook.com>',
         'created_at': '2019-12-1'},
        {'body': '### System information\r\n'
                 '- **Have I written custom code (as opposed to using a stock example ' \
                 'script provided in TensorFlow)**: Yes\r\n'
                 '- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: '
                 'Google Colab (GPU)\r\n'
                 '- **TensorFlow installed from (source or binary)**: Binary\r\n'
                 '- **TensorFlow version (use command below)**: 2.1.0-dev20191217 \r\n'
                 '- **Python version**: 3\r\n'
                 '- **Exact command to reproduce**:\r\n' \
                 '\r\n'
                 '```bash\r\n'
                 '!pip install tf-nightly\r\n' \
                 'import tensorflow as tf\r\n'
                 'from tensorflow.keras import Sequential\r\n'
                 'from tensorflow.keras.layers import GRU, Dense, Dropout\r\n'
                 '\r\n'
                 'model = Sequential()\r\n' \
                 "model.add(GRU(100, activation='relu', return_sequences=False, "
                 'input_shape=(128,2)))\r\n'
                 'model.add(Dropout(0.2))\r\n'
                 "model.add(Dense(11, activation='softmax'))\r\n" \
                 '\r\n' \
                 'converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n'
                 'converter.experimental_new_converter = True\r\n' \
                 'tflite_model = converter.convert()\r\n' \
                 'converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n'
                 'tflite_model_quant = converter.convert()\r\n'
                 '```\r\n' \
                 '\r\n'
                 '### Error message\r\n'
                 '```bash\r\n'
                 'E tensorflow/lite/tools/optimize/quantize_weights.cc:351] Quantize ' \
                 'weights tool only supports tflite models with one subgraph.\r\n'
                 '```\r\n'
                 '\r\n'
                 '### Describe the problem\r\n' \
                 'First, I used the new converter (with the experimental flag '
                 'converter.experimental_new_converter = True) to convert an RNN '
                 "model from TensorFlow to TensorFlow Lite's flat buffer format, as "
                 'suggested in the issue #32608.\r\n'
                 '\r\n'
                 'This works correctly, but then when I try to perform a ' \
                 'post-training weight quantization, I got an error saying that the '
                 'quantize weights tool only supports tflite models with one '
                 'subgraph. \r\n'
                 '\r\n' \
                 'Is there a problem with my procedure? Or is that feature not yet '
                 'supported? In that case, I would like to request this feature.\r\n'
                 '\r\n' \
                 'Thanks in advance for your help.\r\n'
                 '\r\n'
                 '### Source code / logs\r\n' \
                 'The attached file can be used to reproduce the error with a trained '
                 'model (.h5). \r\n'
                 '[GRU_1L.zip](https://github.com/tensorflow/tensorflow/files/3974453/GRU_1L.zip)\r\n'
                 '\r\n' \
                 '\r\n',
         'created_at': '2019-12-1'},
        {'body': 'Add TfLite micro ExpandDims reference kernel and tests (int8, ' \
                 'uint8, float32)\r\n'
                 '\r\n'
                 'Signed-off-by: SiCongLi <sicong.li@arm.com>',
         'created_at': '2019-12-1'},
        {'body': 'Add TfLite micro Squeeze reference kernel and tests (uint8, int8, '
                 'float32).\r\n'
                 '\r\n'
                 'Signed-off-by: SiCongLi <sicong.li@arm.com>', \
         'created_at': '2019-12-1'},
        {'body': 'Dear all,\r\n'
                 ' I have a point about DropoutWrapper and its use with Recurrent '
                 'Neural Networks.\r\n'
                 '\r\n'
                 'Due to the possibility that the dropout can be applied to the state '
                 'or the output (state_keep_prob and output_keep_prob), I found that, '
                 'during the recurrent process, the state propagated through the time '
                 'can take values not bounded in the interval [-1, 1]. This is ' \
                 'probably due to the way in which the dropout is implemented (at '
                 'training time with a scaling instead of testing time with '
                 'expectation). Since the dropout is applied after the activation '
                 '(i.e. tanh), the feature values will range between -inf and +inf. '
                 'This point is a bit strange for me since the current implementation '
                 'can induce exploding gradient issues in the GRU/LSTM process while '
                 'such cells were introduced to deal with vanishing as well as '
                 'exploding gradients.\r\n'
                 '\r\n'
                 'Please, could you supply me some feedback about my issue since, ' \
                 'practically, it can impact people that commonly employ such Wrapper '
                 'that induces behaviours that are divergent w.r.t. the theoretical '
                 'behaviour of RNN (GRU/LSTM).\r\n'
                 '\r\n'
                 'All the best\r\n',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n' \
                 '- OS Platform and Distribution: Windows 7 Professional SP1 64 '
                 'bit\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: tensorflow-master (downloaded 18/12/2019)\r\n'
                 '- Python version: 3.7\r\n'
                 '- Installed using virtualenv? pip? conda?: YES, conda\r\n' \
                 '- Bazel version (if compiling from source): 1.1.0\r\n'
                 '- GCC/Compiler version (if compiling from source): MSVC 14.16.27023 '
                 '(Visual Studio 2017)\r\n'
                 '- CUDA/cuDNN version: build for CPU\r\n'
                 '- GPU model and memory: (Quadro K200M)\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'Followed this guide: '
                 'https://www.tensorflow.org/install/source_windows and just build '
                 'failed.\r\n' \
                 'I tested different combinations '
                 '(https://www.tensorflow.org/install/source_windows#cpu) and I was '
                 'just able to compile just the **r1.14 successfully**, by the '
                 'way.\r\n'
                 '**r2.0** failed with the same message as master.\r\n'
                 'But back to master release.\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n' \
                 '\r\n'
                 '- Donwloaded the `tensorflow-master.zip` from github\r\n' \
                 '- Installed Bazel version 1.1.0\r\n'
                 '- cmd to folder `\\tensorflow-master`\r\n'
                 '- `activate python37 environment` (conda)\r\n'
                 '- Set env var for bazel: `set BAZEL_VC=C:\\Program Files '
                 '(x86)\\Microsoft Visual Studio\\2017\\Community\\VC`\r\n'
                 '- Check Bazel version\r\n' \
                 '```\r\n'
                 '(python37) C:\\Users\\username\\bin\\tensorflow-master>bazel '
                 '--version\r\n'
                 'bazel 1.1.0\r\n'
                 '```\r\n'
                 '- Configure\r\n'
                 '```\r\n'
                 '(python37) C:\\Users\\ username\\bin\\tensorflow-master>python ' \
                 './configure.py\r\n'
                 'You have bazel 1.1.0 installed.\r\n'
                 'Please specify the location of python. [Default is C:\\Users\\ '
                 'username\\Anaconda3\\envs\\python37\\python.exe]:\r\n'
                 '\r\n'
                 'Found possible Python library paths:\r\n'
                 '  C:\\Users\\ ' \
                 'username\\Anaconda3\\envs\\python37\\lib\\site-packages\r\n'
                 'Please input the desired Python library path to use.  Default is '
                 '[C:\\Users\\ '
                 'username\\Anaconda3\\envs\\python37\\lib\\site-packages]\r\n' \
                 '\r\n' \
                 'Do you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\n'
                 'No XLA JIT support will be enabled for TensorFlow.\r\n' \
                 '\r\n'
                 'Do you wish to build TensorFlow with ROCm support? [y/N]: n\r\n'
                 'No ROCm support will be enabled for TensorFlow.\r\n'
                 '\r\n' \
                 'Do you wish to build TensorFlow with CUDA support? [y/N]: n\r\n'
                 'No CUDA support will be enabled for TensorFlow.\r\n'
                 '\r\n'
                 'Please specify optimization flags to use during compilation when '
                 'bazel option "--config=opt" is specified [Default is /arch:AVX]:\r\n' \
                 '\r\n'
                 'Would you like to override eigen strong inline for some C++ ' \
                 'compilation to reduce the compilation time? [Y/n]: y\r\n' \
                 'Eigen strong inline overridden.\r\n'
                 '```\r\n' \
                 '- Launch the compilation: `bazel build '
                 '//tensorflow:tensorflow_cc.lib`\r\n'
                 '- After time elapsed the process ended with: `FAILED: Build did NOT '
                 'complete successfully`\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Just attaching the last rows of the terminal messages\r\n'
                 '\r\n'
                 "Message has Italian chunks of which I'm trying to provide a "
                 'translation here:\r\n'
                 '```\r\n'
                 "note: vedere il riferimento all'istanza '<Sconosciuto>' della "
                 'funzione <Sconosciuto> di cui è in corso la compilazione\r\n'
                 "note: check the reference to instance '<Unknown>' of the function " \
                 '<Unknown> which is compiling\r\n' \
                 '```\r\n' \
                 '\r\n'
                 '```\r\n'
                 'Errore interno del compilatore in C:\\Program Files '
                 '(x86)\\Microsoft Visual '
                 'Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe. '
                 'Verrà richiesto di inviare una segnalazione errori a Microsoft in '
                 'un momento successivo.\r\n'
                 'Internal error of the compiler in C:\\Program Files '
                 '(x86)\\Microsoft Visual '
                 'Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe. '
                 'It will be required to send an error report to Microsoft next.\r\n'
                 '```\r\n' \
                 '\r\n' \
                 '```\r\n'
                 "ERRORE INTERNO DEL COMPILATORE in 'C:\\Program Files "
                 '(x86)\\Microsoft Visual '
                 "Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe' " \
                 'Per altre informazioni, scegliere Supporto tecnico dal menu ? di '
                 'Visual C++ o aprire il file della Guida relativo al supporto ' \
                 'tecnico\r\n'
                 "INTERNAL ERROR OF THE COMPILER in 'C:\\Program Files " \
                 '(x86)\\Microsoft Visual ' \
                 "Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\bin\\HostX64\\x64\\cl.exe' "
                 'For more information, choose Technical support from ? menu of ' \
                 'Visual C++ or open the file from the guide related with technical '
                 'support\r\n' \
                 '```\r\n'
                 '\r\n'
                 '[tensorflow-master-build-fail-log.txt](https://github.com/tensorflow/tensorflow/files/3971998/tensorflow-master-build-fail-log.txt)\r\n'
                 '\r\n', \
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- OS Platform: Windows10\r\n'
                 '- TensorFlow installed from:source\r\n' \
                 '- TensorFlow version:1.13.0\r\n' \
                 '- Python version: NO\r\n' \
                 '- Bazel version (if compiling from source):0.19\r\n' \
                 '- GCC/Compiler version: msvc14\r\n'
                 '\r\n'
                 '\r\n' \
                 '\r\n'
                 '**Describe the problem**\r\n' \
                 'I want to build tensorflow from the source using the below ' \
                 'command \r\n'
                 '`bazel build -c opt  --config=mkl --config=monolithic '
                 '--linkopt="/FORCE:MULTIPLE" --define=no_tensorflow_py_deps=true ' \
                 '//tensorflow:libtensorflow_cc.so //tensorflow:install_headers   '
                 '//tensorflow/tools/lib_package:libtensorflow --verbose_failures`\r\n' \
                 '\r\n'
                 'However after the dll file generated, some symbol is not inside of '
                 'the dll, I will paste some of this error in here(As far as I know '
                 'this problem is accured because number of symbol is limited in '
                 "windows, So all functions in tensorflow source can't be exported( I "
                 "_check the exported function using dumpbin.exe /EXPORT and it's "
                 'weird because only 3000 symbols is exported )_\r\n' \
                 '`tfwrapper.obj : error LNK2001: unresolved external symbol "class '
                 'tensorflow::Output __cdecl tensorflow::ops::Const(class ' \
                 'tensorflow::Scope const &,struct tensorflow::Input::Initializer '
                 'const &)" ' \
                 '(?Const@ops@tensorflow@@YA?AVOutput@2@AEBVScope@2@AEBUInitializer@Input@2@@Z)`\r\n'
                 '\r\n'
                 '\r\n' \
                 '`tfwrapper.obj : error LNK2001: unresolved external symbol "public: '
                 '__cdecl tensorflow::ops::DecodePng::DecodePng(class '
                 'tensorflow::Scope const &,class tensorflow::Input,struct ' \
                 'tensorflow::ops::DecodePng::Attrs const &)" '
                 '(??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z)`\r\n'
                 '\r\n' \
                 '\r\n' \
                 '`tfwrapper.obj : error LNK2001: unresolved external symbol "public: ' \
                 '__cdecl tensorflow::ops::Placeholder::Placeholder(class '
                 'tensorflow::Scope const &,enum tensorflow::DataType)" ' \
                 '(??0Placeholder@ops@tensorflow@@QEAA@AEBVScope@2@W4DataType@2@@Z)`\r\n'
                 '\r\n'
                 'and for solving this issue I work on two idea\r\n'
                 ' **first one** is using `def_file_filter.py.tpl`, as may you know '
                 'in this file I have to add missed symbol to it like below(I just '
                 'paste some of them in here) and reconfigure & recompile the '
                 "source(I guess). But this solution can't export any of this missing "
                 'symbol for me\r\n'
                 '\r\n'
                 '     # Header for the def file.\r\n'
                 '     def_fp.write("LIBRARY " + args.target + "\\n")\r\n' \
                 '    def_fp.write("EXPORTS\\n")\r\n'
                 '    def_fp.write("\\t ??1OpDef@tensorflow@@UEAA@XZ\\n")\r\n'
                 '\tdef_fp.write("\\t ' \
                 '??0DecodePng@ops@tensorflow@@QEAA@AEBVScope@2@VInput@2@AEBUAttrs@012@@Z\\n")\r\n' \
                 '\tdef_fp.write("\\t '
                 '??6tensorflow@@YAAEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@AEAV12@AEBVStatus@0@@Z\\n")`\r\n' \
                 '\r\n'
                 '\r\n' \
                 "So, I can't solve my problem using this method. So I use another " \
                 'Idea that I got this Idea from `ashley tharp` (you can see her '
                 'great work in this[ '
                 'link](https://github.com/robosina/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows)) ' \
                 'and this workaround solve 8 of this problems. Main idea of her is '
                 'to add `TF_EXPORT` macro to the function names to force symbol to ' \
                 'include in the dll file. but my problem is raised in the DecodePng ' \
                 'file, there is a function in tensorflow source ' \
                 '->`tensorflow/core/kernels/decode_image_op.cc` the function ' \
                 'signature is \r\n' \
                 '     \r\n'
                 '     void DecodePng(OpKernelContext* context, StringPiece input)\r\n' \
                 " I can't find any other. But in examples of the tensorflow we will "
                 'see this way to read png files\r\n'
                 '\r\n'
                 '    image_reader = DecodePng(root.WithOpName("png_reader"), ' \
                 'file_reader,\r\n'
                 '                             '
                 'DecodePng::Channels(wanted_channels));\r\n'
                 "this two function is not same and I couldn't find this function in " \
                 "the tensorflow source code so I can't add `TF_EXPORT` macro to it, "
                 'but how it is possible to use a function that is not present in ' \
                 'source code?? So I look in the bazel genereated files and this '
                 'function is in there.(it is in '
                 '`bazel-source/bazel-out/x64_windows-opt/genfiles/tensorflow/cc/ops/image_ops.h`)\r\n' \
                 '\r\n'
                 '     DecodePng(const ::tensorflow::Scope& scope, ' \
                 '::tensorflow::Input contents);\r\n' \
                 '     DecodePng(const ::tensorflow::Scope& scope, '
                 '::tensorflow::Input contents, const\r\n'
                 '          DecodePng::Attrs& attrs);\r\n'
                 '\r\n' \
                 " I don't know exactly how bazel created it and how to add "
                 '`TF_EXPORT` macro to it(because this files is machine generated '
                 "files and you can't add TF_EXPORT to it, because it will be "
                 'overwritten in compile time)',
         'created_at': '2019-12-1'},
        {'body': 'I run my c++ code, it load file .pb have op ' \
                 'tf.contrib.image.transform() and out error:\r\n'
                 '```\r\n' \
                 "Not found: Op type not registered 'ImageProjectiveTransformV2' in "
                 'binary running on mendel. Make sure the Op and Kernel are '
                 'registered in the binary running in this process. Note that if you '
                 'are loading a saved graph which used ops from tf.contrib, accessing ' \
                 '(e.g.) `tf.contrib.resampler` should be done before importing the '
                 'graph, as contrib ops are lazily registered when the module is '
                 'first accessed.\r\n' \
                 '```\r\n'
                 '\r\n' \
                 'I use tensorflow 1.14 build from source on ubuntu16.04.\r\n' \
                 'How to build to use op ImageProjectiveTransformV2 ?',
         'created_at': '2019-12-1'},
        {'body': '### **System information**\r\n'
                 '- OS Platform and Distribution: Linux Ubuntu 16.04\r\n'
                 '- Mobile device: LG G7\r\n' \
                 '- TensorFlow installed from (source or binary): Source\r\n'
                 '- TensorFlow version: r2.0\r\n'
                 '- Python version: 3.5\r\n' \
                 '- Bazel version: 0.24.1\r\n'
                 '- Android NDK version: r17c\r\n' \
                 '- GCC/Compiler version (if compiling from source): GCC 4.8\r\n' \
                 '\r\n' \
                 '\r\n'
                 '\r\n' \
                 '### **Describe the problem**\r\n'
                 'Built Tensorflow lite from source, but the inference is about 3 ' \
                 'times slower than original.\r\n'
                 "(What I mean by 'original' is the prebuilt tensorflow-lite " \
                 'downloaded from Maven repository.)\r\n'
                 'For example, the model which originally costs 20 ms on GPU takes 50 '
                 'ms.\r\n' \
                 'I tried different versions of Tensorflow and Android NDK but '
                 'resulted all same.\r\n' \
                 '\r\n'
                 "Below is the process I've done to build tensorflow lite " \
                 'libraries.\r\n' \
                 '\r\n'
                 '\r\n' \
                 '**1. Download Tensorflow source code from github**\r\n'
                 '```shell\r\n'
                 '$ git clone https://github.com/tensorflow/tensorflow.git\r\n'
                 '$ git checkout r2.0\r\n' \
                 '```\r\n' \
                 '\r\n'
                 '**2. Download Android NDK and install standalone toolchains**\r\n'
                 'Download NDK r17c from ' \
                 '[here](https://developer.android.com/ndk/downloads/older_releases.html).\r\n'
                 'Extract files and move to Android Sdk directory.\r\n' \
                 '```shell\r\n'
                 '$ cd Android/Sdk/android-ndk-r17c\r\n'
                 '$ python build/tools/make_standalone_toolchain.py --arch arm64 '
                 '--api 21\r\n'
                 '$ export ANDROID_NDK=/path/to/ndk\r\n'
                 '$ export ANDROID_NDK_HOME=$ANDROID_NDK\r\n'
                 '```\r\n' \
                 '\r\n'
                 '**3. Download and install Bazel**\r\n' \
                 'Download installer script from ' \
                 '[here](https://github.com/bazelbuild/bazel/releases/0.24.1).\r\n' \
                 '```shell\r\n'
                 '$ cd /path/to/download\r\n'
                 '$ chmod +x bazel-0.24.1-installer-linux-x86_64.sh\r\n'
                 '$ ./bazel-0.24.1-installer-linux-x86_64.sh --user\r\n' \
                 '```\r\n'
                 '\r\n'
                 '**4. Build tensorflow-lite.aar and tensorflow-lite-gpu.aar**\r\n' \
                 '```shell\r\n'
                 '$ cd tensorflow\r\n'
                 '$ ./configure\r\n' \
                 "$ bazel build --cxxopt='--std=c++11' -c opt " \
                 '--fat_apk_cpu=arm64-v8a,armeabi-v7a '
                 'tensorflow/lite/java:tensorflow-lite\r\n' \
                 "$ bazel build --cxxopt='--std=c++11' -c opt " \
                 '--fat_apk_cpu=arm64-v8a,armeabi-v7a ' \
                 'tensorflow/lite/java:tensorflow-lite-gpu\r\n' \
                 '```\r\n'
                 "On `./configure`, set all 'n' for support question and leave others " \
                 'on default.\r\n'
                 '\r\n'
                 '| Configuration | Value |\r\n'
                 '| --- | --- |\r\n'
                 '| XLA JIT support | No |\r\n'
                 '| OpenCL SYCL support | No |\r\n'
                 '| ROCm support | No |\r\n' \
                 '| CUDA support | No |\r\n'
                 '| Fresh released clang | No |\r\n' \
                 '| MPI support | No |\r\n'
                 '| Bazel comilation option | Default |\r\n'
                 '| Android ./WORKSPACE configuration | Yes |\r\n'
                 '| Android NDK API level | 22 |\r\n'
                 '| Android SDK path | Default |\r\n'
                 '| Android SDK API level | Default (29) |\r\n'
                 '| Android build tools version | Default (29.0.2) |\r\n' \
                 '\r\n' \
                 '**5. Import libraries to Android project**\r\n' \
                 'Copy the generated `.aar` files into Android project, and change ' \
                 '`build.gradle` file:\r\n'
                 '\r\n'
                 '```gradle\r\n'
                 'dependencies {\r\n'
                 '    ...\r\n' \
                 "    implementation files('libs/tensorflow-lite.aar')\r\n" \
                 "    implementation files('libs/tensorflow-lite-gpu.aar')\r\n"
                 '\r\n' \
                 "    // implementation 'org.tensorflow:tensorflow-lite:2.0.0'\r\n" \
                 "    // implementation 'org.tensorflow:tensorflow-lite-gpu:2.0.0'\r\n" \
                 '}\r\n'
                 '```\r\n' \
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 'Any suggestion or help will be appreciated.\r\n' \
                 'Thanks in advance.',
         'created_at': '2019-12-1'}, \
        {'body': '@tensorflow/micro\r\n'
                 '\r\n'
                 '**System information**\r\n' \
                 '- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ' \
                 'Linux Ubuntu 18.04\r\n'
                 '- TensorFlow installed from (source or binary): `pip install '
                 'tensorflow`\r\n'
                 '- Tensorflow version (commit SHA if source): 1.15.0\r\n' \
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'The example provided '
                 '[here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/examples/lstm/g3doc) ' \
                 "doesn't work.\r\n" \
                 '**Please provide the exact sequence of commands/steps when you ran '
                 'into the problem**\r\n'
                 'If I put together the example, it gives this : \r\n'
                 '```\r\n'
                 '# Note this needs to happen before import tensorflow.\r\n' \
                 'import os\r\n' \
                 "os.environ['TF_ENABLE_CONTROL_FLOW_V2'] = '1'\r\n"
                 'import sys\r\n' \
                 'from absl import app\r\n'
                 'import argparse\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 '\r\n'
                 'class MnistLstmModel(object):\r\n'
                 '  """Build a simple LSTM based MNIST model.\r\n'
                 '\r\n'
                 '  Attributes:\r\n'
                 '    time_steps: The maximum length of the time_steps, but since '
                 "we're just using\r\n"
                 "      the 'width' dimension as time_steps, it's actually a fixed "
                 'number.\r\n' \
                 '    input_size: The LSTM layer input size.\r\n'
                 '    num_lstm_layer: Number of LSTM layers for the stacked LSTM cell ' \
                 'case.\r\n'
                 '    num_lstm_units: Number of units in the LSTM cell.\r\n'
                 '    units: The units for the last layer.\r\n'
                 '    num_class: Number of classes to predict.\r\n'
                 '  """\r\n'
                 '\r\n'
                 '  def __init__(self, time_steps, input_size, num_lstm_layer, '
                 'num_lstm_units,\r\n'
                 '               units, num_class):\r\n'
                 '    self.time_steps = time_steps\r\n'
                 '    self.input_size = input_size\r\n'
                 '    self.num_lstm_layer = num_lstm_layer\r\n'
                 '    self.num_lstm_units = num_lstm_units\r\n'
                 '    self.units = units\r\n'
                 '    self.num_class = num_class\r\n' \
                 '\r\n'
                 '  def build_model(self):\r\n'
                 '    """Build the model using the given configs.\r\n'
                 '\r\n'
                 '    Returns:\r\n'
                 '      x: The input placehoder tensor.\r\n'
                 '      logits: The logits of the output.\r\n'
                 '      output_class: The prediction.\r\n' \
                 '    """\r\n'
                 '    x = tf.placeholder(\r\n'
                 "        'float32', [None, self.time_steps, self.input_size], "
                 "name='INPUT')\r\n"
                 '    lstm_layers = []\r\n'
                 '    for _ in range(self.num_lstm_layer):\r\n'
                 '      lstm_layers.append(\r\n'
                 '          # Important:\r\n'
                 '          #\r\n'
                 '          # Note here, we use ' \
                 '`tf.lite.experimental.nn.TFLiteLSTMCell`\r\n'
                 '          # (OpHinted LSTMCell).\r\n'
                 '          tf.lite.experimental.nn.TFLiteLSTMCell(\r\n'
                 '              self.num_lstm_units, forget_bias=0))\r\n'
                 '    # Weights and biases for output softmax layer.\r\n' \
                 '    out_weights = tf.Variable(tf.random.normal([self.units, '
                 'self.num_class]))\r\n'
                 '    out_bias = tf.Variable(tf.zeros([self.num_class]))\r\n'
                 '\r\n'
                 '    # Transpose input x to make it time major.\r\n'
                 '    lstm_inputs = tf.transpose(x, perm=[1, 0, 2])\r\n'
                 '    lstm_cells = tf.keras.layers.StackedRNNCells(lstm_layers)\r\n' \
                 '    # Important:\r\n'
                 '    #\r\n' \
                 '    # Note here, we use `tf.lite.experimental.nn.dynamic_rnn` and '
                 '`time_major`\r\n'
                 '    # is set to True.\r\n' \
                 '    outputs, _ = tf.lite.experimental.nn.dynamic_rnn(\r\n'
                 "        lstm_cells, lstm_inputs, dtype='float32', " \
                 'time_major=True)\r\n'
                 '\r\n'
                 '    # Transpose the outputs back to [batch, time, output]\r\n'
                 '    outputs = tf.transpose(outputs, perm=[1, 0, 2])\r\n' \
                 '    outputs = tf.unstack(outputs, axis=1)\r\n' \
                 '    logits = tf.matmul(outputs[-1], out_weights) + out_bias\r\n'
                 "    output_class = tf.nn.softmax(logits, name='OUTPUT_CLASS')\r\n" \
                 '\r\n'
                 '    return x, logits, output_class\r\n'
                 '\r\n'
                 'def train(model,\r\n'
                 '          model_dir,\r\n' \
                 '          batch_size=20,\r\n'
                 '          learning_rate=0.001,\r\n'
                 '          train_steps=200,\r\n'
                 '          eval_steps=50,\r\n'
                 '          save_every_n_steps=100):\r\n'
                 '  """Train & save the MNIST recognition model."""\r\n'
                 '  # Train & test dataset.\r\n' \
                 '  (x_train, y_train), (x_test, y_test) = '
                 'tf.keras.datasets.mnist.load_data()\r\n'
                 '  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, ' \
                 'y_train))\r\n'
                 '  train_iterator = train_dataset.shuffle(\r\n'
                 '      ' \
                 'buffer_size=1000).batch(batch_size).repeat().make_one_shot_iterator()\r\n'
                 '  x, logits, output_class = model.build_model()\r\n'
                 '  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, '
                 'y_test))\r\n' \
                 '  test_iterator = test_dataset.batch(\r\n'
                 '      batch_size).repeat().make_one_shot_iterator()\r\n'
                 '  # input label placeholder\r\n' \
                 '  y = tf.placeholder(tf.int32, [\r\n'
                 '      None,\r\n'
                 '  ])\r\n'
                 '  one_hot_labels = tf.one_hot(y, depth=model.num_class)\r\n'
                 '  # Loss function\r\n'
                 '  loss = tf.reduce_mean(\r\n'
                 '      tf.nn.softmax_cross_entropy_with_logits(\r\n'
                 '          logits=logits, labels=one_hot_labels))\r\n'
                 '  correct = tf.nn.in_top_k(output_class, y, 1)\r\n'
                 '  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\r\n'
                 '  # Optimization\r\n'
                 '  opt = '
                 'tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\r\n' \
                 '\r\n'
                 '  # Initialize variables\r\n'
                 '  init = tf.global_variables_initializer()\r\n' \
                 '  saver = tf.train.Saver()\r\n'
                 '  batch_x, batch_y = train_iterator.get_next()\r\n'
                 '  batch_test_x, batch_test_y = test_iterator.get_next()\r\n'
                 '  with tf.Session() as sess:\r\n'
                 '    sess.run([init])\r\n'
                 '    for i in range(train_steps):\r\n'
                 '      batch_x_value, batch_y_value = sess.run([batch_x, '
                 'batch_y])\r\n'
                 '      _, loss_value = sess.run([opt, loss],\r\n' \
                 '                               feed_dict={\r\n'
                 '                                   x: batch_x_value,\r\n'
                 '                                   y: batch_y_value\r\n'
                 '                               })\r\n'
                 '      if i % 100 == 0:\r\n'
                 "        tf.logging.info('Training step %d, loss is %f' % (i, "
                 'loss_value))\r\n'
                 '      if i > 0 and i % save_every_n_steps == 0:\r\n'
                 '        accuracy_sum = 0.0\r\n'
                 '        for _ in range(eval_steps):\r\n'
                 '          test_x_value, test_y_value = sess.run([batch_test_x, '
                 'batch_test_y])\r\n' \
                 '          accuracy_value = sess.run(\r\n'
                 '              accuracy, feed_dict={\r\n' \
                 '                  x: test_x_value,\r\n' \
                 '                  y: test_y_value\r\n'
                 '              })\r\n'
                 '          accuracy_sum += accuracy_value\r\n'
                 "        tf.logging.info('Training step %d, accuracy is %f' %\r\n"
                 '                        (i, accuracy_sum / (eval_steps * 1.0)))\r\n'
                 '        saver.save(sess, model_dir)\r\n'
                 '\r\n'
                 'def export(model, model_dir, tflite_model_file,\r\n'
                 '           use_post_training_quantize=True):\r\n'
                 '  """Export trained model to tflite model."""\r\n' \
                 '  tf.reset_default_graph()\r\n' \
                 '  x, _, output_class = model.build_model()\r\n' \
                 '  saver = tf.train.Saver()\r\n'
                 '  sess = tf.Session()\r\n'
                 '  saver.restore(sess, model_dir)\r\n' \
                 '  # Convert to Tflite model.\r\n'
                 '  converter = tf.lite.TFLiteConverter.from_session(sess, [x], ' \
                 '[output_class])\r\n' \
                 '  converter.post_training_quantize = use_post_training_quantize\r\n' \
                 '  tflite = converter.convert()\r\n'
                 "  with open(tflite_model_file, 'w') as f:\r\n" \
                 '    f.write(tflite)\r\n' \
                 '\r\n' \
                 'def train_and_export(parsed_flags):\r\n' \
                 '  """Train the MNIST LSTM model and export to TfLite."""\r\n'
                 '  model = MnistLstmModel(\r\n'
                 '      time_steps=28,\r\n'
                 '      input_size=28,\r\n' \
                 '      num_lstm_layer=2,\r\n' \
                 '      num_lstm_units=64,\r\n'
                 '      units=64,\r\n'
                 '      num_class=10)\r\n' \
                 "  tf.logging.info('Starts training...')\r\n"
                 '  train(model, parsed_flags.model_dir)\r\n'
                 "  tf.logging.info('Finished training, starts exporting to tflite to " \
                 "%s ...' %\r\n"
                 '                  parsed_flags.tflite_model_file)\r\n'
                 '  export(model, parsed_flags.model_dir, '
                 'parsed_flags.tflite_model_file,\r\n' \
                 '         parsed_flags.use_post_training_quantize)\r\n'
                 '  tf.logging.info(\r\n' \
                 "      'Finished exporting, model is %s' % " \
                 'parsed_flags.tflite_model_file)\r\n'
                 '\r\n' \
                 '\r\n'
                 'def run_main(_):\r\n' \
                 '  """Main in the TfLite LSTM tutorial."""\r\n'
                 '  parser = argparse.ArgumentParser(\r\n' \
                 "      description=('Train a MNIST recognition model then export to "
                 "TfLite.'))\r\n" \
                 '  parser.add_argument(\r\n'
                 "      '--model_dir',\r\n"
                 '      type=str,\r\n'
                 "      help='Directory where the models will store.',\r\n" \
                 '      required=True)\r\n'
                 '  parser.add_argument(\r\n' \
                 "      '--tflite_model_file',\r\n" \
                 '      type=str,\r\n'
                 "      help='Full filepath to the exported tflite model file.',\r\n"
                 '      required=True)\r\n'
                 '  parser.add_argument(\r\n'
                 "      '--use_post_training_quantize',\r\n" \
                 "      action='store_true',\r\n"
                 '      default=True,\r\n'
                 "      help='Whether or not to use post_training_quantize.')\r\n"
                 '  parsed_flags, _ = parser.parse_known_args()\r\n' \
                 '  train_and_export(parsed_flags)\r\n' \
                 '\r\n'
                 '\r\n' \
                 'def main():\r\n'
                 '  app.run(main=run_main, argv=sys.argv[:1])\r\n'
                 '\r\n' \
                 "if __name__ == '__main__':\r\n"
                 '  main()\r\n' \
                 '```\r\n' \
                 '\r\n'
                 'When ran simply like `python example.py --model_dir lstms/ '
                 '--tflite_model_file lstms/model.tflite`, I get the following error '
                 'message : \r\n' \
                 '```\r\n' \
                 'INFO:tensorflow:Starts training...\r\n'
                 'I1216 23:55:41.555022 140529868711744 doc_example.py:159] Starts ' \
                 'training...\r\n'
                 'INFO:tensorflow:Training step 0, loss is 2.657418\r\n' \
                 'I1216 23:55:45.383375 140529868711744 doc_example.py:120] Training '
                 'step 0, loss is 2.657418\r\n'
                 'INFO:tensorflow:Training step 100, loss is 0.867711\r\n'
                 'I1216 23:55:47.319205 140529868711744 doc_example.py:120] Training '
                 'step 100, loss is 0.867711\r\n'
                 'INFO:tensorflow:Training step 100, accuracy is 0.540000\r\n' \
                 'I1216 23:55:47.966933 140529868711744 doc_example.py:132] Training '
                 'step 100, accuracy is 0.540000\r\n'
                 'INFO:tensorflow:Finished training, starts exporting to tflite to ' \
                 'lstm_doc/model.tflite ...\r\n' \
                 'I1216 23:55:50.603394 140529868711744 doc_example.py:162] Finished '
                 'training, starts exporting to tflite to lstm_doc/model.tflite ' \
                 '...\r\n'
                 'INFO:tensorflow:Restoring parameters from lstm_doc/\r\n' \
                 'I1216 23:55:50.832539 140529868711744 saver.py:1284] Restoring ' \
                 'parameters from lstm_doc/\r\n'
                 '2019-12-16 23:55:50.880481: I ' \
                 'tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs ' \
                 '(core count >= 8, compute capability >= 0.0): 0\r\n' \
                 '2019-12-16 23:55:50.880555: I '
                 'tensorflow/core/grappler/clusters/single_machine.cc:356] Starting '
                 'new session\r\n' \
                 '2019-12-16 23:55:50.900838: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] '
                 'Optimization results for grappler item: graph_to_optimize\r\n' \
                 '2019-12-16 23:55:50.900867: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   ' \
                 'function_optimizer: Graph size after: 412 nodes (0), 507 edges (0), ' \
                 'time = 3.325ms.\r\n' \
                 '2019-12-16 23:55:50.900872: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   '
                 'function_optimizer: Graph size after: 412 nodes (0), 507 edges (0), '
                 'time = 5.794ms.\r\n' \
                 '2019-12-16 23:55:50.900875: I ' \
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] '
                 'Optimization results for grappler item: hey_rnn_while_body_8743\r\n'
                 '2019-12-16 23:55:50.900878: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   '
                 'function_optimizer: function_optimizer did nothing. time = '
                 '0.002ms.\r\n' \
                 '2019-12-16 23:55:50.900882: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   ' \
                 'function_optimizer: function_optimizer did nothing. time = 0ms.\r\n'
                 '2019-12-16 23:55:50.900884: I '
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] '
                 'Optimization results for grappler item: hey_rnn_while_cond_8742\r\n' \
                 '2019-12-16 23:55:50.900888: I ' \
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   ' \
                 'function_optimizer: function_optimizer did nothing. time = 0ms.\r\n'
                 '2019-12-16 23:55:50.900891: I ' \
                 'tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   '
                 'function_optimizer: function_optimizer did nothing. time = 0ms.\r\n' \
                 'INFO:tensorflow:Froze 26 variables.\r\n'
                 'I1216 23:55:50.942811 140529868711744 graph_util_impl.py:334] Froze '
                 '26 variables.\r\n'
                 'INFO:tensorflow:Converted 26 variables to const ops.\r\n'
                 'I1216 23:55:50.946878 140529868711744 graph_util_impl.py:394] '
                 'Converted 26 variables to const ops.\r\n'
                 '/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py:846: '
                 'UserWarning: Property post_training_quantize is deprecated, please '
                 'use optimizations=[Optimize.DEFAULT] instead.\r\n'
                 '  " instead." % name)\r\n'
                 '---------------------------------------------------------------------------\r\n'
                 'ConverterError                            Traceback (most recent ' \
                 'call last)\r\n' \
                 '~/code_perso/decibel-light/prod/doc_example.py in <module>\r\n'
                 '    195 \r\n'
                 "    196 if __name__ == '__main__':\r\n"
                 '--> 197   main()\r\n'
                 '\r\n'
                 '~/code_perso/decibel-light/prod/doc_example.py in main()\r\n'
                 '    191 \r\n'
                 '    192 def main():\r\n'
                 '--> 193   app.run(main=run_main, argv=sys.argv[:1])\r\n'
                 '    194 \r\n'
                 '    195 \r\n'
                 '\r\n' \
                 '~/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py in '
                 'run(main, argv, flags_parser)\r\n' \
                 '    297       callback()\r\n'
                 '    298     try:\r\n'
                 '--> 299       _run_main(main, args)\r\n'
                 '    300     except UsageError as error:\r\n'
                 '    301       usage(shorthelp=True, detailed_error=error, '
                 'exitcode=error.exitcode)\r\n' \
                 '\r\n'
                 '~/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py in '
                 '_run_main(main, argv)\r\n'
                 '    248     sys.exit(retval)\r\n'
                 '    249   else:\r\n'
                 '--> 250     sys.exit(main(argv))\r\n' \
                 '    251 \r\n'
                 '    252 \r\n'
                 '\r\n' \
                 '~/code_perso/decibel-light/prod/doc_example.py in run_main(_)\r\n'
                 "    187       help='Whether or not to use "
                 "post_training_quantize.')\r\n"
                 '    188   parsed_flags, _ = parser.parse_known_args()\r\n' \
                 '--> 189   train_and_export(parsed_flags)\r\n' \
                 '    190 \r\n' \
                 '    191 \r\n' \
                 '\r\n'
                 '~/code_perso/decibel-light/prod/doc_example.py in '
                 'train_and_export(parsed_flags)\r\n'
                 '    162                   parsed_flags.tflite_model_file)\r\n'
                 '    163   export(model, parsed_flags.model_dir, '
                 'parsed_flags.tflite_model_file,\r\n' \
                 '--> 164          parsed_flags.use_post_training_quantize)\r\n'
                 '    165   tf.logging.info(\r\n'
                 "    166       'Finished exporting, model is %s' % "
                 'parsed_flags.tflite_model_file)\r\n'
                 '\r\n' \
                 '~/code_perso/decibel-light/prod/doc_example.py in export(model, ' \
                 'model_dir, tflite_model_file, use_post_training_quantize)\r\n'
                 '    144   converter = tf.lite.TFLiteConverter.from_session(sess, '
                 '[x], [output_class])\r\n' \
                 '    145   converter.post_training_quantize = '
                 'use_post_training_quantize\r\n'
                 '--> 146   tflite = converter.convert()\r\n'
                 "    147   with open(tflite_model_file, 'w') as f:\r\n"
                 '    148     f.write(tflite)\r\n'
                 '\r\n'
                 '~/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py '
                 'in convert(self)\r\n'
                 '    981           input_tensors=self._input_tensors,\r\n'
                 '    982           output_tensors=self._output_tensors,\r\n' \
                 '--> 983           **converter_kwargs)\r\n' \
                 '    984     else:\r\n'
                 '    985       result = _toco_convert_graph_def(\r\n'
                 '\r\n'
                 '~/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py '
                 'in toco_convert_impl(input_data, input_tensors, output_tensors, ' \
                 'enable_mlir_converter, *args, **kwargs)\r\n' \
                 '    447       input_data.SerializeToString(),\r\n'
                 '    448       debug_info_str=debug_info_str,\r\n'
                 '--> 449       enable_mlir_converter=enable_mlir_converter)\r\n' \
                 '    450   return data\r\n'
                 '    451 \r\n'
                 '\r\n'
                 '~/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/python/convert.py '
                 'in toco_convert_protos(model_flags_str, toco_flags_str, ' \
                 'input_data_str, debug_info_str, enable_mlir_converter)\r\n' \
                 '    198       stdout = _try_convert_to_unicode(stdout)\r\n'
                 '    199       stderr = _try_convert_to_unicode(stderr)\r\n'
                 '--> 200       raise ConverterError("See console for '
                 'info.\\n%s\\n%s\\n" % (stdout, stderr))\r\n'
                 '    201   finally:\r\n'
                 '    202     # Must manually cleanup files.\r\n'
                 '\r\n'
                 'ConverterError: See console for info.\r\n' \
                 '2019-12-16 23:55:52.418001: I ' \
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting ' \
                 'unsupported operation: ReadVariableOp\r\n' \
                 '2019-12-16 23:55:52.418032: I ' \
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting ' \
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418038: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418043: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418047: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418051: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418056: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418061: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418066: I ' \
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418071: I ' \
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n' \
                 '2019-12-16 23:55:52.418076: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n' \
                 '2019-12-16 23:55:52.418081: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting ' \
                 'unsupported operation: ReadVariableOp\r\n' \
                 '2019-12-16 23:55:52.418146: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting ' \
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418153: I ' \
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418158: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting ' \
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418162: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n' \
                 '2019-12-16 23:55:52.418166: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418171: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418175: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418179: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418184: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418188: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418193: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting ' \
                 'unsupported operation: ReadVariableOp\r\n' \
                 '2019-12-16 23:55:52.418199: I ' \
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: ReadVariableOp\r\n'
                 '2019-12-16 23:55:52.418227: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:659] Converting '
                 'unsupported operation: TensorListReserve\r\n'
                 '2019-12-16 23:55:52.418236: I '
                 'tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data '
                 'type in placeholder op: 21\r\n'
                 '2019-12-16 23:55:52.418479: F '
                 'tensorflow/lite/toco/tooling_util.cc:1074] Check failed: '
                 'name.substr(colon_pos + 1).find_first_not_of("0123456789") == '
                 'string::npos (0 vs. 18446744073709551615)Array '
                 "'stacked_rnn_cells/InputHint-UnidirectionalSequenceLstm-34aa74ee205711ea831bad6e4148a879-12-None-input_bias/ReadVariableOp:value' "
                 'has non-digit characters after colon.\r\n'
                 'Fatal Python error: Aborted\r\n'
                 '\r\n'
                 'Current thread 0x00007f7ffc1c8740 (most recent call first):\r\n'
                 '  File '
                 '"/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", '
                 'line 52 in execute\r\n'
                 '  File '
                 '"/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py", '
                 'line 250 in _run_main\r\n'
                 '  File '
                 '"/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/absl/app.py", '
                 'line 299 in run\r\n' \
                 '  File '
                 '"/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py", ' \
                 'line 40 in run\r\n'
                 '  File '
                 '"/home/mparient/.virtualenvs/decibel/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py", '
                 'line 89 in main\r\n' \
                 '  File "/home/mparient/.virtualenvs/decibel/bin/toco_from_protos", '
                 'line 8 in <module>\r\n'
                 'Aborted\r\n'
                 '```\r\n',
         'created_at': '2019-12-1'},
        {'body': "I'd like to understand the performance issue I faced :\r\n" \
                 'Why TF2.0+TRT6 (keras) slower than TF1+keras about 4 times ?\r\n'
                 '\r\n' \
                 'GPU K80\r\n' \
                 '\r\n'
                 'before : TF1 + keras (as separate package)\r\n'
                 'after: TF2 + TRT6 integrated with integrated keras\r\n'
                 '\r\n'
                 "on the same example TF2 is 4 times slower, when I'm waiting " \
                 'acceleration of training/predition in new version with TRT '
                 'integration.\r\n'
                 '\r\n',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu ' \
                 '18.04\r\n'
                 '- TensorFlow installed from: pip\r\n'
                 '- TensorFlow version (use command below): 1.13.1\r\n'
                 '- Python version: python3.7\r\n'
                 '- CUDA/cuDNN version: 10.1\r\n'
                 '- GPU model and memory: GTX 1080 ti\r\n' \
                 '\r\n' \
                 '\r\n' \
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'Since g++7 is now the default version on Ubuntu 18 and most '
                 'distributions, most builds will use  `_GLIBCXX_USE_CXX11_ABI=1`. It '
                 'seems also that when tensorflow is built with  '
                 '`_GLIBCXX_USE_CXX11_ABI=0`, it implies recompiling all other '
                 'libraries of the project with this flag which can be '
                 'unconvenient. \r\n'
                 '\r\n'
                 'We noticed that building with `_GLIBCXX_USE_CXX11_ABI=1` increases '
                 'the RAM by a lot. \r\n'
                 '\r\n' \
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'Both packages should consume the same amount of RAM.\r\n'
                 '\r\n' \
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 'You can install tensorflow using `python3.7 -m pip install '
                 'tensorflow==1.13.1` (related to '
                 'https://github.com/tensorflow/tensorflow/issues/27078) and makes '
                 'sure \r\n'
                 '`python3.7 -c "import tensorflow; '
                 'print(tensorflow.sysconfig.get_compile_flags())"` prints '
                 '`-D_GLIBCXX_USE_CXX11_ABI=1`. \r\n'
                 'Then you can install it in python3.6 `python3.6 -m pip install '
                 'tensorflow==1.13.1` and make sure ` python3.6 -c "import ' \
                 'tensorflow; print(tensorflow.sysconfig.get_compile_flags())"` '
                 'prints `D_GLIBCXX_USE_CXX11_ABI=0`.\r\n' \
                 '\r\n'
                 'Now run this script with python3.6 and python3.7 and you will see ' \
                 'that the second one consume a lot more (x3 on the model I use). Any '
                 '`saved_model.pb` should work.\r\n' \
                 '\r\n'
                 '```python\r\n'
                 'import io\r\n'
                 'import os\r\n'
                 'import sys\r\n' \
                 'try:\r\n'
                 '    from urllib import urlopen\r\n' \
                 'except ImportError:\r\n'
                 '    from urllib.request import urlopen\r\n' \
                 '\r\n'
                 'import numpy\r\n' \
                 'import psutil\r\n' \
                 'from PIL import Image\r\n'
                 '\r\n'
                 'import tensorflow as tf\r\n' \
                 'from tensorflow.core.protobuf import saved_model_pb2\r\n'
                 'from tensorflow.python.platform import gfile\r\n' \
                 'from tensorflow.python.util import compat\r\n' \
                 '\r\n'
                 'process = psutil.Process(os.getpid())\r\n'
                 '\r\n' \
                 "def print_ram(prefix=''):\r\n"
                 '    print("RAM", prefix, process.memory_info().rss / 1024. / '
                 '1024.)\r\n'
                 '\r\n' \
                 '\r\n' \
                 "if __name__ == '__main__':\r\n"
                 '\r\n'
                 '    if len(sys.argv) == 1:\r\n'
                 "        model_filename = 'saved_model.pb'\r\n"
                 '    else:\r\n' \
                 '        model_filename = sys.argv[1]\r\n'
                 '\r\n' \
                 "    with gfile.FastGFile(model_filename, 'rb') as f:\r\n"
                 '        data = compat.as_bytes(f.read())\r\n'
                 '        sm = saved_model_pb2.SavedModel()\r\n'
                 '        sm.ParseFromString(data)\r\n' \
                 '        if 1 != len(sm.meta_graphs):\r\n' \
                 "            print('More than one graph found. Not sure which to "
                 "write')\r\n"
                 '            sys.exit(1)\r\n' \
                 '\r\n'
                 '    img_url = '
                 "'https://i.dailymail.co.uk/1s/2019/11/23/09/21370544-7717313-image-a-1_1574501083030.jpg'\r\n"
                 '    image_data = urlopen(img_url).read()\r\n'
                 '    decoded_data = '
                 'numpy.array(Image.open(io.BytesIO(image_data)))\r\n'
                 '    decoded_data = numpy.expand_dims(decoded_data, axis=0)\r\n'
                 '\r\n'
                 "    print_ram('before graph import')\r\n"
                 '    graph = tf.import_graph_def(sm.meta_graphs[0].graph_def)\r\n' \
                 '\r\n'
                 "    print_ram('before device')\r\n" \
                 '    with tf.device("/device:GPU:0"):\r\n' \
                 '        with tf.Session(graph=graph, config=None) as sess:\r\n'
                 "            print_ram('after session')\r\n"
                 '            output = '
                 "sess.graph.get_tensor_by_name('import/predictions:0')\r\n"
                 "            print_ram('before run')\r\n"
                 '            for i in range(10000):\r\n' \
                 '                results = sess.run(output, ' \
                 'feed_dict={"import/image_tensor:0": decoded_data})\r\n' \
                 "                print_ram('after run')\r\n"
                 '```\r\n' \
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '\r\n'
                 'python3.6:\r\n'
                 '```\r\n'
                 'RAM before graph import 527.55078125\r\n'
                 'RAM before device 856.59375\r\n'
                 '2019-12-16 17:51:27.400388: I ' \
                 'tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU '
                 'supports instructions that this TensorFlow binary was not compiled '
                 'to use: AVX2 FMA\r\n'
                 '2019-12-16 17:51:27.425979: I ' \
                 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU '
                 'Frequency: 3399580000 Hz\r\n'
                 '2019-12-16 17:51:27.426637: I '
                 'tensorflow/compiler/xla/service/service.cc:150] XLA service '
                 '0x229fee0 executing computations on platform Host. Devices:\r\n' \
                 '2019-12-16 17:51:27.426655: I '
                 'tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor '
                 'device (0): <undefined>, <undefined>\r\n'
                 'RAM after session 860.25\r\n'
                 'RAM before run 860.25\r\n' \
                 'RAM after run 948.12109375\r\n' \
                 'RAM after run 998.0\r\n'
                 'RAM after run 1022.2265625\r\n'
                 'RAM after run 1038.984375\r\n'
                 'RAM after run 1038.984375\r\n'
                 'RAM after run 1056.0\r\n'
                 'RAM after run 1092.1953125\r\n'
                 'RAM after run 1097.09375\r\n' \
                 'RAM after run 1097.09375\r\n' \
                 'RAM after run 1097.09375\r\n'
                 'RAM after run 1097.09375\r\n'
                 'RAM after run 1097.09375\r\n' \
                 'RAM after run 1116.42578125\r\n'
                 'RAM after run 1116.42578125\r\n' \
                 'RAM after run 1116.42578125\r\n' \
                 'RAM after run 1116.42578125\r\n'
                 'RAM after run 1116.42578125\r\n'
                 '```\r\n' \
                 '\r\n'
                 'python3.7:\r\n'
                 '```\r\n' \
                 'RAM before graph import 510.33984375\r\n'
                 'RAM before device 752.87890625\r\n'
                 '2019-12-16 17:26:00.118658: I ' \
                 'tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU ' \
                 'supports instructions that this TensorFlow binary was not compiled '
                 'to use: AVX2 FMA\r\n' \
                 '2019-12-16 17:26:00.141979: I ' \
                 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU ' \
                 'Frequency: 3399580000 Hz\r\n' \
                 '2019-12-16 17:26:00.142737: I ' \
                 'tensorflow/compiler/xla/service/service.cc:150] XLA service ' \
                 '0x265aff0 executing computations on platform Host. Devices:\r\n' \
                 '2019-12-16 17:26:00.142775: I ' \
                 'tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor ' \
                 'device (0): <undefined>, <undefined>\r\n' \
                 'RAM after session 756.5625\r\n' \
                 'RAM before run 756.5625\r\n'
                 'RAM after run 2977.8515625\r\n' \
                 'RAM after run 3030.1640625\r\n' \
                 'RAM after run 3047.8046875\r\n' \
                 'RAM after run 3090.08203125\r\n' \
                 'RAM after run 3121.296875\r\n'
                 'RAM after run 3122.0703125\r\n'
                 'RAM after run 3123.1015625\r\n'
                 'RAM after run 3123.1015625\r\n' \
                 'RAM after run 3135.9921875\r\n'
                 '```',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n' \
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu ' \
                 '18.04.3 LTS (in docker container)\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: NA\r\n' \
                 '- TensorFlow installed from (source or binary): pip wheel\r\n'
                 '- TensorFlow version (use command below): 2.0.0\r\n'
                 '- Python version: 3.6.8 \r\n'
                 '- Bazel version (if compiling from source): NA\r\n'
                 '- GCC/Compiler version (if compiling from source): NA\r\n' \
                 '- CUDA/cuDNN version: NA\r\n' \
                 '- GPU model and memory: NA\r\n' \
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'The training is down due to low layer system call (corrupted size ' \
                 'vs. prev_size). the process is immediately corrupted without much ' \
                 'log to trace.\r\n'
                 '\r\n' \
                 '**Describe the expected behavior**\r\n'
                 'the estimator (BoostedTreesRegressor) should be trained in '
                 'normal.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n' \
                 '```\r\n' \
                 'import os\r\n'
                 'import uuid\r\n'
                 'import json\r\n' \
                 'from io import StringIO\r\n'
                 '\r\n'
                 'import pandas as pd\r\n'
                 'import tensorflow as tf\r\n'
                 'from sklearn.model_selection import train_test_split\r\n'
                 'from tensorflow.estimator import BoostedTreesRegressor\r\n'
                 '\r\n'
                 'def split_data(df: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame, '
                 'pd.DataFrame, pd.DataFrame):\r\n' \
                 '    """Split data into train and test sets\r\n'
                 '\r\n' \
                 '    Args:\r\n' \
                 '        df (DataFrame): pandas dataframe\r\n'
                 '\r\n'
                 '    Returns:\r\n' \
                 '        X_train, X_test, y_train, y_test\r\n' \
                 '    """\r\n'
                 '    feature_cols = [c for c in df.columns if '
                 "c.startswith('FEATURE')]\r\n" \
                 "    label_col = [c for c in df.columns if c.startswith('LABEL')]\r\n" \
                 '\r\n'
                 '    return train_test_split(df[feature_cols], df[label_col], '
                 'test_size=.2, random_state=42)\r\n' \
                 '\r\n' \
                 '\r\n' \
                 'def make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n'
                 '    def input_fn():\r\n'
                 '        dataset = tf.data.Dataset.from_tensor_slices((dict(X), '
                 'y))\r\n'
                 '        if shuffle:\r\n'
                 '            dataset = dataset.shuffle(len(y))\r\n'
                 '        dataset = dataset.repeat(n_epochs)\r\n'
                 '        dataset = dataset.batch(len(y))\r\n'
                 '        return dataset\r\n' \
                 '    return input_fn\r\n'
                 '\r\n'
                 '\r\n'
                 'def make_serving_receiver_fn(df: pd.DataFrame):\r\n'
                 '    feature_col_names = [c for c in df.columns if '
                 "c.startswith('FEATURE')]\r\n"
                 '    feature_cols = [tf.feature_column.numeric_column(fc) for fc in '
                 'feature_col_names]\r\n'
                 '    feature_spec = '
                 'tf.feature_column.make_parse_example_spec(feature_cols)\r\n'
                 '    return ' \
                 'tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\n' \
                 '\r\n' \
                 '\r\n'
                 'def train_model(df: pd.DataFrame, **params) -> '
                 '(BoostedTreesRegressor, dict, dict):\r\n'
                 '    """Train Boost Tree\r\n'
                 '\r\n'
                 '    Args:\r\n'
                 '        df (DataFrame): pandas dataframe\r\n'
                 '        params (**dict): parameters for training\r\n'
                 '\r\n' \
                 '    Returns:\r\n'
                 '        BoostTreesRegressor, saved model dir, a dict containing the '
                 'evaluation metrics\r\n' \
                 '    """\r\n' \
                 '    feature_col_names = [c for c in df.columns if ' \
                 "c.startswith('FEATURE')]\r\n" \
                 '    label_col_name = [c for c in df.columns if ' \
                 "c.startswith('LABEL')]\r\n"
                 '    feature_cols = [tf.feature_column.numeric_column(fc_name) for ' \
                 'fc_name in feature_col_names]\r\n' \
                 '    default_params = {\r\n'
                 "        'feature_columns': feature_cols,\r\n" \
                 "        'n_batches_per_layer': 1,\r\n"
                 "        'model_dir': os.path.join('../output', "
                 'str(uuid.uuid4())),\r\n'
                 '    }\r\n'
                 '    default_params.update(params)\r\n' \
                 '    regressor = BoostedTreesRegressor(**default_params)\r\n'
                 '    X_train, X_valid, y_train, y_valid = '
                 'train_test_split(df[feature_col_names], df[label_col_name], '
                 'test_size=0.2, random_state=42)\r\n' \
                 '    train_input_fn = make_input_fn(X_train, y_train)\r\n'
                 '    evaluate_input_fn = make_input_fn(X_valid, y_valid, ' \
                 'n_epochs=1)\r\n'
                 '    regressor.train(input_fn=train_input_fn)\r\n'
                 '    summary = regressor.evaluate(input_fn=evaluate_input_fn)\r\n' \
                 '    receiver_fn = make_serving_receiver_fn(df)\r\n'
                 '    export_dir = regressor.export_saved_model(regressor.model_dir, '
                 'receiver_fn)\r\n'
                 '\r\n'
                 '    summary = {k: float(v) for k, v in summary.items()}\r\n'
                 '\r\n'
                 '    return regressor, export_dir, summary\r\n' \
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n' \
                 'No problem under environment __OSX 10.14.5__ with __pyenv ' \
                 'virtualenv 3.7.0 (default, Nov 22 2019, 12:39:30) \\n[Clang 11.0.0 '
                 '(clang-1100.0.33.12)]__\r\n'
                 '\r\n'
                 'No problem when use google colab runtime (__Ubuntu 18.04.3 LTS '
                 'Python 3.6.9 compiled by GCC 8.3.0__). See __[notebook ' \
                 'shared](https://colab.research.google.com/drive/1aAjJCWOBd7R0Hb6w4d7UCSnkfk4c-xyE)__\r\n' \
                 '\r\n'
                 'Same problem in Kaggle runtime (__Debian GNU/Linux 9 Python 3.6.6 '
                 'Anaconda GCC 7.3.0__) using tensorflow 2.0.0.\r\n'
                 '\r\n' \
                 'Full error log in docker using official image '
                 '__tensorflow/tensorflow:2.0.0-py3__:\r\n'
                 '\r\n'
                 '```\r\n'
                 'WARNING: Logging before flag parsing goes to stderr.\r\n'
                 'I1216 13:01:22.775628 140590360139584 estimator.py:1800] Using ' \
                 'default config.\r\n'
                 'I1216 13:01:22.778364 140590360139584 estimator.py:212] Using ' \
                 "config: {'_model_dir': "
                 "'/app/WINDMIL_PoC_Data_FE/service/estimator/saved_model/8f979f53-4821-4def-bc6e-9692d868e2ea', "
                 "'_tf_random_seed': None, '_save_summary_steps': 100, " \
                 "'_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, "
                 "'_session_config': allow_soft_placement: true\r\n" \
                 'graph_options {\r\n'
                 '  rewrite_options {\r\n'
                 '    meta_optimizer_iterations: ONE\r\n'
                 '  }\r\n'
                 '}\r\n'
                 ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': " \
                 "10000, '_log_step_count_steps': 100, '_train_distribute': None, "
                 "'_device_fn': None, '_protocol': None, '_eval_distribute': None, "
                 "'_experimental_distribute': None, "
                 "'_experimental_max_worker_delay_secs': None, "
                 "'_session_creation_timeout_secs': 7200, '_service': None, "
                 "'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec "
                 "object at 0x7fdd837c5d30>, '_task_type': 'worker', '_task_id': 0, " \
                 "'_global_id_in_cluster': 0, '_master': '', '_evaluation_master': "
                 "'', '_is_chief': True, '_num_ps_replicas': 0, "
                 "'_num_worker_replicas': 1}\r\n"
                 'W1216 13:01:22.809657 140590360139584 deprecation.py:506] From '
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: '
                 'calling BaseResourceVariable.__init__ (from '
                 'tensorflow.python.ops.resource_variable_ops) with constraint is '
                 'deprecated and will be removed in a future version.\r\n' \
                 'Instructions for updating:\r\n' \
                 'If using Keras pass *_constraint arguments to layers.\r\n' \
                 'W1216 13:01:22.811977 140590360139584 deprecation.py:323] From ' \
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: '
                 'Variable.initialized_value (from tensorflow.python.ops.variables) ' \
                 'is deprecated and will be removed in a future version.\r\n'
                 'Instructions for updating:\r\n'
                 'Use Variable.read_value. Variables in 2.X are initialized '
                 'automatically both in eager and graph (inside tf.defun) '
                 'contexts.\r\n'
                 'I1216 13:01:22.888739 140590360139584 estimator.py:1147] Calling ' \
                 'model_fn.\r\n'
                 'W1216 13:01:22.968535 140590360139584 deprecation.py:323] From '
                 '/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: '
                 'to_float (from tensorflow.python.ops.math_ops) is deprecated and '
                 'will be removed in a future version.\r\n'
                 'Instructions for updating:\r\n'
                 'Use `tf.cast` instead.\r\n'
                 'I1216 13:01:23.278414 140590360139584 estimator.py:1149] Done '
                 'calling model_fn.\r\n' \
                 'I1216 13:01:23.278981 140590360139584 '
                 'basic_session_run_hooks.py:541] Create CheckpointSaverHook.\r\n'
                 'W1216 13:01:23.327183 140590360139584 meta_graph.py:448] Issue '
                 'encountered when serializing resources.\r\n'
                 "Type is unsupported, or the types of the items don't match field "
                 'type in CollectionDef. Note this is a warning and probably safe to ' \
                 'ignore.\r\n'
                 "'_Resource' object has no attribute 'name'\r\n"
                 'I1216 13:01:23.468171 140590360139584 monitored_session.py:240] '
                 'Graph was finalized.\r\n'
                 '2019-12-16 13:01:23.468761: I '
                 'tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU '
                 'supports instructions that this TensorFlow binary was not compiled '
                 'to use: AVX2 FMA\r\n'
                 '2019-12-16 13:01:23.475253: I '
                 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU '
                 'Frequency: 2904000000 Hz\r\n'
                 '2019-12-16 13:01:23.476582: I '
                 'tensorflow/compiler/xla/service/service.cc:168] XLA service '
                 '0x4ed0210 executing computations on platform Host. Devices:\r\n' \
                 '2019-12-16 13:01:23.476657: I '
                 'tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor '
                 'device (0): Host, Default Version\r\n' \
                 'I1216 13:01:23.541273 140590360139584 session_manager.py:500] ' \
                 'Running local_init_op.\r\n'
                 'I1216 13:01:23.561274 140590360139584 session_manager.py:502] Done '
                 'running local_init_op.\r\n'
                 'W1216 13:01:23.960061 140590360139584 meta_graph.py:448] Issue '
                 'encountered when serializing resources.\r\n' \
                 "Type is unsupported, or the types of the items don't match field "
                 'type in CollectionDef. Note this is a warning and probably safe to '
                 'ignore.\r\n'
                 "'_Resource' object has no attribute 'name'\r\n" \
                 'I1216 13:01:24.019694 140590360139584 ' \
                 'basic_session_run_hooks.py:606] Saving checkpoints for 0 into '
                 '/app/WINDMIL_PoC_Data_FE/service/estimator/saved_model/8f979f53-4821-4def-bc6e-9692d868e2ea/model.ckpt.\r\n' \
                 'W1216 13:01:24.100714 140590360139584 meta_graph.py:448] Issue '
                 'encountered when serializing resources.\r\n'
                 "Type is unsupported, or the types of the items don't match field " \
                 'type in CollectionDef. Note this is a warning and probably safe to '
                 'ignore.\r\n' \
                 "'_Resource' object has no attribute 'name'\r\n"
                 'I1216 13:01:24.385114 140590360139584 '
                 'basic_session_run_hooks.py:262] loss = 0.017109463, step = 0\r\n'
                 'W1216 13:01:24.594182 140590360139584 '
                 'basic_session_run_hooks.py:724] It seems that global step ' \
                 '(tf.train.get_global_step) has not been increased. Current value '
                 '(could be stable): 0 vs previous value: 0. You could increase the '
                 'global step by passing tf.train.get_global_step() to ' \
                 'Optimizer.apply_gradients or Optimizer.minimize.\r\n'
                 'corrupted size vs. prev_size\r\n' \
                 'Aborted\r\n'
                 '```\r\n',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux '
                 'Ubuntu 16.04\r\n'
                 '- TensorFlow installed from (source or binary):  binary\r\n'
                 '- TensorFlow version (use command below): 2.1.0rc1\r\n'
                 '- Python version: 3.6.8\r\n' \
                 '- Bazel version (if compiling from source): N/A \r\n'
                 '- GCC/Compiler version (if compiling from source): N/A \r\n' \
                 '- CUDA/cuDNN version: N/A \r\n' \
                 '- GPU model and memory: N/A \r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'If we use `tf.strings.split` in map_func to process each element in '
                 'tf.data.Dataset, the used memory grows when we iterate the dataset ' \
                 "and the used memory is not freed after iteration. What's more, the " \
                 'used memory continues to grow greatly if we repeatedly create the '
                 'same tf.data.Dataset instance. However, the used memory keeps ' \
                 'stable if we use tf.py_function to implement the split logic.\r\n' \
                 '![image](https://user-images.githubusercontent.com/18071380/70906668-debb4780-2041-11ea-945c-bf2a05cbb637.png)\r\n'
                 '\r\n' \
                 '\r\n' \
                 '**Describe the expected behavior**\r\n'
                 'The used memory when iterating the dataset should be freed and ' \
                 'should grow the create the same tf.data.Dataset instance.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '#### Experiment\r\n'
                 '```python\r\n'
                 'import psutil\r\n'
                 'import tensorflow as tf\r\n' \
                 'import pandas as pd\r\n'
                 'import numpy as np\r\n'
                 '\r\n'
                 'FEATURE_COUNT = 400\r\n'
                 '\r\n' \
                 '# mock feature names and feature data\r\n' \
                 'def gen_feature_names(feature_count):\r\n' \
                 '    feature_names = []\r\n' \
                 '    for i in range(feature_count):\r\n'
                 '        feature_names.append("f{}".format(i))\r\n'
                 '    return feature_names\r\n'
                 '\r\n'
                 '\r\n'
                 'def gen_samples(feature_names, sample_count=5000):\r\n'
                 '    samples = []\r\n'
                 '    for _ in range(sample_count):\r\n'
                 '        feature_str = ""\r\n'
                 '        for name in feature_names:\r\n'
                 '            feature_str += "{};".format(np.random.random())\r\n'
                 '        feature_str += str(np.random.randint(0,2))\r\n'
                 '        yield feature_str\r\n'
                 '        \r\n'
                 'def dataset_fn(dataset):\r\n' \
                 '    def _py_parse_data(record):\r\n'
                 '        record = record.numpy()\r\n'
                 '        feature_labels = bytes.decode(record).split(";")\r\n'
                 '        return feature_labels\r\n'
                 '\r\n'
                 '    def _parse_data(*record):\r\n'
                 '        feature_values = record[0:-1]\r\n'
                 '        features = {}\r\n' \
                 '        for i,feature_name in enumerate(FEATURE_NAMES):\r\n'
                 '            features[feature_name] = feature_values[i]\r\n'
                 '        label = tf.strings.to_number(record[-1], tf.int64)\r\n'
                 '        return features, label\r\n'
                 '    \r\n'
                 '    tout = [tf.string] * FEATURE_COUNT\r\n'
                 '    tout.append(tf.string)\r\n' \
                 '    \r\n' \
                 '    dataset = dataset.map(\r\n' \
                 '        lambda record: tf.py_function(\r\n' \
                 '            _py_parse_data,\r\n'
                 '            [record],\r\n'
                 '            tout\r\n' \
                 '        )\r\n'
                 '    )\r\n'
                 '    dataset = dataset.map(_parse_data)\r\n' \
                 '    dataset = dataset.shuffle(buffer_size=100)\r\n' \
                 '    return dataset\r\n' \
                 '\r\n' \
                 '\r\n'
                 'def dataset_fn_using_split(dataset):\r\n'
                 '    def _parse_data(record):\r\n'
                 "        feature_label = tf.strings.split([record], sep=';')[0]\r\n"
                 '        feature_values = feature_label[0:-1]\r\n'
                 '        features = {}\r\n' \
                 '        for i, feature_name in enumerate(FEATURE_NAMES):\r\n' \
                 '            features[feature_name] = feature_values[i]\r\n' \
                 '        label = feature_label[-1]\r\n'
                 '\r\n'
                 '        return features, label\r\n'
                 '    \r\n'
                 '    dataset = dataset.map(_parse_data)\r\n' \
                 '    dataset = dataset.shuffle(buffer_size=100)\r\n'
                 '    return dataset\r\n'
                 '\r\n'
                 'def create_dataset(feature_names, using_split=True):\r\n' \
                 '    dataset = tf.data.Dataset.from_generator(\r\n'
                 '        lambda : gen_samples(feature_names), tf.string\r\n'
                 '    )\r\n'
                 '    if using_split:\r\n'
                 '        dataset = dataset_fn_using_split(dataset)\r\n'
                 '    else:\r\n'
                 '        dataset = dataset_fn(dataset)\r\n' \
                 '    dataset = dataset.batch(512)\r\n'
                 '    return dataset\r\n'
                 '\r\n' \
                 '\r\n'
                 'def view_used_mem():\r\n'
                 '    used_mem = psutil.virtual_memory().used\r\n' \
                 "    print('used memory: {} Mb'.format(used_mem / 1024 / 1024))\r\n"
                 '\r\n'
                 'FEATURE_NAMES = gen_feature_names(FEATURE_COUNT)\r\n' \
                 '\r\n'
                 '# Test used memory by using tf.strings.split in `map_func`\r\n'
                 'FEATURE_NAMES = gen_feature_names(FEATURE_COUNT)\r\n'
                 '\r\n' \
                 'start_time = time.time()\r\n' \
                 'for i in range(4):\r\n'
                 '    print("loop {}".format(i))\r\n'
                 '    view_used_mem()\r\n'
                 '    dataset = create_dataset(FEATURE_NAMES, using_split=True)\r\n'
                 '    for batch in dataset:\r\n' \
                 '        pass\r\n' \
                 'print("Consume time : {}".format(time.time() - start_time))\r\n'
                 'print("end")\r\n'
                 'view_used_mem()\r\n' \
                 '\r\n'
                 '# Test used memory by using `tf.py_function`\r\n'
                 'start_time = time.time()\r\n'
                 'for i in range(4):\r\n'
                 '    print("loop {}".format(i))\r\n'
                 '    view_used_mem()\r\n'
                 '    dataset = create_dataset(FEATURE_NAMES, using_split=False)\r\n'
                 '    for batch in dataset:\r\n' \
                 '        pass\r\n' \
                 'print("Consume time : {}".format(time.time() - start_time))\r\n' \
                 'print("end")\r\n'
                 'view_used_mem()\r\n' \
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'We encounter this issue when building inputs pipeline using ' \
                 '`tf.data.Dataset` in ' \
                 '[ElasticDL](https://github.com/sql-machine-learning/elasticdl)\r\n',
         'created_at': '2019-12-1'}, \
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): '
                 'Linux-4.14.106+-x86_64-with-debian-buster-sid\r\n' \
                 '- TensorFlow installed from (source or binary): pip install\r\n'
                 '- TensorFlow version (use command below): 2.0.0\r\n' \
                 '- Python version: 3.6.6\r\n'
                 '\r\n' \
                 '**Describe the current behavior**\r\n'
                 'I am trying to train a model using:\r\n'
                 '* TF 2.0 Keras functional API\r\n' \
                 '* Feature columns and `DenseFeatures` as the input layer\r\n'
                 '* tf.data.Dataset API as the `x` parameter in model.fit()\r\n'
                 '\r\n'
                 "This raises a ValueError, and I'm not sure why because this doesn't " \
                 'work even with (what I think is) a minimal example. I suspect that ' \
                 'this might be due to TF trying to match the feature columns to the ' \
                 'inputs by name, but the name of the Input tensor contains a suffix '
                 'e.g. `age:0`, but I may very well be mistaken.\r\n' \
                 '\r\n'
                 'Might be related to: ' \
                 'https://github.com/tensorflow/tensorflow/issues/30143\r\n'
                 '\r\n' \
                 '**Describe the expected behavior**\r\n'
                 'I expect to be able to train a model using the Keras functional API '
                 'using data from the Dataset API, with feature_columns being fed ' \
                 'into Input layers.\r\n' \
                 '\r\n' \
                 '**Code to reproduce the issue**\r\n'
                 'The following test case does not represent my use case, but it does '
                 'reproduce the problem. Despite what the simplistic example ' \
                 'suggests, I explicitly require both feature_columns and the '
                 'functional API so suggesting that I use other TF libraries would ' \
                 'not be an option, unless I can replicate the same functionality '
                 'with minimal effort.\r\n'
                 '\r\n'
                 '```python\r\n'
                 'import tensorflow as tf\r\n'
                 'import numpy as np\r\n'
                 'from tensorflow.keras.layers import DenseFeatures, Dense, Input\r\n'
                 '\r\n'
                 'def make_model(features):\r\n'
                 '    feature_columns = [tf.feature_column.numeric_column(key) for '
                 'key in features]\r\n'
                 '    nn_input = {key: Input(name=key, shape=(), dtype=tf.float32) '
                 'for key in features}\r\n'
                 '\r\n' \
                 '    feat = DenseFeatures(feature_columns)(nn_input)\r\n'
                 '    dense = Dense(16)(feat)\r\n' \
                 '    output = Dense(1)(dense)\r\n'
                 '    model = tf.keras.Model(inputs=nn_input, outputs=output)\r\n'
                 '    model.compile(\r\n'
                 '        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n'
                 '        '
                 'loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\r\n'
                 '        metrics=[tf.keras.metrics.AUC()],\r\n'
                 '    )\r\n'
                 '    return model\r\n'
                 '\r\n'
                 'features = ["age", "income"]\r\n'
                 'label = "is_male"\r\n'
                 '\r\n'
                 'input_dataset = tf.data.Dataset.from_tensor_slices(\r\n' \
                 '    {key: np.ones((1000, 1), dtype=np.float) for key in '
                 'features}\r\n'
                 ')\r\n' \
                 'target_dataset = tf.data.Dataset.from_tensor_slices(\r\n'
                 '    {label: np.ones((1000, 1), dtype=np.int)}\r\n' \
                 ')\r\n'
                 'complete_dataset = tf.data.Dataset.zip((input_dataset, '
                 'target_dataset)).shuffle(10000)\r\n'
                 '\r\n'
                 'model = make_model(features)\r\n'
                 'model.summary()\r\n' \
                 'model.fit(complete_dataset)\r\n' \
                 '```\r\n'
                 '\r\n' \
                 'This code references the issue created by @durandg12. Thank you.\r\n' \
                 '\r\n'
                 '**Stack trace**\r\n' \
                 '```bash\r\n' \
                 '---------------------------------------------------------------------------\r\n'
                 'KeyError                                  Traceback (most recent ' \
                 'call last)\r\n' \
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py '
                 'in standardize_input_data(data, names, shapes, check_batch_axis, '
                 'exception_prefix)\r\n'
                 "    498           if data[x].__class__.__name__ == 'DataFrame' else "
                 'data[x]\r\n' \
                 '--> 499           for x in names\r\n'
                 '    500       ]\r\n' \
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py ' \
                 'in <listcomp>(.0)\r\n' \
                 "    498           if data[x].__class__.__name__ == 'DataFrame' else " \
                 'data[x]\r\n' \
                 '--> 499           for x in names\r\n' \
                 '    500       ]\r\n'
                 '\r\n' \
                 "KeyError: 'dense_1'\r\n" \
                 '\r\n'
                 'During handling of the above exception, another exception ' \
                 'occurred:\r\n' \
                 '\r\n' \
                 'ValueError                                Traceback (most recent '
                 'call last)\r\n' \
                 '<ipython-input-1-0081a84113eb> in <module>\r\n' \
                 '     27 model = make_model(features)\r\n'
                 '     28 model.summary()\r\n' \
                 '---> 29 model.fit(complete_dataset)\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py ' \
                 'in fit(self, x, y, batch_size, epochs, verbose, callbacks, ' \
                 'validation_split, validation_data, shuffle, class_weight, '
                 'sample_weight, initial_epoch, steps_per_epoch, validation_steps, ' \
                 'validation_freq, max_queue_size, workers, use_multiprocessing, '
                 '**kwargs)\r\n' \
                 '    726         max_queue_size=max_queue_size,\r\n' \
                 '    727         workers=workers,\r\n'
                 '--> 728         use_multiprocessing=use_multiprocessing)\r\n' \
                 '    729 \r\n' \
                 '    730   def evaluate(self,\r\n'
                 '\r\n' \
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py ' \
                 'in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, '
                 'validation_split, validation_data, shuffle, class_weight, '
                 'sample_weight, initial_epoch, steps_per_epoch, validation_steps, ' \
                 'validation_freq, **kwargs)\r\n'
                 '    322                 mode=ModeKeys.TRAIN,\r\n'
                 '    323                 training_context=training_context,\r\n'
                 '--> 324                 total_epochs=epochs)\r\n'
                 '    325             cbks.make_logs(model, epoch_logs, ' \
                 'training_result, ModeKeys.TRAIN)\r\n'
                 '    326 \r\n' \
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py '
                 'in run_one_epoch(model, iterator, execution_function, dataset_size, '
                 'batch_size, strategy, steps_per_epoch, num_samples, mode, '
                 'training_context, total_epochs)\r\n'
                 '    121         step=step, mode=mode, size=current_batch_size) as ' \
                 'batch_logs:\r\n'
                 '    122       try:\r\n' \
                 '--> 123         batch_outs = execution_function(iterator)\r\n'
                 '    124       except (StopIteration, errors.OutOfRangeError):\r\n' \
                 '    125         # TODO(kaftan): File bug about tf function and '
                 'errors.OutOfRangeError?\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py '
                 'in execution_function(input_fn)\r\n'
                 '     84     # `numpy` translates Tensors to values in Eager ' \
                 'mode.\r\n'
                 '     85     return nest.map_structure(_non_none_constant_value,\r\n'
                 '---> 86                               ' \
                 'distributed_function(input_fn))\r\n'
                 '     87 \r\n'
                 '     88   return execution_function\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py ' \
                 'in __call__(self, *args, **kwds)\r\n'
                 '    455 \r\n'
                 '    456     tracing_count = self._get_tracing_count()\r\n'
                 '--> 457     result = self._call(*args, **kwds)\r\n'
                 '    458     if tracing_count == self._get_tracing_count():\r\n' \
                 '    459       self._call_counter.called_without_tracing()\r\n' \
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py ' \
                 'in _call(self, *args, **kwds)\r\n' \
                 '    501       # This is the first call of __call__, so we have to ' \
                 'initialize.\r\n' \
                 '    502       initializer_map = '
                 'object_identity.ObjectIdentityDictionary()\r\n' \
                 '--> 503       self._initialize(args, kwds, ' \
                 'add_initializers_to=initializer_map)\r\n'
                 '    504     finally:\r\n' \
                 '    505       # At this point we know that the initialization is '
                 'complete (or less\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py '
                 'in _initialize(self, args, kwds, add_initializers_to)\r\n' \
                 '    406     self._concrete_stateful_fn = (\r\n' \
                 '    407         ' \
                 'self._stateful_fn._get_concrete_function_internal_garbage_collected(  '
                 '# pylint: disable=protected-access\r\n' \
                 '--> 408             *args, **kwds))\r\n'
                 '    409 \r\n'
                 '    410     def invalid_creator_scope(*unused_args, '
                 '**unused_kwds):\r\n' \
                 '\r\n' \
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py '
                 'in _get_concrete_function_internal_garbage_collected(self, *args, '
                 '**kwargs)\r\n'
                 '   1846     if self.input_signature:\r\n'
                 '   1847       args, kwargs = None, None\r\n'
                 '-> 1848     graph_function, _, _ = '
                 'self._maybe_define_function(args, kwargs)\r\n'
                 '   1849     return graph_function\r\n'
                 '   1850 \r\n' \
                 '\r\n' \
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py ' \
                 'in _maybe_define_function(self, args, kwargs)\r\n'
                 '   2148         graph_function = '
                 'self._function_cache.primary.get(cache_key, None)\r\n'
                 '   2149         if graph_function is None:\r\n'
                 '-> 2150           graph_function = '
                 'self._create_graph_function(args, kwargs)\r\n'
                 '   2151           self._function_cache.primary[cache_key] = '
                 'graph_function\r\n'
                 '   2152         return graph_function, args, kwargs\r\n'
                 '\r\n' \
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py '
                 'in _create_graph_function(self, args, kwargs, '
                 'override_flat_arg_shapes)\r\n'
                 '   2039             arg_names=arg_names,\r\n' \
                 '   2040             ' \
                 'override_flat_arg_shapes=override_flat_arg_shapes,\r\n'
                 '-> 2041             capture_by_value=self._capture_by_value),\r\n'
                 '   2042         self._function_attributes,\r\n' \
                 '   2043         # Tell the ConcreteFunction to clean up its graph '
                 'once it goes out of\r\n' \
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py '
                 'in func_graph_from_py_func(name, python_func, args, kwargs, '
                 'signature, func_graph, autograph, autograph_options, '
                 'add_control_dependencies, arg_names, op_return_value, collections, ' \
                 'capture_by_value, override_flat_arg_shapes)\r\n' \
                 '    913                                           '
                 'converted_func)\r\n'
                 '    914 \r\n'
                 '--> 915       func_outputs = python_func(*func_args, '
                 '**func_kwargs)\r\n'
                 '    916 \r\n'
                 '    917       # invariant: `func_outputs` contains only Tensors, ' \
                 'CompositeTensors,\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py '
                 'in wrapped_fn(*args, **kwds)\r\n'
                 '    356         # __wrapped__ allows AutoGraph to swap in a '
                 'converted function. We give\r\n' \
                 '    357         # the function a weak reference to itself to avoid '
                 'a reference cycle.\r\n'
                 '--> 358         return weak_wrapped_fn().__wrapped__(*args, '
                 '**kwds)\r\n'
                 '    359     weak_wrapped_fn = weakref.ref(wrapped_fn)\r\n'
                 '    360 \r\n' \
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py ' \
                 'in distributed_function(input_iterator)\r\n'
                 '     71     strategy = '
                 'distribution_strategy_context.get_strategy()\r\n' \
                 '     72     outputs = strategy.experimental_run_v2(\r\n' \
                 '---> 73         per_replica_function, args=(model, x, y, '
                 'sample_weights))\r\n' \
                 '     74     # Out of PerReplica outputs reduce or pick values to '
                 'return.\r\n'
                 '     75     all_outputs = dist_utils.unwrap_output_dict(\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py '
                 'in experimental_run_v2(self, fn, args, kwargs)\r\n'
                 '    758       fn = autograph.tf_convert(fn, '
                 'ag_ctx.control_status_ctx(),\r\n'
                 '    759                                 '
                 'convert_by_default=False)\r\n'
                 '--> 760       return self._extended.call_for_each_replica(fn, ' \
                 'args=args, kwargs=kwargs)\r\n'
                 '    761 \r\n' \
                 '    762   def reduce(self, reduce_op, value, axis):\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py ' \
                 'in call_for_each_replica(self, fn, args, kwargs)\r\n'
                 '   1785       kwargs = {}\r\n' \
                 '   1786     with self._container_strategy().scope():\r\n' \
                 '-> 1787       return self._call_for_each_replica(fn, args, '
                 'kwargs)\r\n'
                 '   1788 \r\n' \
                 '   1789   def _call_for_each_replica(self, fn, args, kwargs):\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py ' \
                 'in _call_for_each_replica(self, fn, args, kwargs)\r\n' \
                 '   2130         self._container_strategy(),\r\n' \
                 '   2131         replica_id_in_sync_group=constant_op.constant(0, '
                 'dtypes.int32)):\r\n' \
                 '-> 2132       return fn(*args, **kwargs)\r\n' \
                 '   2133 \r\n'
                 '   2134   def _reduce_to(self, reduce_op, value, destinations):\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py '
                 'in wrapper(*args, **kwargs)\r\n'
                 '    290   def wrapper(*args, **kwargs):\r\n'
                 '    291     with ' \
                 'ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\r\n'
                 '--> 292       return func(*args, **kwargs)\r\n'
                 '    293 \r\n' \
                 '    294   if inspect.isfunction(func) or inspect.ismethod(func):\r\n'
                 '\r\n' \
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py ' \
                 'in train_on_batch(model, x, y, sample_weight, class_weight, '
                 'reset_metrics)\r\n'
                 '    251   x, y, sample_weights = model._standardize_user_data(\r\n' \
                 '    252       x, y, sample_weight=sample_weight, '
                 'class_weight=class_weight,\r\n'
                 '--> 253       extract_tensors_from_dataset=True)\r\n'
                 '    254   batch_size = array_ops.shape(nest.flatten(x, '
                 'expand_composites=True)[0])[0]\r\n'
                 '    255   # If `model._distribution_strategy` is True, then we are ' \
                 'in a replica context\r\n'
                 '\r\n'
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py '
                 'in _standardize_user_data(self, x, y, sample_weight, class_weight, '
                 'batch_size, check_steps, steps_name, steps, validation_split, '
                 'shuffle, extract_tensors_from_dataset)\r\n'
                 '   2517           shapes=None,\r\n'
                 "   2518           check_batch_axis=False,  # Don't enforce the "
                 'batch size.\r\n'
                 "-> 2519           exception_prefix='target')\r\n"
                 '   2520 \r\n'
                 '   2521       # Generate sample-wise weight values given the ' \
                 '`sample_weight` and\r\n'
                 '\r\n' \
                 '/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_utils.py '
                 'in standardize_input_data(data, names, shapes, check_batch_axis, ' \
                 'exception_prefix)\r\n'
                 '    501     except KeyError as e:\r\n'
                 '    502       raise ValueError(\'No data provided for "\' + '
                 'e.args[0] + \'". Need data \'\r\n'
                 "--> 503                        'for each key in: ' + str(names))\r\n" \
                 '    504   elif isinstance(data, (list, tuple)):\r\n'
                 '    505     if isinstance(data[0], (list, tuple)):\r\n'
                 '\r\n'
                 'ValueError: No data provided for "dense_1". Need data for each key ' \
                 "in: ['dense_1']\r\n"
                 '```\r\n',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n' \
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch '
                 'Linux\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the ' \
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n' \
                 '- TensorFlow version (use command below): 2.1.0.rc0\r\n' \
                 '- Python version: 3.8\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version: Not relevant\r\n'
                 '- GPU model and memory: Not relevant\r\n' \
                 '\r\n'
                 '**Describe the current behavior**\r\n' \
                 'Loading a SavedModel in C++, the outputs (and probably also inputs, '
                 'but I do not have models with multiple inputs) from the signature '
                 'are returned in a random order, which then causes the network to '
                 'fail when used for inference.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'The outputs (and probably inputs) should be returned in the correct '
                 'order.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '```\r\n' \
                 'std::string model_dir = "/path/to/my_model";\r\n' \
                 'std::vector<string> input_names;\r\n'
                 'std::vector<string> output_names;\r\n'
                 'tensorflow::SavedModelBundleLite bundle; // Same with '
                 'SavedModelBundle\r\n'
                 '\r\n'
                 '// Create default options.\r\n'
                 'tensorflow::SessionOptions session_options;\r\n'
                 'tensorflow::RunOptions run_options;\r\n'
                 '\r\n'
                 '// Load model.\r\n' \
                 'auto status = tensorflow::LoadSavedModel(\r\n'
                 '\tsession_options,\r\n' \
                 '\trun_options,\r\n'
                 '\tmodel_dir,\r\n' \
                 '\t{tensorflow::kSavedModelTagServe},\r\n' \
                 '\t&bundle\r\n'
                 ');\r\n'
                 '\r\n' \
                 '// Check if model has been loaded correctly.\r\n'
                 'if (!status.ok()) {\r\n' \
                 '\tstd::cerr << status.ToString() << std::endl;\r\n'
                 '\treturn;\r\n' \
                 '}\r\n' \
                 '\r\n'
                 '// Get model signature.\r\n'
                 'auto signatures = bundle.GetSignatures();\r\n'
                 'if (!signatures.contains("serving_default")) {\r\n' \
                 '\tstd::cerr << "Could not find serving_default in model '
                 'signatures." << std::endl;\r\n'
                 '\treturn;\r\n'
                 '}\r\n'
                 '\r\n' \
                 '// Get the inputs names.\r\n'
                 'for (auto const & input : ' \
                 'signatures.at("serving_default").inputs()) {\r\n' \
                 '\tinput_names.push_back(input.second.name());\r\n'
                 '}\r\n' \
                 '\r\n'
                 '// Get the outputs names.\r\n'
                 'for (auto const & output : ' \
                 'signatures.at("serving_default").outputs()) {\r\n' \
                 '\toutput_names.push_back(output.second.name());\r\n' \
                 '}\r\n' \
                 'std::vector<tensorflow::Tensor>  inputs = getMyInputs();  // Some '
                 'compatible input vector.\r\n'
                 'std::vector<tensorflow::Tensor>  outputs;\r\n'
                 '\r\n' \
                 '// Create a vector of pairs for associating inputs to their ' \
                 'names.\r\n' \
                 'std::vector<std::pair<std::string, tensorflow::Tensor>> '
                 'input_pairs;\r\n' \
                 'for (std::size_t i = 0; i < inputs.size(); ++i) {\r\n' \
                 '\tinput_pairs.push_back({input_names.at(i), inputs.at(i)});\r\n' \
                 '}\r\n'
                 '\r\n' \
                 '// Run the network.\r\n' \
                 'bundle.GetSession()->Run(\r\n'
                 '\tinput_pairs,\r\n' \
                 '\toutput_names,\r\n'
                 '\t{},\r\n' \
                 '\t&outputs\r\n'
                 ');\r\n' \
                 '```\r\n' \
                 '\r\n' \
                 'When running this the output order changes, giving a mismatch error ' \
                 '(eg. `Check failed: dtype() == expected_dtype (3 vs. 1) float '
                 'expected, got int32`) after calling `Run`. \r\n' \
                 '\r\n'
                 '\r\n',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): No\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows ' \
                 '10\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device: NA\r\n'
                 '- TensorFlow installed from (source or binary): Binary (pip)\r\n'
                 '- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca '
                 '2.0.0\r\n' \
                 '- Python version: Python 3.6.8\r\n' \
                 '- Bazel version (if compiling from source): NA\r\n'
                 '- GCC/Compiler version (if compiling from source): NA\r\n'
                 '- CUDA/cuDNN version: CUDA 10.0.130_411.31; cuDNN 10.0 v7.6.5.32\r\n' \
                 '- GPU model and memory: NVIDIA Quadro P2000, 4 GB\r\n'
                 '\r\n' \
                 '**Describe the current behavior**\r\n'
                 'When the model is saved in the default tf format, warnings are '
                 'logged when trying to serve the model.\r\n'
                 '\r\n' \
                 'Examplary warning logs:\r\n'
                 '```\r\n' \
                 'WARNING:tensorflow:5 out of the last 5 calls to <function '
                 'recreate_function.<locals>.restored_function_body at '
                 '0x000001ED79058730> triggered tf.function retracing. Tracing is '
                 'expensive and the excessive number of tracings is likely due to ' \
                 'passing python objects instead of tensors. Also, tf.function has '
                 'experimental_relax_shapes=True option that relaxes argument shapes '
                 'that can avoid unnecessary retracing. Please refer to '
                 'https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args ' \
                 'and https://www.tensorflow.org/api_docs/python/tf/function for more '
                 'details.\r\n'
                 '```\r\n'
                 '\r\n'
                 'When the model is saved in the hdf5 format, the warnings do not '
                 'occur.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 'The save formats should be equivalent and behave in the same '
                 'way.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 'Execute the following scripts to create and serve model\r\n' \
                 "1. Run the first script with `format_ext = ''` which saves the "
                 'model in tf format, **restart the Python console**, serve the model ' \
                 'with the second script which creates the aforementioned '
                 'warnings.\r\n'
                 "1. When running the scripts with `format_ext = '.h5'`, the model is " \
                 'saved in hdf5 format and no warnings appear.\r\n'
                 '\r\n' \
                 'Model creation:\r\n'
                 '```python\r\n' \
                 'import os\r\n' \
                 '\r\n' \
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 "format_ext = ''  # '.h5' or empty for tf format\r\n"
                 "model_path = os.path.join('out', "
                 "'mnist-classifier{}'.format(format_ext))\r\n"
                 '\r\n' \
                 "gpus = tf.config.experimental.list_physical_devices('GPU')\r\n"
                 '\r\n'
                 'tf.config.experimental.set_virtual_device_configuration(\r\n'
                 '    gpus[0],\r\n'
                 '    '
                 '[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),\r\n' \
                 '     '
                 'tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),\r\n' \
                 '     '
                 'tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512)]\r\n'
                 ')\r\n'
                 '\r\n' \
                 'strategy = tf.distribute.MirroredStrategy()\r\n' \
                 'with strategy.scope():\r\n' \
                 "    inputs = tf.keras.Input(shape=(784,), name='digits')\r\n" \
                 "    x = tf.keras.layers.Dense(64, activation='relu', "
                 "name='dense_1')(inputs)\r\n" \
                 "    x = tf.keras.layers.Dense(64, activation='relu', "
                 "name='dense_2')(x)\r\n"
                 "    outputs = tf.keras.layers.Dense(10, activation='softmax', "
                 "name='predictions')(x)\r\n"
                 '\r\n' \
                 '    model = tf.keras.Model(inputs=inputs, outputs=outputs)\r\n' \
                 '\r\n'
                 '    model.compile(optimizer=tf.keras.optimizers.RMSprop(),  # ' \
                 'Optimizer\r\n' \
                 '                  # Loss function to minimize\r\n' \
                 '                  '
                 'loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n' \
                 '                  # List of metrics to monitor\r\n'
                 '                  ' \
                 'metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\r\n' \
                 '\r\n'
                 'model.save(model_path)\r\n'
                 '```\r\n'
                 '\r\n' \
                 'Model serving:\r\n'
                 '```python\r\n' \
                 'import os\r\n' \
                 '\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n'
                 "format_ext = ''  # '.h5' or empty for tf format\r\n"
                 "model_path = os.path.join('out', " \
                 "'mnist-classifier{}'.format(format_ext))\r\n" \
                 '\r\n'
                 "gpus = tf.config.experimental.list_physical_devices('GPU')\r\n" \
                 '\r\n'
                 'tf.config.experimental.set_virtual_device_configuration(\r\n' \
                 '    gpus[0],\r\n'
                 '    ' \
                 '[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),\r\n'
                 '     '
                 'tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512),\r\n'
                 '     '
                 'tf.config.experimental.VirtualDeviceConfiguration(memory_limit=512)]\r\n'
                 ')\r\n'
                 '\r\n' \
                 '(_, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\r\n'
                 "x_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n" \
                 '\r\n'
                 'strategy = tf.distribute.MirroredStrategy()\r\n'
                 'with strategy.scope():\r\n'
                 '    loaded_model = tf.keras.models.load_model(model_path)\r\n'
                 '    predictions = loaded_model.predict(x_test, batch_size=64)\r\n' \
                 '```\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n'
                 'The warnings occur only if more than two vGPUs are used.',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- **Linux Ubuntu 18.04**\r\n'
                 '- TensorFlow installed from (source or binary): **source**\r\n'
                 '- TensorFlow version: v2 (master)\r\n'
                 '- Python version: python 3.7.5\r\n' \
                 '- Installed using virtualenv? conda:  conda 4.7.12\r\n' \
                 '- Bazel version (if compiling from source): **bazel 1.1.0**\r\n' \
                 '- GCC/Compiler version (if compiling from source): **gcc 8.3.0**\r\n'
                 '- CUDA/cuDNN version: **CUDA 10.2/cuDNN7.6.5**\r\n'
                 '- GPU model and memory: GeForce GTX670 4035Mb\r\n'
                 '- CPU Intel i7860@2.80GHz\r\n' \
                 '- Kernel  Linux-headers-4.15.0-73\r\n'
                 '- NVCC -  v2.5.6, for CUDA 10.2, Nov 19,2019\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 'It seems Bazel Build is disregarding my choice of **NOT INSTALLING '
                 'XLA**.\r\n'
                 '\r\n'
                 'I installed CUDA and cuDNN, NCCL, created a python 3.7.5 ' \
                 'environment and decided to build tensorflow from MASTER branch ' \
                 'since my  GPU has capabilities 3.0 and the other installs I tried, ' \
                 'from docker or conda, where complaining about a minimum capability '
                 'of 3.5.\r\n'
                 '\r\n'
                 'There were plenty of documentation for the need of a build from ' \
                 'source for 3.0 capability and I decided to do it. \r\n'
                 '\r\n'
                 'From git I realized it was necessary to disable XLA. `./configure` '
                 'asked for **XLA** capabitity and I answered **N** (Y was the ' \
                 'default), it correctly detected python, cuda and gcc, I wrote '
                 '**3.0** for desired capability, (A message showed me the need to ' \
                 'disable XLA, what I already did). \r\n'
                 '\r\n'
                 'When I did the bazel build it took a huge lot of time and seemed to '
                 'freeze with something like the piece below.\r\n' \
                 '````bash\r\n'
                 './tensorflow/compiler/xla/service/hlo_computation.h:562:3: note: in '
                 'expansion of macro ‘TF_RET_CHECK’\r\n'
                 '   TF_RET_CHECK(order.size() == instruction_count());\r\n' \
                 '   ^~~~~~~~~~~~\r\n' \
                 './tensorflow/compiler/xla/service/hlo_computation.h: In ' \
                 'instantiation of ‘tensorflow::Status '
                 'xla::HloComputation::AcceptOrdered(xla::DfsHloVisitorBase<HloInstructionPtr>*, ' \
                 'absl::Span<xla::HloInstruction* const>) const [with '
                 'HloInstructionPtr = const xla::HloInstruction*]’:\r\n'
                 './tensorflow/compiler/xla/service/hlo_computation.h:588:61:   '
                 'required from here\r\n'
                 './tensorflow/compiler/xla/service/hlo_computation.h:562:29: '
                 'warning: comparison of integer expressions of different signedness: '
                 '‘absl::Span<xla::HloInstruction* const>::size_type’ {aka ‘long '
                 'unsigned int’} and ‘tensorflow::int64’ {aka ‘long long int’} ' \
                 '[-Wsign-compare]\r\n' \
                 '   TF_RET_CHECK(order.size() == instruction_count());\r\n'
                 './tensorflow/core/platform/macros.h:87:47: note: in definition of '
                 'macro ‘TF_PREDICT_FALSE’\r\n' \
                 ' #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))\r\n'
                 '                                               ^\r\n'
                 './tensorflow/compiler/xla/service/hlo_computation.h:562:3: note: in ' \
                 'expansion of macro ‘TF_RET_CHECK’\r\n'
                 '   TF_RET_CHECK(order.size() == instruction_count());\r\n'
                 '   ^~~~~~~~~~~~\r\n'
                 './tensorflow/compiler/xla/array.h: In instantiation of '
                 '‘tensorflow::int64 xla::Array<T>::dim(tensorflow::int64) const ' \
                 '[with T = int; tensorflow::int64 = long long int]’:\r\n'
                 '\r\n' \
                 '````\r\n'
                 '\r\n'
                 '\r\n' \
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '````bash \r\n'
                 'bazel build --config=opt -- config=--local_ram_resources=6000  '
                 '--cxxopt="-D_GLIBCXX_USE_CXX11_ABI=0" --config=cuda ' \
                 '//tensorflow/tools/pip_package:build_pip_package\r\n'
                 '````\r\n'
                 '\r\n'
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose '
                 'the problem. If including tracebacks, please include the full ' \
                 'traceback. Large logs and files should be attached.\r\n'
                 'The pieces showing the actual error ocurrered as follows:\r\n'
                 '````\r\n'
                 'ERROR: ' \
                 '/home/lbarosi/Pythonia/tensorflow/tensorflow/core/kernels/BUILD:5735:1: '
                 "C++ compilation of rule '//tensorflow/core/kernels:training_ops' "
                 'failed (Exit 1)\r\n' \
                 'x86_64-linux-gnu-gcc-8: fatal error: Killed signal terminated '
                 'program cc1plus\r\n'
                 'compilation terminated.\r\n'
                 'Target //tensorflow/tools/pip_package:build_pip_package failed to ' \
                 'build\r\n' \
                 'Use --verbose_failures to see the command lines of failed build '
                 'steps.\r\n'
                 'ERROR: '
                 '/home/lbarosi/Pythonia/tensorflow/tensorflow/python/tools/BUILD:141:1 ' \
                 "C++ compilation of rule '//tensorflow/core/kernels:training_ops' "
                 'failed (Exit 1)\r\n'
                 'INFO: Elapsed time: 4520.437s, Critical Path: 1586.97s\r\n' \
                 'INFO: 5492 processes: 5492 local.\r\n'
                 'FAILED: Build did NOT complete successfully\r\n'
                 '\r\n'
                 '````\r\n'
                 '\r\n' \
                 'Any help is appreciated. I would like to try as much as possible to '
                 'stick with this configurations without downgradings of CUDA or ' \
                 'cuDNN.\r\n',
         'created_at': '2019-12-1'}, \
        {'body': '- TensorFlow version (you are using): 2.0\r\n' \
                 '- Are you willing to contribute it (Yes/No): Yes\r\n' \
                 '\r\n'
                 '```\r\n'
                 'tf.keras.backend.floatx = tf.float64\r\n'
                 'f64 = tf.Variable([0,0.2,0.5,0.7,1], dtype=tf.float64)\r\n'
                 'tf.where(f64 > 0.5, 1., 0. )\r\n'
                 '\r\n'
                 '<tf.Tensor: id=16, shape=(5,), dtype=float32, numpy=array([0., 0., ' \
                 '0., 1., 1.], dtype=float32)>\r\n' \
                 '```\r\n'
                 '\r\n'
                 'alternatives for tf.float64 models lead to difficult to read ' \
                 'code\r\n'
                 '\r\n' \
                 '```\r\n'
                 'tf.where(f64 > 0.5, tf.constant(1., dtype=tf.float64), ' \
                 'tf.constant(0., dtype=tf.float64) )\r\n'
                 'tf.cast(tf.where(f64 > 0.5, 1., 0. ), dtype=tf.float64)\r\n' \
                 '```\r\n'
                 '\r\n'
                 "assuming that tf.where can't respect the float type of the input "
                 'variable nor the floatx setting, as those change the api, better ' \
                 'would be:\r\n' \
                 '\r\n' \
                 '```\r\n'
                 'tf.where(f64 > 0.5, 1., 0. , dtype=tf.float64)\r\n'
                 '```\r\n' \
                 '\r\n', \
         'created_at': '2019-12-1'},
        {'body': 'It looks like `experimental_run_tf_function` was removed from '
                 '`tf.keras.Model.compile` in this commit a few days ago: ' \
                 'https://github.com/tensorflow/tensorflow/commit/c73c99ca3e0bacf2bca313f270bb3eae28869530#diff-de9b96ac2d81503324cbbbe21732031fR1159\r\n'
                 '\r\n'
                 'In [Horovod](http://horovod.ai/), this flag / graph mode is '
                 'necessary in order for `Optimizer.get_gradients()` to be called, '
                 'which aggregates gradients across workers.  Since this flag has '
                 'been removed, distributed training in Horovod with `tf.keras` is '
                 'not working in our nightly builds.\r\n' \
                 '\r\n' \
                 'Is there a workaround to achieve the same behavior with the latest '
                 'changes on master?\r\n'
                 '\r\n'
                 'Note that we cannot perform the allreduce aggregation in '
                 '`apply_gradients` due to interactions with gradient clipping and '
                 'loss scaling (see '
                 'https://github.com/horovod/horovod/pull/1347).\r\n',
         'created_at': '2019-12-1'},
        {'body': '<em>Please make sure that this is a build/installation issue. As ' \
                 'per our [GitHub ' \
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- OS Platform and Distribution: Ubuntu 18.04.3 LTS\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: v2.1.0-rc1\r\n'
                 '\r\n'
                 " I'm building using the docker image for a raspberry pi 3 build.\r\n"
                 '\r\n' \
                 '\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 '\r\n'
                 "I'm able to correctly build "
                 '`tensorflow-2.1.0rc1-cp35-none-linux_armv7l.whl` using:\r\n' \
                 '\r\n'
                 '```\r\n'
                 'CI_DOCKER_EXTRA_PARAMS="-e CI_BUILD_PYTHON=python3 -e '
                 'CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4"     ' \
                 'tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3     '
                 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh \r\n'
                 '```\r\n'
                 "I'm trying to do the same but with the python 3.7 docker images "
                 '`PI-PYTHON37` (`tensorflow/tools/ci_build/Dockerfile.pi-python37`) '
                 'but it fails with:\r\n'
                 '\r\n'
                 '```\r\n'
                 'ERROR: '
                 '/workspace/tensorflow/lite/python/interpreter_wrapper/BUILD:8:1: '
                 'C++ compilation of rule '
                 "'//tensorflow/lite/python/interpreter_wrapper:numpy' failed (Exit "
                 '1)\r\n'
                 "cc1plus: warning: command line option '-std=gnu11' is valid for "
                 'C/ObjC but not for C++\r\n'
                 'In file included from ' \
                 'bazel-out/armeabi-py2-opt/bin/external/local_config_python/python_include/Python.h:8:0,\r\n'
                 '                 from ' \
                 './tensorflow/lite/python/interpreter_wrapper/numpy.h:49,\r\n'
                 '                 from '
                 'tensorflow/lite/python/interpreter_wrapper/numpy.cc:17:\r\n'
                 'bazel-out/armeabi-py2-opt/bin/external/local_config_python/python_include/pyconfig.h:13:55: ' \
                 'fatal error: arm-linux-gnueabihf/python3.5m/pyconfig.h: No such '
                 'file or directory\r\n'
                 ' #  include <arm-linux-gnueabihf/python3.5m/pyconfig.h>\r\n'
                 '                                                       ^\r\n'
                 'compilation terminated.\r\n'
                 'INFO: Elapsed time: 339.221s, Critical Path: 36.02s\r\n' \
                 'INFO: 6204 processes: 6204 local.\r\n'
                 'FAILED: Build did NOT complete successfully\r\n'
                 'FAILED: Build did NOT complete successfully\r\n'
                 '```\r\n'
                 '\r\n'
                 '**Provide the exact sequence of commands / steps that you executed '
                 'before running into the problem**\r\n'
                 '\r\n' \
                 '```\r\n'
                 'cd tensorflow\r\n'
                 'git checkout v2.1.0-rc1\r\n'
                 'CI_DOCKER_EXTRA_PARAMS="-e CI_BUILD_PYTHON=python3 -e ' \
                 'CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4"     '
                 'tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37     '
                 'tensorflow/tools/ci_build/pi/build_raspberry_pi.sh\r\n' \
                 '```\r\n'
                 '\r\n' \
                 '**Any other info / logs**\r\n'
                 'Include any logs or source code that would be helpful to diagnose ' \
                 'the problem. If including tracebacks, please include the full '
                 'traceback. Large logs and files should be attached.\r\n',
         'created_at': '2019-12-1'},
        {'body': '@tensorflow/micro\r\n' \
                 '\r\n'
                 '**System information**\r\n'
                 '- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n'
                 'NAME="Ubuntu"\r\n'
                 'VERSION="18.04.3 LTS"\r\n'
                 'ID=ubuntu\r\n'
                 'PRETTY_NAME="Ubuntu 18.04.3 LTS"\r\n'
                 'VERSION_ID="18.04"\r\n'
                 'Python versionL 2.7.15+\r\n'
                 '- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ' \
                 'STM32F7-Disco and also tried on K64F based on ARM-M4\r\n'
                 '\r\n'
                 '**Describe the problem**\r\n'
                 'I am attempting to build the Hello World example for the '
                 'STM32F7-Disco from the following link but it is failing to build\r\n' \
                 'https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world.\r\n'
                 '\r\n'
                 '**Please provide the exact sequence of commands/steps when you ran ' \
                 'into the problem**\r\n'
                 'make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed '
                 'TAGS="CMSIS disco_f746ng" generate_hello_world_mbed_project\r\n'
                 '\r\n'
                 'cd ' \
                 'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed\r\n'
                 '\r\n'
                 'mbed config root .\r\n'
                 '\r\n'
                 'mbed deploy\r\n' \
                 "python -c 'import fileinput, glob;\r\n" \
                 'for filename in glob.glob("mbed-os/tools/profiles/*.json"):\r\n'
                 '  for line in fileinput.input(filename, inplace=True):\r\n'
                 '    print line.replace("\\"-std=gnu++98\\"","\\"-std=c++11\\", ' \
                 '\\"-fpermissive\\"")\'\r\n' \
                 '\r\n' \
                 'mbed compile -m DISCO_F746NG -t GCC_ARM\r\n' \
                 '\r\n'
                 'This is the error I seen before making a change to :\r\n'
                 '\r\n'
                 '`Compile [ 98.7%]: arm_mult_q15.c\r\n'
                 '[Error] arm_mult_q15.c@101,6: conflicting types for ' \
                 "'arm_mult_q15'\r\n" \
                 '[ERROR] '
                 './tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:101:6: '
                 "error: conflicting types for 'arm_mult_q15'\r\n"
                 ' void arm_mult_q15(\r\n' \
                 '      ^~~~~~~~~~~~\r\n'
                 'In file included from ' \
                 './tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Source/BasicMathFunctions/arm_mult_q15.c:29:0:\r\n'
                 './mbed-os/cmsis/TARGET_CORTEX_M/arm_math.h:1924:8: note: previous '
                 "declaration of 'arm_mult_q15' was here\r\n"
                 '   void arm_mult_q15(\r\n' \
                 '        ^~~~~~~~~~~~\r\n' \
                 '\r\n'
                 '[mbed] ERROR: "/usr/bin/python" returned error.\r\n' \
                 '       Code: 1\r\n' \
                 '       Path: '
                 '"/home/pramod/tensorflow-master/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed"\r\n' \
                 '       Command: "/usr/bin/python -u '
                 '/home/pramod/tensorflow-master/tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/hello_world/mbed/mbed-os/tools/make.py '
                 '-D TF_LITE_STATIC_MEMORY -t GCC_ARM -m DISCO_F746NG --source . '
                 '--build ./BUILD/DISCO_F746NG/GCC_ARM"\r\n' \
                 '       Tip: You could retry the last command with "-v" flag for '
                 'verbose output\r\n' \
                 '`\r\n' \
                 'Any help regarding this is deeply appreciated.\r\n'
                 '\r\n' \
                 'Thank you', \
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 16.04 in Docker\r\n'
                 '- TensorFlow installed from (source or binary): pip install\r\n'
                 '- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n'
                 '- Python version: 3.5\r\n' \
                 '- CUDA/cuDNN version: 10.0 / 7\r\n'
                 '- GPU model and memory: GTX 1080Ti / 11175MiB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 'Hi authors and developers,\r\n'
                 '\r\n'
                 'I am developing our project in tf=2.0.0 and eager_mode is '
                 'disable.\r\n' \
                 '\r\n'
                 'The main reason is tf=1.x will not be maintained but third party '
                 'libraries have not been ready for tf=2.0 yet.\r\n'
                 '\r\n'
                 'This issues is a separate issues from '
                 '[#35050](https://github.com/tensorflow/tensorflow/issues/35050#issuecomment-565395512)\r\n'
                 '\r\n'
                 'This potential issue is somethine wrong if users do custom training '
                 'with level API which includes ' \
                 '`tf.keras.layers.BatchNormalization()` in tf=2.0 and eager model is '
                 'disable.\r\n' \
                 '\r\n' \
                 'I summary the testcaset as the following:\r\n'
                 '\r\n'
                 '```python\r\n' \
                 '#%%\r\n'
                 'import tensorflow as tf\r\n'
                 'tf.compat.v1.disable_eager_execution()\r\n' \
                 '#tf.compat.v1.disable_v2_behavior()\r\n'
                 '\r\n'
                 'import numpy as np\r\n' \
                 '\r\n'
                 'batch_size = 100\r\n' \
                 '\r\n' \
                 'def download_data():\r\n'
                 '\r\n' \
                 '    # get raw data\r\n'
                 '    (trainX, trainY), (testX, testY) = ' \
                 'tf.keras.datasets.cifar10.load_data()\r\n'
                 '    trainX = trainX.astype(np.float32)\r\n'
                 '    testX  = testX.astype(np.float32)\r\n'
                 '\r\n'
                 '    # ont-hot\r\n'
                 '    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n' \
                 '    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n'
                 '\r\n'
                 '    # get validation sets\r\n' \
                 '    training_size = 45000\r\n'
                 '    validX = trainX[training_size:,:]\r\n'
                 '    validY = trainY[training_size:,:]\r\n'
                 '\r\n'
                 '    trainX = trainX[:training_size,:]\r\n'
                 '    trainY = trainY[:training_size,:]\r\n'
                 '\r\n' \
                 '    return trainX, trainY, validX, validY, testX, testY\r\n'
                 '\r\n'
                 'def data_pipeline(dataX, dataY):\r\n'
                 '\r\n'
                 '        dataset = tf.data.Dataset.from_tensor_slices( (dataX, ' \
                 'dataY) )\r\n' \
                 '        dataset = dataset.shuffle(batch_size * 8)\r\n'
                 '        dataset = dataset.repeat()\r\n'
                 '        dataset = dataset.batch(batch_size)\r\n'
                 '        dataset = '
                 'dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n' \
                 '        return dataset\r\n' \
                 '\r\n'
                 'class custom_model():\r\n' \
                 '    def __init__(self):\r\n'
                 '\r\n'
                 '        def Acc():\r\n' \
                 '            acc = tf.keras.metrics.categorical_accuracy(label_ref, '
                 'clf_out)\r\n' \
                 '            return tf.math.reduce_mean(acc)\r\n' \
                 '\r\n'
                 '        def c_loss():\r\n'
                 '            loss = '
                 'tf.keras.losses.categorical_crossentropy(label_ref, clf_out)\r\n' \
                 '            loss = tf.math.reduce_mean(loss)\r\n'
                 '            return loss\r\n'
                 '\r\n'
                 '        # create model\r\n' \
                 '        clf_input = tf.keras.layers.Input(shape=(32,32,3), '
                 'name="model/input")\r\n'
                 '        model = '
                 'tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, '
                 "weights=None, input_tensor=clf_input, pooling='max', classes=10)\r\n"
                 '        #model = ' \
                 'tf.keras.applications.vgg16.VGG16(include_top=True, weights=None, '
                 "input_tensor=clf_input, pooling='max', classes=10)\r\n" \
                 "        model.compile(loss='categorical_crossentropy', "
                 "optimizer='SGD', metrics=['accuracy'])\r\n"
                 '\r\n'
                 '        label_ref = tf.keras.layers.Input(shape=(10,) , '
                 "name='label_ref')\r\n"
                 '        clf_out = model(clf_input)\r\n' \
                 '\r\n' \
                 '        # using tf.keras.optimizers.Nadam would get error\r\n'
                 '        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n'
                 '        optimizer = ' \
                 'tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n'
                 '        self.train_op = optimizer.minimize(c_loss(), ' \
                 'var_list=[model.trainable_variables])\r\n'
                 '\r\n'
                 '        self.clf_model = model\r\n' \
                 '        self.clf_input = clf_input\r\n' \
                 '        self.label_ref = label_ref\r\n'
                 '        self.op_acc = Acc()\r\n' \
                 '        self.c_loss = c_loss()\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n"
                 '\r\n'
                 '    # set GPU\r\n'
                 '    import os\r\n'
                 '    if os.environ.get("CUDA_VISIBLE_DEVICES") is None:\r\n'
                 '        os.environ["CUDA_VISIBLE_DEVICES"] = "0"\r\n'
                 '\r\n'
                 '    # reset tf session\r\n'
                 '    tf.compat.v1.keras.backend.clear_session()\r\n'
                 '    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n'
                 '    sess = '
                 'tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n'
                 '    tf.compat.v1.keras.backend.set_session(sess) \r\n'
                 '\r\n'
                 '    # prepare data\r\n'
                 '    trainX, trainY, validX, validY, testX, testY = '
                 'download_data()\r\n'
                 '    train_gen = data_pipeline(trainX, trainY)\r\n'
                 '    valid_gen = data_pipeline(validX, validY)\r\n'
                 '    test_gen = data_pipeline(testX, testY)\r\n'
                 '\r\n'
                 '    # build targeted model\r\n'
                 '    model = ' \
                 'tf.keras.applications.resnet_v2.ResNet50V2(include_top=True, '
                 "weights=None, input_shape=(32,32,3), pooling='max', classes=10)\r\n" \
                 '    #model = tf.keras.applications.vgg16.VGG16(include_top=True, '
                 'weights=None, input_shape=(32,32,3), pooling=None, classes=10)\r\n' \
                 "    model.compile(loss='categorical_crossentropy', optimizer='SGD', "
                 "metrics=['accuracy'])\r\n"
                 '\r\n' \
                 '    # fit and evalutate\r\n'
                 '    model.fit(train_gen,\r\n'
                 '            steps_per_epoch = trainY.shape[0] // batch_size,\r\n'
                 '            validation_data = valid_gen,\r\n'
                 '            validation_steps= validY.shape[0] // batch_size,\r\n'
                 '            epochs=5,\r\n'
                 '            verbose=2)\r\n' \
                 '    model.evaluate(testX, testY, verbose=2, '
                 'batch_size=batch_size)\r\n'
                 '\r\n' \
                 '    # create a new model\r\n' \
                 "    print('Make sure that we create a new model.')\r\n" \
                 '    model = custom_model()\r\n'
                 '    sess.run(tf.compat.v1.global_variables_initializer())\r\n'
                 '    model.clf_model.evaluate(testX, testY, verbose=2, '
                 'batch_size=batch_size)\r\n'
                 '\r\n' \
                 '    # train model\r\n' \
                 '    num_epoch = 5\r\n'
                 '    total_len = trainY.shape[0] // batch_size\r\n'
                 '    tf_iter = ' \
                 'tf.compat.v1.data.make_initializable_iterator(train_gen)\r\n'
                 '    tf_next = tf_iter.get_next()\r\n'
                 '    sess.run(tf_iter.initializer)\r\n' \
                 '    for epoch in range(num_epoch):\r\n'
                 '        c_loss, acc = 0.0, 0.0\r\n'
                 '        for ii in range(total_len):\r\n' \
                 '            X, Y = sess.run(tf_next)\r\n' \
                 '            [b_c_loss, b_acc, _] = sess.run([model.c_loss, '
                 'model.op_acc, model.train_op],\r\n'
                 '                                                feed_dict={ '
                 'model.clf_input: X,\r\n' \
                 '                                                            ' \
                 'model.label_ref: Y,\r\n' \
                 '                                                            ' \
                 'tf.keras.backend.learning_phase(): 1})\r\n'
                 '            c_loss = c_loss + b_c_loss\r\n'
                 '            acc = acc + b_acc\r\n' \
                 '        \r\n'
                 '        c_loss = c_loss / total_len\r\n' \
                 '        acc = acc / total_len\r\n' \
                 "        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: "
                 "{:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n"
                 '\r\n' \
                 "    print('Show loss and accuracy with keras API')\r\n" \
                 '    model.clf_model.evaluate(trainX, trainY, verbose=2, '
                 'batch_size=batch_size)\r\n' \
                 '    model.clf_model.evaluate(validX, validY, verbose=2, ' \
                 'batch_size=batch_size)\r\n'
                 '    model.clf_model.evaluate(testX, testY, verbose=2, ' \
                 'batch_size=batch_size)\r\n'
                 '\r\n' \
                 "    print('Show loss and accuracy with low level API')\r\n" \
                 '    # evaluate\r\n'
                 '    num_epoch = 1\r\n'
                 '    total_len = validY.shape[0] // batch_size\r\n' \
                 '    tf_iter = '
                 'tf.compat.v1.data.make_initializable_iterator(valid_gen)\r\n' \
                 '    tf_next = tf_iter.get_next()\r\n' \
                 '    sess.run(tf_iter.initializer)\r\n'
                 '    for epoch in range(num_epoch):\r\n'
                 '        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n'
                 '        for ii in range(total_len):\r\n' \
                 '            X, Y = sess.run(tf_next)\r\n' \
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, '
                 'model.op_acc],\r\n'
                 '                                        feed_dict={ '
                 'model.clf_input: X,\r\n'
                 '                                                    '
                 'model.label_ref: Y,\r\n' \
                 '                                                    ' \
                 'tf.keras.backend.learning_phase(): 1})\r\n' \
                 '            c_loss_t = c_loss_t + b_c_loss\r\n'
                 '            acc_t = acc_t + b_acc\r\n' \
                 '\r\n' \
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, '
                 'model.op_acc],\r\n'
                 '                                        feed_dict={ ' \
                 'model.clf_input: X,\r\n' \
                 '                                                    '
                 'model.label_ref: Y,\r\n' \
                 '                                                    ' \
                 'tf.keras.backend.learning_phase(): 0})\r\n'
                 '            c_loss_f = c_loss_f + b_c_loss\r\n' \
                 '            acc_f = acc_f + b_acc\r\n' \
                 '\r\n'
                 '        c_loss_t = c_loss_t / total_len\r\n'
                 '        c_loss_f = c_loss_f / total_len\r\n'
                 '        acc_t = acc_t / total_len\r\n'
                 '        acc_f = acc_f / total_len\r\n' \
                 "        print('[Validation][learning_phase=1] Epoch: {:d}/{:d} - "
                 "loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, "
                 'acc_t) )\r\n' \
                 "        print('[Validation][learning_phase=0] Epoch: {:d}/{:d} - "
                 "loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, " \
                 'acc_f) )\r\n' \
                 '\r\n'
                 '    # evaluate\r\n' \
                 '    num_epoch = 1\r\n'
                 '    total_len = testY.shape[0] // batch_size\r\n'
                 '    tf_iter = ' \
                 'tf.compat.v1.data.make_initializable_iterator(test_gen)\r\n'
                 '    tf_next = tf_iter.get_next()\r\n' \
                 '    sess.run(tf_iter.initializer)\r\n'
                 '    for epoch in range(num_epoch):\r\n' \
                 '        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n' \
                 '        for ii in range(total_len):\r\n' \
                 '            X, Y = sess.run(tf_next)\r\n'
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, '
                 'model.op_acc],\r\n' \
                 '                                        feed_dict={ ' \
                 'model.clf_input: X,\r\n' \
                 '                                                    ' \
                 'model.label_ref: Y,\r\n'
                 '                                                    '
                 'tf.keras.backend.learning_phase(): 1})\r\n'
                 '            c_loss_t = c_loss_t + b_c_loss\r\n'
                 '            acc_t = acc_t + b_acc\r\n' \
                 '\r\n' \
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, ' \
                 'model.op_acc],\r\n'
                 '                                        feed_dict={ ' \
                 'model.clf_input: X,\r\n'
                 '                                                    '
                 'model.label_ref: Y,\r\n'
                 '                                                    '
                 'tf.keras.backend.learning_phase(): 0})\r\n' \
                 '            c_loss_f = c_loss_f + b_c_loss\r\n' \
                 '            acc_f = acc_f + b_acc\r\n'
                 '\r\n' \
                 '        c_loss_t = c_loss_t / total_len\r\n'
                 '        c_loss_f = c_loss_f / total_len\r\n' \
                 '        acc_t = acc_t / total_len\r\n'
                 '        acc_f = acc_f / total_len\r\n'
                 "        print('[Testing][learning_phase=1] Epoch: {:d}/{:d} - loss: "
                 "{:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) "
                 ')\r\n'
                 "        print('[Testing][learning_phase=0] Epoch: {:d}/{:d} - loss: "
                 "{:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) " \
                 ')\r\n'
                 '\r\n' \
                 '```\r\n'
                 '\r\n'
                 'The first part of testing case is training model with high leval '
                 'API and the result is as expected.\r\n' \
                 '```\r\n' \
                 '450/450 - 39s - loss: 1.9658 - accuracy: 0.2993 - val_loss: 1.7215 ' \
                 '- val_accuracy: 0.3738\r\n'
                 'Epoch 2/5\r\n'
                 '450/450 - 28s - loss: 1.5722 - accuracy: 0.4334 - val_loss: 1.5897 '
                 '- val_accuracy: 0.4152\r\n' \
                 'Epoch 3/5\r\n'
                 '450/450 - 27s - loss: 1.3876 - accuracy: 0.4993 - val_loss: 1.4867 '
                 '- val_accuracy: 0.4770\r\n' \
                 'Epoch 4/5\r\n' \
                 '450/450 - 28s - loss: 1.2564 - accuracy: 0.5477 - val_loss: 1.3498 '
                 '- val_accuracy: 0.5060\r\n'
                 'Epoch 5/5\r\n' \
                 '450/450 - 27s - loss: 1.1488 - accuracy: 0.5888 - val_loss: 1.3380 '
                 '- val_accuracy: 0.5232\r\n'
                 '10000/10000 - 3s - loss: 1.3523 - accuracy: 0.5289\r\n'
                 '```\r\n'
                 '\r\n' \
                 'I got a strange loss and the ourput can be seen the following:\r\n' \
                 '```\r\n'
                 'Make sure that we create a new model.\r\n'
                 '10000/10000 - 3s - loss: 10.2004 - accuracy: 0.1048\r\n'
                 '[Training]Epoch: 1/5 - loss: 2.288 - acc: 0.268\r\n' \
                 '[Training]Epoch: 2/5 - loss: 1.513 - acc: 0.448\r\n'
                 '[Training]Epoch: 3/5 - loss: 1.285 - acc: 0.537\r\n' \
                 '[Training]Epoch: 4/5 - loss: 1.426 - acc: 0.487\r\n'
                 '[Training]Epoch: 5/5 - loss: 1.306 - acc: 0.535\r\n'
                 'Show loss and accuracy with keras API\r\n'
                 '45000/45000 - 9s - loss: nan - accuracy: 0.1002\r\n'
                 '5000/5000 - 1s - loss: nan - accuracy: 0.0986\r\n'
                 '10000/10000 - 2s - loss: nan - accuracy: 0.1000\r\n'
                 'Show loss and accuracy with low level API\r\n'
                 '[Validation][learning_phase=1] Epoch: 1/1 - loss: 1.163 - acc: '
                 '0.585\r\n' \
                 '[Validation][learning_phase=0] Epoch: 1/1 - loss: nan - acc: '
                 '0.099\r\n' \
                 '[Testing][learning_phase=1] Epoch: 1/1 - loss: 1.179 - acc: ' \
                 '0.587\r\n' \
                 '[Testing][learning_phase=0] Epoch: 1/1 - loss: nan - acc: 0.100\r\n'
                 '```\r\n' \
                 '\r\n' \
                 'Obviously, after training custom model with low level API, the ' \
                 'result would be wrong when setting '
                 '`tf.keras.backend.learning_phase(): 0`\r\n'
                 '\r\n' \
                 'Also, the result from keras API is wrong too.\r\n'
                 '\r\n'
                 '`tf.keras.backend.learning_phase(): 0` may affect the behavior of ' \
                 "`tf.keras.layers.BatchNormalization()` but I'm not sure whether "
                 'this is root cause.\r\n'
                 '\r\n' \
                 'I have tried a small custom model without ' \
                 '`tf.keras.layers.BatchNormalization()` for MNIST dataset and the '
                 'result is normal.\r\n'
                 '\r\n'
                 'The testcase for MNIST as shown in the following:\r\n'
                 '\r\n' \
                 '```python\r\n'
                 'import tensorflow as tf\r\n'
                 'tf.compat.v1.disable_eager_execution()\r\n' \
                 '#tf.compat.v1.disable_v2_behavior()\r\n' \
                 '\r\n'
                 'import numpy as np\r\n' \
                 '\r\n'
                 'batch_size = 100\r\n'
                 '\r\n' \
                 'def download_data():\r\n'
                 '\r\n' \
                 '    # get raw data\r\n'
                 '    (trainX, trainY), (testX, testY) = '
                 'tf.keras.datasets.mnist.load_data()\r\n'
                 '    trainX = trainX.astype(np.float32)\r\n'
                 '    testX  = testX.astype(np.float32)\r\n'
                 '\r\n' \
                 '    # ont-hot\r\n'
                 '    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n'
                 '    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n'
                 '\r\n' \
                 '    # get validation sets\r\n'
                 '    training_size = 55000\r\n'
                 '    validX = trainX[training_size:,:]\r\n'
                 '    validY = trainY[training_size:,:]\r\n' \
                 '\r\n'
                 '    trainX = trainX[:training_size,:]\r\n' \
                 '    trainY = trainY[:training_size,:]\r\n' \
                 '\r\n'
                 '    # expand dimesion\r\n'
                 '    trainX = np.expand_dims(trainX, axis=3)\r\n'
                 '    validX = np.expand_dims(validX, axis=3)\r\n'
                 '    testX  = np.expand_dims(testX , axis=3)\r\n'
                 '\r\n'
                 '    return trainX, trainY, validX, validY, testX, testY\r\n' \
                 '\r\n'
                 'def data_pipeline(dataX, dataY):\r\n' \
                 '\r\n' \
                 '        dataset = tf.data.Dataset.from_tensor_slices( (dataX, ' \
                 'dataY) )\r\n'
                 '        dataset = dataset.shuffle(batch_size * 8)\r\n'
                 '        dataset = dataset.repeat()\r\n'
                 '        dataset = dataset.batch(batch_size)\r\n'
                 '        dataset = ' \
                 'dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n' \
                 '        return dataset\r\n' \
                 '\r\n' \
                 'class custom_model():\r\n'
                 '    def __init__(self):\r\n' \
                 '\r\n'
                 '        def Acc():\r\n'
                 '            acc = tf.keras.metrics.categorical_accuracy(label_ref, '
                 'clf_out)\r\n'
                 '            return tf.math.reduce_mean(acc)\r\n' \
                 '\r\n'
                 '        def c_loss():\r\n'
                 '            loss = '
                 'tf.keras.losses.categorical_crossentropy(label_ref, clf_out)\r\n'
                 '            loss = tf.math.reduce_mean(loss)\r\n'
                 '            return loss\r\n'
                 '\r\n' \
                 '        # declare variables\r\n'
                 '        self.init_op = ' \
                 'tf.compat.v1.keras.initializers.he_normal()\r\n'
                 '        model_layers = [ tf.keras.layers.Conv2D(16, (3, 3), '
                 'padding="same", activation="relu", kernel_initializer=self.init_op, '
                 'name="clf/c1"),\r\n' \
                 '                         tf.keras.layers.Conv2D(32, (3, 3), '
                 'padding="same", activation="relu", kernel_initializer=self.init_op, ' \
                 'name="clf/c2"),\r\n'
                 '                         tf.keras.layers.MaxPooling2D(pool_size=(2, ' \
                 '2), name="clf/p1"),\r\n'
                 '                         tf.keras.layers.Conv2D(32, (3, 3), ' \
                 'padding="same", activation="relu", kernel_initializer=self.init_op, '
                 'name="clf/c3"),\r\n' \
                 '                         tf.keras.layers.Conv2D(64, (3, 3), '
                 'padding="same", activation="relu", kernel_initializer=self.init_op, ' \
                 'name="clf/c4"),\r\n'
                 '                         tf.keras.layers.MaxPooling2D(pool_size=(2, '
                 '2), name="clf/p2"),\r\n' \
                 '                         tf.keras.layers.Flatten(name="clf/f1"),\r\n'
                 '                         tf.keras.layers.Dense(256, ' \
                 'activation="relu", kernel_initializer=self.init_op, '
                 'name="clf/d1"),\r\n'
                 '                         tf.keras.layers.Dense(10 , '
                 'activation=None  , kernel_initializer=self.init_op, ' \
                 'name="clf/d2"),\r\n'
                 "                         tf.keras.layers.Activation('softmax', "
                 'name="clf/a1")\r\n'
                 '                        ]\r\n'
                 '\r\n'
                 '        # clf_model\r\n'
                 '        clf_input = tf.keras.layers.Input(shape=(28,28,1 ), ' \
                 'name="model/input")\r\n'
                 '        clf_out   = clf_input\r\n'
                 '        for ii in model_layers:\r\n' \
                 '            clf_out = ii(clf_out)\r\n' \
                 '        clf_model = tf.keras.models.Model(inputs=clf_input, '
                 "outputs=clf_out, name='clf_model')\r\n"
                 "        clf_model.compile(loss='categorical_crossentropy', "
                 "optimizer='Nadam', metrics=['accuracy'])\r\n"
                 '\r\n'
                 '\r\n' \
                 '        label_ref = tf.keras.layers.Input(shape=(10,) , '
                 "name='label_ref')\r\n"
                 '        clf_out = clf_model(clf_input)\r\n'
                 '\r\n' \
                 '        # using tf.keras.optimizers.Nadam would get error\r\n'
                 '        #optimizer = tf.keras.optimizers.Nadam(lr=0.0005)\r\n'
                 '        optimizer = '
                 'tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\r\n'
                 '        self.train_op = optimizer.minimize(c_loss(), '
                 'var_list=[clf_model.trainable_variables])\r\n'
                 '\r\n'
                 '        self.clf_model = clf_model\r\n'
                 '        self.clf_input = clf_input\r\n'
                 '        self.label_ref = label_ref\r\n'
                 '        self.op_acc = Acc()\r\n'
                 '        self.c_loss = c_loss()\r\n'
                 '\r\n'
                 "if __name__ == '__main__':\r\n" \
                 '\r\n' \
                 '    # set GPU\r\n' \
                 '    import os\r\n'
                 '    if os.environ.get("CUDA_VISIBLE_DEVICES") is None:\r\n'
                 '        os.environ["CUDA_VISIBLE_DEVICES"] = "0"\r\n'
                 '\r\n' \
                 '    # reset tf session\r\n'
                 '    tf.compat.v1.keras.backend.clear_session()\r\n'
                 '    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n' \
                 '    sess = '
                 'tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n' \
                 '    tf.compat.v1.keras.backend.set_session(sess) \r\n'
                 '\r\n' \
                 '    # prepare data\r\n' \
                 '    trainX, trainY, validX, validY, testX, testY = ' \
                 'download_data()\r\n'
                 '    train_gen = data_pipeline(trainX, trainY)\r\n' \
                 '    valid_gen = data_pipeline(validX, validY)\r\n'
                 '    test_gen = data_pipeline(testX, testY)\r\n'
                 '\r\n' \
                 '    # create a new model\r\n'
                 "    print('Make sure that we create a new model.')\r\n"
                 '    model = custom_model()\r\n'
                 '    sess.run(tf.compat.v1.global_variables_initializer())\r\n' \
                 '    model.clf_model.evaluate(testX, testY, verbose=2, ' \
                 'batch_size=batch_size)\r\n'
                 '\r\n'
                 '    # train model\r\n'
                 '    num_epoch = 5\r\n'
                 '    total_len = trainY.shape[0] // batch_size\r\n'
                 '    tf_iter = '
                 'tf.compat.v1.data.make_initializable_iterator(train_gen)\r\n'
                 '    tf_next = tf_iter.get_next()\r\n'
                 '    sess.run(tf_iter.initializer)\r\n'
                 '    for epoch in range(num_epoch):\r\n'
                 '        c_loss, acc = 0.0, 0.0\r\n'
                 '        for ii in range(total_len):\r\n'
                 '            X, Y = sess.run(tf_next)\r\n'
                 '            [b_c_loss, b_acc, _] = sess.run([model.c_loss, '
                 'model.op_acc, model.train_op],\r\n' \
                 '                                                feed_dict={ '
                 'model.clf_input: X,\r\n'
                 '                                                            '
                 'model.label_ref: Y,\r\n'
                 '                                                            '
                 'tf.keras.backend.learning_phase(): 1})\r\n'
                 '            c_loss = c_loss + b_c_loss\r\n'
                 '            acc = acc + b_acc\r\n'
                 '        \r\n'
                 '        c_loss = c_loss / total_len\r\n'
                 '        acc = acc / total_len\r\n'
                 "        print('[Training]Epoch: {:d}/{:d} - loss: {:.3f} - acc: " \
                 "{:.3f}'.format(epoch+1, num_epoch, c_loss, acc) )\r\n" \
                 '\r\n' \
                 "    print('Show loss and accuracy with keras API')\r\n" \
                 '    model.clf_model.evaluate(trainX, trainY, verbose=2, '
                 'batch_size=batch_size)\r\n' \
                 '    model.clf_model.evaluate(validX, validY, verbose=2, ' \
                 'batch_size=batch_size)\r\n' \
                 '    model.clf_model.evaluate(testX, testY, verbose=2, '
                 'batch_size=batch_size)\r\n' \
                 '\r\n' \
                 "    print('Show loss and accuracy with low level API')\r\n" \
                 '    # evaluate\r\n'
                 '    num_epoch = 1\r\n'
                 '    total_len = validY.shape[0] // batch_size\r\n'
                 '    tf_iter = '
                 'tf.compat.v1.data.make_initializable_iterator(valid_gen)\r\n'
                 '    tf_next = tf_iter.get_next()\r\n' \
                 '    sess.run(tf_iter.initializer)\r\n'
                 '    for epoch in range(num_epoch):\r\n'
                 '        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n' \
                 '        for ii in range(total_len):\r\n' \
                 '            X, Y = sess.run(tf_next)\r\n'
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, '
                 'model.op_acc],\r\n'
                 '                                        feed_dict={ '
                 'model.clf_input: X,\r\n'
                 '                                                    '
                 'model.label_ref: Y,\r\n'
                 '                                                    '
                 'tf.keras.backend.learning_phase(): 1})\r\n'
                 '            c_loss_t = c_loss_t + b_c_loss\r\n'
                 '            acc_t = acc_t + b_acc\r\n'
                 '\r\n'
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, '
                 'model.op_acc],\r\n'
                 '                                        feed_dict={ ' \
                 'model.clf_input: X,\r\n'
                 '                                                    '
                 'model.label_ref: Y,\r\n' \
                 '                                                    '
                 'tf.keras.backend.learning_phase(): 0})\r\n' \
                 '            c_loss_f = c_loss_f + b_c_loss\r\n'
                 '            acc_f = acc_f + b_acc\r\n'
                 '\r\n' \
                 '        c_loss_t = c_loss_t / total_len\r\n' \
                 '        c_loss_f = c_loss_f / total_len\r\n' \
                 '        acc_t = acc_t / total_len\r\n'
                 '        acc_f = acc_f / total_len\r\n' \
                 "        print('[Validation][learning_phase=1] Epoch: {:d}/{:d} - "
                 "loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, " \
                 'acc_t) )\r\n'
                 "        print('[Validation][learning_phase=0] Epoch: {:d}/{:d} - " \
                 "loss: {:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, "
                 'acc_f) )\r\n' \
                 '\r\n'
                 '    # evaluate\r\n'
                 '    num_epoch = 1\r\n' \
                 '    total_len = testY.shape[0] // batch_size\r\n' \
                 '    tf_iter = '
                 'tf.compat.v1.data.make_initializable_iterator(test_gen)\r\n' \
                 '    tf_next = tf_iter.get_next()\r\n' \
                 '    sess.run(tf_iter.initializer)\r\n'
                 '    for epoch in range(num_epoch):\r\n' \
                 '        c_loss_t, acc_t, c_loss_f, acc_f = 0.0, 0.0, 0.0, 0.0\r\n'
                 '        for ii in range(total_len):\r\n'
                 '            X, Y = sess.run(tf_next)\r\n'
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, '
                 'model.op_acc],\r\n' \
                 '                                        feed_dict={ '
                 'model.clf_input: X,\r\n'
                 '                                                    '
                 'model.label_ref: Y,\r\n'
                 '                                                    '
                 'tf.keras.backend.learning_phase(): 1})\r\n'
                 '            c_loss_t = c_loss_t + b_c_loss\r\n'
                 '            acc_t = acc_t + b_acc\r\n'
                 '\r\n'
                 '            [b_c_loss, b_acc] = sess.run([model.c_loss, '
                 'model.op_acc],\r\n'
                 '                                        feed_dict={ ' \
                 'model.clf_input: X,\r\n'
                 '                                                    ' \
                 'model.label_ref: Y,\r\n' \
                 '                                                    ' \
                 'tf.keras.backend.learning_phase(): 0})\r\n'
                 '            c_loss_f = c_loss_f + b_c_loss\r\n' \
                 '            acc_f = acc_f + b_acc\r\n'
                 '\r\n'
                 '        c_loss_t = c_loss_t / total_len\r\n'
                 '        c_loss_f = c_loss_f / total_len\r\n'
                 '        acc_t = acc_t / total_len\r\n'
                 '        acc_f = acc_f / total_len\r\n'
                 "        print('[Testing][learning_phase=1] Epoch: {:d}/{:d} - loss: " \
                 "{:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_t, acc_t) "
                 ')\r\n' \
                 "        print('[Testing][learning_phase=0] Epoch: {:d}/{:d} - loss: " \
                 "{:.3f} - acc: {:.3f}'.format(epoch+1, num_epoch, c_loss_f, acc_f) "
                 ')\r\n'
                 '```\r\n' \
                 '\r\n' \
                 'Definitely, we got a very normal output:\r\n'
                 '\r\n' \
                 '```\r\n'
                 'Make sure that we create a new model.\r\n'
                 '10000/10000 - 1s - loss: 398.0696 - acc: 0.1151\r\n'
                 '[Training]Epoch: 1/5 - loss: 11.997 - acc: 0.558\r\n' \
                 '[Training]Epoch: 2/5 - loss: 0.474 - acc: 0.849\r\n'
                 '[Training]Epoch: 3/5 - loss: 0.282 - acc: 0.914\r\n'
                 '[Training]Epoch: 4/5 - loss: 0.213 - acc: 0.935\r\n'
                 '[Training]Epoch: 5/5 - loss: 0.181 - acc: 0.945\r\n'
                 'Show loss and accuracy with keras API\r\n' \
                 '55000/55000 - 1s - loss: 0.1555 - acc: 0.9535\r\n'
                 '5000/5000 - 0s - loss: 0.1501 - acc: 0.9584\r\n'
                 '10000/10000 - 0s - loss: 0.1687 - acc: 0.9539\r\n' \
                 'Show loss and accuracy with low level API\r\n' \
                 '[Validation][learning_phase=1] Epoch: 1/1 - loss: 0.150 - acc: '
                 '0.958\r\n' \
                 '[Validation][learning_phase=0] Epoch: 1/1 - loss: 0.150 - acc: '
                 '0.958\r\n' \
                 '[Testing][learning_phase=1] Epoch: 1/1 - loss: 0.169 - acc: ' \
                 '0.954\r\n'
                 '[Testing][learning_phase=0] Epoch: 1/1 - loss: 0.169 - acc: '
                 '0.954\r\n' \
                 '```\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n' \
                 '\r\n'
                 'It should work properly.\r\n' \
                 '\r\n'
                 '**Code to reproduce the issue**\r\n' \
                 '\r\n'
                 'Please see the section of **Describe the current behavior**\r\n' \
                 '\r\n'
                 '**Other info / logs**\r\n'
                 '\r\n'
                 'skip ...',
         'created_at': '2019-12-1'}, \
        {'body': '<em>Please make sure that this is a build/installation issue. As '
                 'per our [GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), ' \
                 'we only address code/doc bugs, performance issues, feature requests ' \
                 'and build/installation issues on GitHub. tag:build_template</em>\r\n' \
                 '\r\n'
                 '**System information**\r\n' \
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro ' \
                 'Linux (kernel 4.19)\r\n' \
                 "- TensorFlow installed from (source or binary): binary? I'm not "
                 'sure, I used `pip install tensorflow`\r\n' \
                 '- TensorFlow version: 2.0.0\r\n'
                 '- Python version: Python 3.6.8\r\n' \
                 '- Installed using virtualenv? pip? conda?: pip. This is in a '
                 'virtualenv environment though.\r\n' \
                 '\r\n' \
                 '**Describe the problem**\r\n' \
                 'Running `tf_upgrade_v2` on a file that has async functions causes ' \
                 'it to crash. \r\n' \
                 '\r\n' \
                 '**Provide the exact sequence of commands / steps that you executed ' \
                 'before running into the problem**\r\n'
                 '```tf_upgrade_v2 --infile ./image_classification/test_tf.py ' \
                 '--outfile ./image_classification_v2/test_tf.py\r\n'
                 'Traceback (most recent call last):\r\n' \
                 '  File "/home/alex/git-repos/bic-bot-py/bin/tf_upgrade_v2", line 8, '
                 'in <module>\r\n'
                 '    sys.exit(main())\r\n' \
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py", ' \
                 'line 139, in main\r\n'
                 '    args.input_file, output_file, upgrade)\r\n'
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/tf_upgrade_v2_main.py", '
                 'line 40, in process_file\r\n' \
                 '    upgrader.process_file(in_filename, out_filename)\r\n'
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py", '
                 'line 900, in process_file\r\n'
                 '    temp_file)\r\n' \
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py", '
                 'line 960, in process_opened_file\r\n' \
                 '    self.update_string_pasta("".join(lines), in_filename))\r\n'
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/tensorflow_core/tools/compatibility/ast_edits.py", '
                 'line 916, in update_string_pasta\r\n' \
                 '    t = pasta.parse(text)\r\n'
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/__init__.py", '
                 'line 25, in parse\r\n'
                 '    annotator.visit(t)\r\n'
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 1201, in visit\r\n' \
                 '    super(AstAnnotator, self).visit(node)\r\n'
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", ' \
                 'line 133, in visit\r\n' \
                 '    super(BaseVisitor, self).visit(node)\r\n'
                 '  File "/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py", ' \
                 'line 253, in visit\r\n' \
                 '    return visitor(node)\r\n' \
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 47, in wrapped\r\n'
                 '    f(self, node, *args, **kwargs)\r\n' \
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 225, in visit_Module\r\n'
                 '    self.generic_visit(node)\r\n' \
                 '  File "/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py", '
                 'line 261, in generic_visit\r\n'
                 '    self.visit(item)\r\n' \
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 1201, in visit\r\n'
                 '    super(AstAnnotator, self).visit(node)\r\n' \
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", ' \
                 'line 133, in visit\r\n'
                 '    super(BaseVisitor, self).visit(node)\r\n' \
                 '  File "/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py", '
                 'line 253, in visit\r\n'
                 '    return visitor(node)\r\n'
                 '  File "/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py", ' \
                 'line 261, in generic_visit\r\n'
                 '    self.visit(item)\r\n'
                 '  File '
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 1201, in visit\r\n'
                 '    super(AstAnnotator, self).visit(node)\r\n' \
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 133, in visit\r\n' \
                 '    super(BaseVisitor, self).visit(node)\r\n'
                 '  File "/home/alex/.pyenv/versions/3.6.8/lib/python3.6/ast.py", '
                 'line 253, in visit\r\n' \
                 '    return visitor(node)\r\n'
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 47, in wrapped\r\n' \
                 '    f(self, node, *args, **kwargs)\r\n'
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", '
                 'line 673, in visit_Return\r\n'
                 "    self.token('return')\r\n"
                 '  File ' \
                 '"/home/alex/git-repos/bic-bot-py/lib/python3.6/site-packages/pasta/base/annotate.py", ' \
                 'line 1340, in token\r\n'
                 '    token_val, token.src, token.start[0], token.line))\r\n'
                 "pasta.base.annotate.AnnotationError: Expected 'return' but found "
                 "'async'\r\n"
                 'line 1: async def f():\r\n'
                 '```\r\n'
                 '\r\n' \
                 '**Any other info / logs**\r\n'
                 'test_tf.py:\r\n'
                 '```\r\n' \
                 'async def f():\r\n'
                 '    return\r\n'
                 '```',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example ' \
                 'script provided in TensorFlow): N/A\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu '
                 '16.04.6\r\n' \
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the ' \
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary): binary\r\n'
                 '- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 ' \
                 '2.0.0\r\n'
                 '- Python version: 3.7.3\r\n'
                 '- Bazel version (if compiling from source): N/A\r\n'
                 '- GCC/Compiler version (if compiling from source): N/A\r\n'
                 '- CUDA/cuDNN version: V10.0.130\r\n'
                 '- GPU model and memory: Tesla V100-SXM2\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 'In the Adam paper, we subtract the following quantity from our '
                 'current gradient [0]:\r\n'
                 '\\alpha * \\hat{m_t} / (\\sqrt{v_t / (1-\\beta^t_2)}  + '
                 '\\epsilon)\r\n'
                 '\r\n' \
                 '<img width="475" alt="Screen Shot 2019-12-13 at 2 32 58 PM" ' \
                 'src="https://user-images.githubusercontent.com/54961543/70836600-7f490780-1db5-11ea-9669-27c50fe48cae.png">\r\n' \
                 '\r\n' \
                 'The Tensorflow implementation subtracts a subtly different quantity ' \
                 '([1]):\r\n'
                 '\\alpha * \\hat{m_t} * \\sqrt{1-\\beta^T_2} / (\\sqrt{v_t} + '
                 '\\epsilon)\r\n' \
                 '\r\n'
                 '<img width="438" alt="Screen Shot 2019-12-13 at 2 34 36 PM" ' \
                 'src="https://user-images.githubusercontent.com/54961543/70836654-c1724900-1db5-11ea-9260-5fc0678f6f39.png">\r\n'
                 '\r\n'
                 'The difference between the two expressions is that in the first, we ' \
                 'de-bias only the moving average of the squared gradient, v_t. In '
                 'the second, this bias correction is also applied to \\epsilon. This '
                 'manifests as scaling up epilson quite a lot in very early training ' \
                 'steps, reducing the magnitude of the gradient update.\r\n'
                 '\r\n'
                 'Note that the same bug was present in PyTorch prior to v1.3. It was '
                 'fixed in this PR: https://github.com/pytorch/pytorch/pull/22628. '
                 'That PR description provides a useful visualization.\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n' \
                 'Implement the algorithm as described in the paper. If the old '
                 'implementation is necessary to preserve back-compat, providing a '
                 'flag to trigger the correct implementation would be most ' \
                 'helpful.\r\n' \
                 '\r\n'
                 '\r\n'
                 '[0] https://arxiv.org/pdf/1412.6980.pdf, see final 2 lines of '
                 'Algorithm 1\r\n' \
                 '[1] Notable lines in TF implementation regarding this issue: \r\n'
                 'https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L162\r\n' \
                 '\r\n'
                 'https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/optimizer_v2/adam.py#L245\r\n'
                 '\r\n'
                 'https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L373\r\n'
                 '\r\n',
         'created_at': '2019-12-1'},
        {'body': "I don't understand the cause of different handling of shapes for "
                 'two methods, here is the snippet:\r\n'
                 '\r\n'
                 '```python\r\n'
                 'import tensorflow as tf\r\n'
                 'import tensorflow_probability as tfp\r\n'
                 '\r\n'
                 'def _log_prob(x):\r\n'
                 "    x = tf.convert_to_tensor(x, name='x')\r\n"
                 '    distribution_log_probs = [x for i in range(5)]\r\n'
                 '    cat_log_probs = [x for i in range(5)]\r\n'
                 '    final_log_probs = [\r\n'
                 '        cat_lp + d_lp\r\n'
                 '        for (cat_lp, d_lp) in zip(cat_log_probs, '
                 'distribution_log_probs)\r\n'
                 '    ]\r\n'
                 '    concat_log_probs = tf.stack(final_log_probs, 0)\r\n'
                 '    log_sum = tf.reduce_logsumexp(concat_log_probs, axis=[0])\r\n'
                 '    # log_sum = tf.reduce_sum(concat_log_probs, axis=[0])\r\n'
                 '    return log_sum\r\n'
                 '\r\n'
                 '@tf.function(autograph=False)\r\n' \
                 'def f():\r\n'
                 '    log_prob = tf.vectorized_map(_log_prob, tf.ones((1,5)))\r\n'
                 '    print(log_prob.shape) # prints (None, 5) for '
                 '`tf.reduce_logsumexp` and (1, 5) for `tf.reduce_sum`\r\n' \
                 '```\r\n'
                 '\r\n'
                 'So basically `tf.reduce_logsumexp` gives dynamic shape for the '
                 'output tensor while `tf.reduce_sum` assigns static shape. Can '
                 'anybody please give some clear picture on such behaviour and is it '
                 'expected?\r\n'
                 '\r\n'
                 '```\r\n'
                 'tf: 2.0.0\r\n'
                 'tfp: 0.8.0\r\n'
                 '```', \
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX ' \
                 '10.14.6\r\n'
                 '- TensorFlow installed from (source or binary): source\r\n'
                 '- TensorFlow version: 1.15.0 (same observation for version 2.0), '
                 'from '
                 'https://github.com/tensorflow/tensorflow/releases/tag/v1.15.0\r\n'
                 '- Python version: python3\r\n'
                 '- GCC/Compiler version (if compiling from source): '
                 'arm-none-eabi-g++\r\n'
                 '\r\n' \
                 '**Describe the problem**\r\n'
                 'The standalone projects generated with Tensorflow Lite seem to be '
                 'missing some headers.\r\n' \
                 '\r\n'
                 'Following this tutorial: \r\n'
                 '[https://www.tensorflow.org/lite/microcontrollers/library#generate_projects_for_other_platforms\r\n'
                 '](https://www.tensorflow.org/lite/microcontrollers/library#generate_projects_for_other_platforms\r\n'
                 ')\r\n' \
                 '\r\n'
                 'I run the following command (the link seems to be off here in the '
                 "tutorial, as 'micro' is in another subfolder' experimental'):\r\n" \
                 '`gmake -f tensorflow/lite/experimental/micro/tools/make/Makefile ' \
                 'TARGET=sparkfun_edge generate_projects`\r\n'
                 '\r\n'
                 'After this command is finished, the files are generated as expected '
                 'in:\r\n'
                 '`tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/prj`\r\n'
                 '\r\n'
                 'But when trying to build the micro_speech binary like this:\r\n'
                 '```\r\n'
                 'cd '
                 'tensorflow/lite/experimental/micro/tools/make/gen/sparkfun_edge_cortex-m4/prj/micro_speech/make\r\n'
                 'gmake\r\n'
                 '```\r\n' \
                 '\r\n'
                 'I am getting the following error:\r\n' \
                 '```\r\n'
                 'arm-none-eabi-g++ -O3 -DNDEBUG -std=c++11 -g '
                 '-DTF_LITE_STATIC_MEMORY -DPART_apollo3 -DAM_PACKAGE_BGA '
                 '-DAM_PART_APOLLO3 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '
                 '-DTF_LITE_STATIC_MEMORY -DNDEBUG -DTF_LITE_MCU_DEBUG_LOG -D ' \
                 '__FPU_PRESENT=1 -DARM_MATH_CM4 -fno-rtti -fmessage-length=0 '
                 '-fno-exceptions -fno-unwind-tables -fno-builtin -ffunction-sections '
                 '-fdata-sections -funsigned-char -MMD -mcpu=cortex-m4 -mthumb '
                 '-mfpu=fpv4-sp-d16 -mfloat-abi=hard -std=gnu++11 -Wvla -Wall -Wextra '
                 '-Wno-unused-parameter -Wno-missing-field-initializers '
                 '-Wno-write-strings -Wno-sign-compare '
                 '-fno-delete-null-pointer-checks -fomit-frame-pointer -fpermissive '
                 '-nostdlib -ggdb -O3 -I. -I./third_party/gemmlowp '
                 '-I./third_party/flatbuffers/include -I./third_party/kissfft  -c '
                 'tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.cc -o '
                 'tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.o\r\n' \
                 'tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.cc:22:10: '
                 'fatal error: am_bsp.h: No such file or directory\r\n'
                 ' #include "am_bsp.h"   // NOLINT\r\n'
                 '          ^~~~~~~~~~\r\n'
                 'compilation terminated.\r\n'
                 'make: *** '
                 '[tensorflow/lite/experimental/micro/sparkfun_edge/debug_log.o] '
                 'Error 1\r\n'
                 '```\r\n' \
                 '\r\n'
                 'This is the header file of the sparkfun edge board that I want to '
                 'build this for.\r\n',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow): Yes\r\n'
                 '- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux '
                 'Ubuntu 16.04 in Docker\r\n'
                 '- TensorFlow installed from (source or binary): pip install\r\n' \
                 '- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38\r\n'
                 '- Python version: 3.5\r\n' \
                 '- CUDA/cuDNN version: 10.0 / 7\r\n' \
                 '- GPU model and memory: GTX 1080Ti / 11175MiB\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n' \
                 'Hi authors and developers,\r\n'
                 '\r\n'
                 'I am developing our project in tf=2.0.0 and eager_mode is ' \
                 'disable.\r\n' \
                 '\r\n'
                 'The main reason is tf=1.x will not be maintained but third party '
                 'libraries have not been ready for tf=2.0 yet.\r\n' \
                 '\r\n' \
                 'I got bug when I was running custom loss keras model with ' \
                 'tf.distribute.MirroredStrategy()\r\n'
                 '\r\n'
                 'This bug can be reproduced by the following minimal testcase:\r\n'
                 '\r\n'
                 '```python\r\n'
                 '#%%\r\n'
                 'from distutils.version import LooseVersion\r\n'
                 'import numpy as np\r\n'
                 'import tensorflow as tf\r\n'
                 '\r\n' \
                 '# disable eager model for tf=2.x\r\n'
                 'tf.compat.v1.disable_eager_execution()\r\n'
                 '\r\n'
                 'batch_size = 100\r\n'
                 'img_h = 32\r\n'
                 'img_w = 32\r\n'
                 'img_min = 0\r\n'
                 'img_max = 1\r\n'
                 'channels = 3\r\n'
                 'num_classes = 10\r\n'
                 '\r\n'
                 'strategy = tf.distribute.MirroredStrategy()\r\n'
                 '#%%\r\n'
                 'def download_data():\r\n'
                 '\r\n'
                 '    # get raw data\r\n'
                 '    (trainX, trainY), (testX, testY) = ' \
                 'tf.keras.datasets.cifar10.load_data()\r\n'
                 '    trainX = trainX.astype(np.float32)\r\n'
                 '    testX  = testX.astype(np.float32)\r\n'
                 '\r\n'
                 '    # ont-hot\r\n'
                 '    trainY = tf.keras.utils.to_categorical(trainY, 10)\r\n'
                 '    testY  = tf.keras.utils.to_categorical(testY , 10)\r\n'
                 '\r\n'
                 '    # get validation sets\r\n'
                 '    training_size = 45000\r\n'
                 '    validX = trainX[training_size:,:]\r\n'
                 '    validY = trainY[training_size:,:]\r\n'
                 '\r\n'
                 '    trainX = trainX[:training_size,:]\r\n' \
                 '    trainY = trainY[:training_size,:]\r\n'
                 '\r\n'
                 '    return trainX, trainY, validX, validY, testX, testY\r\n'
                 '\r\n' \
                 '#%%\r\n'
                 'class DataGenerator:\r\n'
                 '\r\n'
                 '    def __init__(self, sess, dataX, dataY, total_len, ' \
                 'batch_size):\r\n'
                 '\r\n' \
                 '        super().__init__()\r\n'
                 '\r\n'
                 '        self.total_len  = total_len\r\n'
                 '        self.batch_size = batch_size\r\n'
                 '        self.cleanX = dataX\r\n'
                 '        self.totalY = dataY\r\n' \
                 '        self.sess = sess\r\n'
                 '        self.on_epoch_end()\r\n'
                 '\r\n'
                 '    def __build_pipeline(self, dataX, dataY):\r\n'
                 '\r\n' \
                 '        # create dataset API\r\n'
                 '        def preprocess_fn(dataX, dataY):\r\n'
                 '            \r\n' \
                 '            dataX = tf.image.random_flip_left_right(dataX)\r\n' \
                 '\r\n'
                 '            # workaround solution\r\n'
                 '            if LooseVersion(tf.__version__) < ' \
                 "LooseVersion('1.14.0'):\r\n"
                 '                outputX = dataX\r\n'
                 '            else:\r\n' \
                 '                outputX = (dataX, dataY)\r\n'
                 '            return outputX, dataY\r\n'
                 '\r\n'
                 '        dataset = tf.data.Dataset.from_tensor_slices( (dataX, '
                 'dataY) )\r\n' \
                 '        dataset = dataset.shuffle(batch_size * 8)\r\n' \
                 '        dataset = dataset.repeat()\r\n'
                 '        dataset = dataset.batch(batch_size)\r\n'
                 '        dataset = dataset.map(preprocess_fn, ' \
                 'num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n' \
                 '        dataset = '
                 'dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n'
                 '\r\n' \
                 '        self.dataset   = dataset\r\n' \
                 '\r\n'
                 '    def  __len__(self):\r\n'
                 '\r\n'
                 '        return self.total_len // self.batch_size\r\n'
                 '\r\n'
                 '    def on_epoch_end(self):\r\n'
                 '\r\n'
                 '        # run permutation\r\n'
                 '        rand_idx = np.random.permutation(self.total_len)\r\n'
                 '        cleanX = self.cleanX[rand_idx]\r\n'
                 '        totalY = self.totalY[rand_idx]\r\n'
                 '\r\n'
                 '        self.__build_pipeline(cleanX, totalY)\r\n'
                 '\r\n' \
                 '#%%\r\n'
                 '# ref: https://keras.io/examples/cifar10_resnet/\r\n'
                 'def build_clf():\r\n'
                 '    #with strategy.scope():\r\n'
                 "    with tf.compat.v1.variable_scope('optimizer'):\r\n" \
                 '        def resnet_layer(inputs,\r\n' \
                 '                        num_filters=16,\r\n'
                 '                        kernel_size=3,\r\n'
                 '                        strides=1,\r\n' \
                 "                        activation='relu',\r\n"
                 '                        batch_normalization=True,\r\n'
                 '                        conv_first=True):\r\n'
                 '            """2D Convolution-Batch Normalization-Activation stack ' \
                 'builder\r\n'
                 '\r\n'
                 '            # Arguments\r\n'
                 '                inputs (tensor): input tensor from input image or '
                 'previous layer\r\n'
                 '                num_filters (int): Conv2D number of filters\r\n' \
                 '                kernel_size (int): Conv2D square kernel ' \
                 'dimensions\r\n'
                 '                strides (int): Conv2D square stride dimensions\r\n'
                 '                activation (string): activation name\r\n'
                 '                batch_normalization (bool): whether to include ' \
                 'batch normalization\r\n' \
                 '                conv_first (bool): conv-bn-activation (True) or\r\n'
                 '                    bn-activation-conv (False)\r\n'
                 '\r\n'
                 '            # Returns\r\n'
                 '                x (tensor): tensor as input to the next layer\r\n'
                 '            """\r\n'
                 '            conv = tf.keras.layers.Conv2D(num_filters,\r\n'
                 '                        kernel_size=kernel_size,\r\n'
                 '                        strides=strides,\r\n'
                 "                        padding='same',\r\n"
                 "                        kernel_initializer='he_normal',\r\n" \
                 '                        ' \
                 'kernel_regularizer=tf.keras.regularizers.l2(1e-4))\r\n'
                 '\r\n' \
                 '            x = inputs\r\n'
                 '            if conv_first:\r\n'
                 '                x = conv(x)\r\n'
                 '                if batch_normalization:\r\n'
                 '                    x = tf.keras.layers.BatchNormalization()(x)\r\n'
                 '                if activation is not None:\r\n' \
                 '                    x = '
                 'tf.keras.layers.Activation(activation)(x)\r\n' \
                 '            else:\r\n'
                 '                if batch_normalization:\r\n'
                 '                    x = tf.keras.layers.BatchNormalization()(x)\r\n'
                 '                if activation is not None:\r\n'
                 '                    x = '
                 'tf.keras.layers.Activation(activation)(x)\r\n'
                 '                x = conv(x)\r\n' \
                 '            return x\r\n' \
                 '\r\n'
                 '        def cw_loss(y_true, y_pred):\r\n'
                 '            label_mask  = label_ref\r\n'
                 '            pre_softmax = x\r\n'
                 '            if LooseVersion(tf.__version__) < '
                 "LooseVersion('1.14.0'):\r\n"
                 '                correct_logit = tf.reduce_sum(label_mask * '
                 'pre_softmax, axis=1, keep_dims=True)\r\n'
                 '            else:\r\n'
                 '                correct_logit = tf.reduce_sum(label_mask * '
                 'pre_softmax, axis=1, keepdims=True)\r\n'
                 '            distance = tf.nn.relu( pre_softmax - correct_logit + ' \
                 '(1-label_mask) * 10)\r\n'
                 '            inactivate = tf.cast( tf.less_equal(distance, 1e-9), ' \
                 'dtype=tf.float32)\r\n'
                 '            weight = '
                 "tf.keras.layers.Activation('softmax')(-1e9*inactivate + " \
                 'distance)\r\n'
                 '            loss = tf.reduce_sum((1-label_mask) * distance * '
                 'weight, axis=1)\r\n' \
                 '            loss = tf.math.reduce_mean(loss)\r\n'
                 '            return loss\r\n' \
                 '\r\n' \
                 "        # set model's parameters (depth = n * 6 + 2)\r\n"
                 '        n = 8\r\n'
                 '        num_filters = 16\r\n'
                 '\r\n' \
                 '        clf_input = tf.keras.layers.Input(shape=(img_h, img_w, '
                 'channels), name="model/input")\r\n'
                 '        label_ref = tf.keras.layers.Input(shape=(num_classes,), '
                 "name='label_ref')\r\n"
                 '        input_list = [clf_input, label_ref]\r\n'
                 '\r\n'
                 '        x = resnet_layer(inputs=clf_input)\r\n'
                 '        for stack in range(3):\r\n'
                 '            for res_block in range(n):\r\n'
                 '                strides = 1\r\n'
                 '                if stack > 0 and res_block == 0:  # first layer but ' \
                 'not first stack\r\n' \
                 '                    strides = 2  # downsample\r\n'
                 '                y = resnet_layer(inputs=x,\r\n' \
                 '                                num_filters=num_filters,\r\n'
                 '                                strides=strides)\r\n'
                 '                y = resnet_layer(inputs=y,\r\n' \
                 '                                num_filters=num_filters,\r\n'
                 '                                activation=None)\r\n'
                 '                if stack > 0 and res_block == 0:  # first layer but '
                 'not first stack\r\n'
                 '                    # linear projection residual shortcut '
                 'connection to match\r\n' \
                 '                    # changed dims\r\n' \
                 '                    x = resnet_layer(inputs=x,\r\n'
                 '                                    num_filters=num_filters,\r\n'
                 '                                    kernel_size=1,\r\n'
                 '                                    strides=strides,\r\n'
                 '                                    activation=None,\r\n'
                 '                                    batch_normalization=False)\r\n'
                 '                x = tf.keras.layers.Add()([x, y])\r\n'
                 "                x = tf.keras.layers.Activation('relu')(x)\r\n"
                 '            num_filters *= 2\r\n' \
                 '\r\n' \
                 '        x = tf.keras.layers.AveragePooling2D(pool_size=8)(x)\r\n'
                 '        x = tf.keras.layers.Flatten()(x)\r\n' \
                 '        x = tf.keras.layers.Dense(num_classes , '
                 "kernel_initializer='he_normal', activation=None)(x)\r\n"
                 "        y = tf.keras.layers.Activation('softmax')(x)\r\n"
                 '\r\n' \
                 '        optimizer = tf.keras.optimizers.Adam(lr=0.001)\r\n' \
                 '        clf_model = tf.keras.models.Model(inputs=input_list, '
                 "outputs=y, name='clf_model')\r\n"
                 "        clf_model.compile(loss='categorical_crossentropy', " \
                 "optimizer=optimizer, metrics=['accuracy', cw_loss])\r\n"
                 '    clf_model.summary()\r\n'
                 '\r\n'
                 '    return clf_model\r\n' \
                 '\r\n'
                 '#%%\r\n'
                 "if __name__ == '__main__':\r\n"
                 '\r\n'
                 '    # set GPU\r\n'
                 '    import os\r\n'
                 '    if os.environ.get("CUDA_VISIBLE_DEVICES") is None:\r\n'
                 '        os.environ["CUDA_VISIBLE_DEVICES"] = "0"\r\n'
                 '\r\n'
                 '    # reset tf session\r\n'
                 '    tf.compat.v1.keras.backend.clear_session()\r\n' \
                 '    gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\r\n'
                 '    sess = '
                 'tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\r\n'
                 '    tf.compat.v1.keras.backend.set_session(sess)\r\n'
                 '\r\n'
                 '    # Hyperparameters\r\n'
                 '    batch_size = 100\r\n'
                 '    epochs = 1\r\n'
                 '\r\n'
                 '    # prepare data\r\n'
                 '    trainX, trainY, validX, validY, testX, testY = ' \
                 'download_data()\r\n'
                 '    train_gen = DataGenerator(sess, trainX, trainY, ' \
                 'trainY.shape[0], batch_size)\r\n'
                 '    valid_gen = DataGenerator(sess, validX, validY, ' \
                 'validY.shape[0], batch_size)\r\n'
                 '    test_gen  = DataGenerator(sess, testX, testY, testY.shape[0], '
                 'batch_size)\r\n'
                 '\r\n' \
                 '    # build model\r\n'
                 '    model = build_clf()\r\n'
                 '\r\n'
                 '    # train model\r\n'
                 '    model.fit(train_gen.dataset,\r\n' \
                 '                    epochs=epochs,\r\n'
                 '                    steps_per_epoch = train_gen.__len__(),\r\n'
                 '                    validation_data=valid_gen.dataset,\r\n'
                 '                    validation_steps= valid_gen.__len__(),\r\n'
                 '                    verbose=1)\r\n' \
                 '\r\n'
                 '    # print result\r\n' \
                 "    meta_string = '[Testing]'\r\n"
                 "    prefix_string = ''\r\n" \
                 '    output = model.evaluate(test_gen.dataset, steps = ' \
                 'test_gen.__len__())\r\n'
                 '    for ii in range( len( model.metrics_names) ):\r\n'
                 "        meta_string = meta_string + '- {:s}{:s}: {:.3f} " \
                 "'.format(prefix_string, model.metrics_names[ii], output[ii])\r\n" \
                 '\r\n'
                 '    print(meta_string)\r\n'
                 '```\r\n'
                 '\r\n'
                 'First, this testing case looks good without enabling '
                 '`tf.distribute.MirroredStrategy()`\r\n'
                 '\r\n'
                 'There is the output for normal case:\r\n'
                 '\r\n' \
                 '```\r\n' \
                 'Train on 450 steps, validate on 50 steps\r\n'
                 '2019-12-13 16:20:30.625379: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] ' \
                 'Successfully opened dynamic library libcublas.so.10.0\r\n' \
                 '2019-12-13 16:20:31.217430: I '
                 'tensorflow/stream_executor/platform/default/dso_loader.cc:44] '
                 'Successfully opened dynamic library libcudnn.so.7\r\n'
                 '2019-12-13 16:20:33.007150: W '
                 'tensorflow/stream_executor/cuda/redzone_allocator.cc:312] Not '
                 'found: ./bin/ptxas not found\r\n' \
                 'Relying on driver to perform ptx compilation. This message will be '
                 'only logged once.\r\n'
                 '450/450 [==============================] - 40s 88ms/step - loss: ' \
                 '1.8299 - accuracy: 0.4744 - cw_loss: 9.5022 - val_loss: 1.9870 - ' \
                 'val_accuracy: 0.4528 - val_cw_loss: 9.6570\r\n'
                 '100/100 [==============================] - 3s 26ms/step - loss: ' \
                 '2.0089 - accuracy: 0.4511 - cw_loss: 9.6708\r\n' \
                 '[Testing]- loss: 2.009 - accuracy: 0.451 - cw_loss: 9.671\r\n'
                 '\r\n'
                 '```\r\n'
                 '\r\n' \
                 'Next, we tried to enable `tf.distribute.MirroredStrategy()` so we ' \
                 'modified the testcase by the following patch:\r\n'
                 '\r\n'
                 '```diff\r\n'
                 'def build_clf():\r\n' \
                 '-    #with strategy.scope():\r\n' \
                 "-    with tf.compat.v1.variable_scope('optimizer'):\r\n"
                 'def build_clf():\r\n'
                 '+    with strategy.scope():\r\n'
                 "+    #with tf.compat.v1.variable_scope('optimizer'):\r\n"
                 '```\r\n' \
                 '\r\n'
                 'And we got the error message:\r\n'
                 '\r\n' \
                 '```\r\n' \
                 'Traceback (most recent call last):\r\n'
                 '  File "bug.py", line 233, in <module>\r\n'
                 '    verbose=1)\r\n'
                 '  File ' \
                 '"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py", '
                 'line 717, in fit\r\n'
                 '    use_multiprocessing=use_multiprocessing)\r\n'
                 '  File '
                 '"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py", '
                 'line 685, in fit\r\n'
                 "    steps_name='steps_per_epoch')\r\n" \
                 '  File ' \
                 '"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py", '
                 'line 299, in model_iteration\r\n'
                 '    batch_outs = f(actual_inputs)\r\n' \
                 '  File ' \
                 '"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/backend.py", '
                 'line 3580, in __call__\r\n'
                 '    run_metadata=self.run_metadata)\r\n' \
                 '  File '
                 '"/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/client/session.py", '
                 'line 1472, in __call__\r\n'
                 '    run_metadata_ptr)\r\n' \
                 'tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 '
                 'root error(s) found.\r\n'
                 '  (0) Invalid argument: You must feed a value for placeholder '
                 "tensor 'model/input' with dtype float and shape [?,32,32,3]\r\n"
                 '         [[{{node model/input}}]]\r\n'
                 '         '
                 '[[batch_normalization_9/cond/else/_325/FusedBatchNormV3/ReadVariableOp/_2529]]\r\n' \
                 '  (1) Invalid argument: You must feed a value for placeholder ' \
                 "tensor 'model/input' with dtype float and shape [?,32,32,3]\r\n"
                 '         [[{{node model/input}}]]\r\n'
                 '0 successful operations.\r\n'
                 '0 derived errors ignored.\r\n' \
                 '```\r\n'
                 '\r\n'
                 'This error is very similar to the previous issue #34866.\r\n'
                 '\r\n'
                 'I guess that those two issues may have some strong connection.\r\n' \
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'It should work properly.\r\n'
                 '\r\n' \
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 'Please see the section of **Describe the current behavior**\r\n'
                 '\r\n'
                 '**Other info / logs**\r\n' \
                 '\r\n'
                 'The following message is the result generated by '
                 '`tf_env_collect.sh`\r\n'
                 '```\r\n' \
                 '== check python '
                 '===================================================\r\n'
                 'python version: 3.5.2\r\n'
                 'python branch:\r\n'
                 "python build version: ('default', 'Oct  8 2019 13:06:37')\r\n"
                 'python compiler version: GCC 5.4.0 20160609\r\n'
                 'python implementation: CPython\r\n'
                 '\r\n' \
                 '\r\n'
                 '== check os platform '
                 '===============================================\r\n' \
                 'os: Linux\r\n'
                 'os kernel version: #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC '
                 '2019\r\n' \
                 'os release version: 5.0.0-37-generic\r\n'
                 'os platform: '
                 'Linux-5.0.0-37-generic-x86_64-with-Ubuntu-16.04-xenial\r\n'
                 "linux distribution: ('Ubuntu', '16.04', 'xenial')\r\n"
                 "linux os distribution: ('Ubuntu', '16.04', 'xenial')\r\n" \
                 "mac version: ('', ('', '', ''), '')\r\n"
                 "uname: uname_result(system='Linux', node='f7f509f1dacf', " \
                 "release='5.0.0-37-generic', version='#40~18.04.1-Ubuntu SMP Thu Nov "
                 "14 12:06:39 UTC 2019', machine='x86_64', processor='x86_64')\r\n"
                 "architecture: ('64bit', 'ELF')\r\n"
                 'machine: x86_64\r\n'
                 '\r\n' \
                 '\r\n'
                 '== are we in docker '
                 '=============================================\r\n' \
                 'Yes\r\n' \
                 '\r\n'
                 '== compiler '
                 '=====================================================\r\n' \
                 'c++ (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609\r\n'
                 'Copyright (C) 2015 Free Software Foundation, Inc.\r\n'
                 'This is free software; see the source for copying conditions.  '
                 'There is NO\r\n' \
                 'warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR '
                 'PURPOSE.\r\n'
                 '\r\n' \
                 '\r\n' \
                 '== check pips '
                 '===================================================\r\n' \
                 'numpy                  1.17.4\r\n'
                 'protobuf               3.11.1\r\n'
                 'tensorflow-estimator   2.0.1\r\n'
                 'tensorflow-gpu         2.0.0\r\n'
                 'tensorflow-probability 0.8.0\r\n'
                 '\r\n'
                 '== check for virtualenv '
                 '=========================================\r\n'
                 'False\r\n' \
                 '\r\n'
                 '== tensorflow import '
                 '============================================\r\n'
                 'tf.version.VERSION = 2.0.0\r\n'
                 'tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d38\r\n'
                 'tf.version.COMPILER_VERSION = 7.3.1 20180303\r\n'
                 'Sanity check: array([1], dtype=int32)\r\n'
                 '       443:     find library=libpthread.so.0 [0]; searching\r\n' \
                 '       443:      search '
                 'path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          '
                 '(LD_LIBRARY_PATH)\r\n'
                 '       443:       trying '
                 'file=/usr/local/nvidia/lib/tls/x86_64/libpthread.so.0\r\n'
                 '       443:       trying '
                 'file=/usr/local/nvidia/lib/tls/libpthread.so.0\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/nvidia/lib/x86_64/libpthread.so.0\r\n'
                 '       443:       trying '
                 'file=/usr/local/nvidia/lib/libpthread.so.0\r\n' \
                 '       443:       trying ' \
                 'file=/usr/local/nvidia/lib64/tls/x86_64/libpthread.so.0\r\n'
                 '       443:       trying '
                 'file=/usr/local/nvidia/lib64/tls/libpthread.so.0\r\n' \
                 '       443:       trying '
                 'file=/usr/local/nvidia/lib64/x86_64/libpthread.so.0\r\n'
                 '       443:       trying '
                 'file=/usr/local/nvidia/lib64/libpthread.so.0\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n' \
                 '       443:       trying '
                 'file=/lib/x86_64-linux-gnu/libpthread.so.0\r\n'
                 '       443:\r\n' \
                 '       443:     find library=libc.so.6 [0]; searching\r\n' \
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n'
                 '       443:\r\n'
                 '       443:     find library=libdl.so.2 [0]; searching\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying file=/lib/x86_64-linux-gnu/libdl.so.2\r\n'
                 '       443:\r\n'
                 '       443:     find library=libutil.so.1 [0]; searching\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying file=/lib/x86_64-linux-gnu/libutil.so.1\r\n'
                 '       443:\r\n'
                 '       443:     find library=libexpat.so.1 [0]; searching\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying '
                 'file=/lib/x86_64-linux-gnu/libexpat.so.1\r\n'
                 '       443:\r\n'
                 '       443:     find library=libz.so.1 [0]; searching\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n' \
                 '       443:      search cache=/etc/ld.so.cache\r\n' \
                 '       443:       trying file=/lib/x86_64-linux-gnu/libz.so.1\r\n'
                 '       443:\r\n'
                 '       443:     find library=libm.so.6 [0]; searching\r\n' \
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying file=/lib/x86_64-linux-gnu/libm.so.6\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/lib/x86_64-linux-gnu/libpthread.so.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: /lib/x86_64-linux-gnu/libm.so.6\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: /lib/x86_64-linux-gnu/libz.so.1\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/lib/x86_64-linux-gnu/libexpat.so.1\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: /lib/x86_64-linux-gnu/libutil.so.1\r\n' \
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: /lib/x86_64-linux-gnu/libdl.so.2\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     initialize program: /usr/local/bin/python\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     transferring control: /usr/local/bin/python\r\n' \
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:     find library=libopenblasp-r0-34a18dc3.3.7.so [0]; ' \
                 'searching\r\n' \
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs            '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/tls/libopenblasp-r0-34a18dc3.3.7.so\r\n' \
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/x86_64/libopenblasp-r0-34a18dc3.3.7.so\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n' \
                 '       443:\r\n'
                 '       443:     find library=libgfortran-ed201abd.so.3.0.0 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs         '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so)\r\n' \
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n' \
                 '       443:     find library=libbz2.so.1.0 [0]; searching\r\n' \
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n' \
                 '       443:       trying ' \
                 'file=/lib/x86_64-linux-gnu/libbz2.so.1.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/lib/x86_64-linux-gnu/libbz2.so.1.0\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:     find library=liblzma.so.5 [0]; searching\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n' \
                 '       443:       trying file=/lib/x86_64-linux-gnu/liblzma.so.5\r\n' \
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: /lib/x86_64-linux-gnu/liblzma.so.5\r\n'
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:     find library=libmpdec.so.2 [0]; searching\r\n' \
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n' \
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying ' \
                 'file=/usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/x86_64-linux-gnu/libmpdec.so.2\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:     find library=libcrypto.so.1.0.0 [0]; searching\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n' \
                 '       443:       trying ' \
                 'file=/lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/lib/x86_64-linux-gnu/libcrypto.so.1.0.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:     find library=libtensorflow_framework.so.2 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..            ' \
                 '(RPATH from file ' \
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/x86_64/libtensorflow_framework.so.2\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/tls/libtensorflow_framework.so.2\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/x86_64/libtensorflow_framework.so.2\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so___Utensorflow/libtensorflow_framework.so.2\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/x86_64/libtensorflow_framework.so.2\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/tls/libtensorflow_framework.so.2\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/x86_64/libtensorflow_framework.so.2\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libtensorflow_framework.so.2\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/x86_64/libtensorflow_framework.so.2\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../tls/libtensorflow_framework.so.2\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../x86_64/libtensorflow_framework.so.2\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n' \
                 '       443:\r\n' \
                 '       443:     find library=librt.so.1 [0]; searching\r\n' \
                 '       443:      search ' \
                 'path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             ' \
                 '(RPATH from file ' \
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/librt.so.1\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../librt.so.1\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying file=/lib/x86_64-linux-gnu/librt.so.1\r\n'
                 '       443:\r\n' \
                 '       443:     find library=libstdc++.so.6 [0]; searching\r\n' \
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libstdc++.so.6\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libstdc++.so.6\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n' \
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying '
                 'file=/usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n'
                 '       443:\r\n' \
                 '       443:     find library=libgcc_s.so.1 [0]; searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             ' \
                 '(RPATH from file ' \
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libgcc_s.so.1\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libgcc_s.so.1\r\n' \
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n' \
                 '       443:      search cache=/etc/ld.so.cache\r\n' \
                 '       443:       trying '
                 'file=/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/lib/x86_64-linux-gnu/libgcc_s.so.1\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: /lib/x86_64-linux-gnu/librt.so.1\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2\r\n' \
                 '       443:\r\n' \
                 '       443:     find library=libhdfs.so [0]; searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..           ' \
                 '(RPATH from file ' \
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n'
                 '       443:      search ' \
                 'path=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python:/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/..             '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/libhdfs.so\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libhdfs.so\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n' \
                 '       443:      search ' \
                 'path=/lib/x86_64-linux-gnu/tls/x86_64:/lib/x86_64-linux-gnu/tls:/lib/x86_64-linux-gnu/x86_64:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu/tls/x86_64:/usr/lib/x86_64-linux-gnu/tls:/usr/lib/x86_64-linux-gnu/x86_64:/usr/lib/x86_64-linux-gnu:/lib/tls/x86_64:/lib/tls:/lib/x86_64:/lib:/usr/lib/tls/x86_64:/usr/lib/tls:/usr/lib/x86_64:/usr/lib              '
                 '(system search path)\r\n'
                 '       443:       trying '
                 'file=/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n' \
                 '       443:       trying ' \
                 'file=/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n'
                 '       443:       trying '
                 'file=/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n'
                 '       443:       trying file=/lib/x86_64-linux-gnu/libhdfs.so\r\n' \
                 '       443:       trying '
                 'file=/usr/lib/x86_64-linux-gnu/tls/x86_64/libhdfs.so\r\n'
                 '       443:       trying '
                 'file=/usr/lib/x86_64-linux-gnu/tls/libhdfs.so\r\n' \
                 '       443:       trying ' \
                 'file=/usr/lib/x86_64-linux-gnu/x86_64/libhdfs.so\r\n'
                 '       443:       trying '
                 'file=/usr/lib/x86_64-linux-gnu/libhdfs.so\r\n'
                 '       443:       trying file=/lib/tls/x86_64/libhdfs.so\r\n'
                 '       443:       trying file=/lib/tls/libhdfs.so\r\n'
                 '       443:       trying file=/lib/x86_64/libhdfs.so\r\n' \
                 '       443:       trying file=/lib/libhdfs.so\r\n'
                 '       443:       trying file=/usr/lib/tls/x86_64/libhdfs.so\r\n'
                 '       443:       trying file=/usr/lib/tls/libhdfs.so\r\n'
                 '       443:       trying file=/usr/lib/x86_64/libhdfs.so\r\n' \
                 '       443:       trying file=/usr/lib/libhdfs.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so\r\n'
                 '       443:\r\n' \
                 '       443:     find library=libuuid.so.1 [0]; searching\r\n' \
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n'
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying file=/lib/x86_64-linux-gnu/libuuid.so.1\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: /lib/x86_64-linux-gnu/libuuid.so.1\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:     find library=libssl.so.1.0.0 [0]; searching\r\n'
                 '       443:      search path=           (LD_LIBRARY_PATH)\r\n' \
                 '       443:      search cache=/etc/ld.so.cache\r\n'
                 '       443:       trying '
                 'file=/lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/lib/x86_64-linux-gnu/libssl.so.1.0.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:     find library=libhdf5-49599f4e.so.103.0.0 [0]; '
                 'searching\r\n'
                 '       443:      search ' \
                 'path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs                '
                 '(RPATH from file ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/x86_64/libhdf5-49599f4e.so.103.0.0\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/tls/libhdf5-49599f4e.so.103.0.0\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/x86_64/libhdf5-49599f4e.so.103.0.0\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n' \
                 '       443:\r\n'
                 '       443:     find library=libhdf5_hl-db841637.so.100.1.1 [0]; '
                 'searching\r\n'
                 '       443:      search ' \
                 'path=/usr/local/lib/python3.5/dist-packages/h5py/.libs          ' \
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so)\r\n' \
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n' \
                 '       443:\r\n'
                 '       443:     find library=libsz-1c7dd0cf.so.2.0.1 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls:/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                '
                 '(RPATH from file ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/x86_64/libsz-1c7dd0cf.so.2.0.1\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./tls/libsz-1c7dd0cf.so.2.0.1\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./x86_64/libsz-1c7dd0cf.so.2.0.1\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n'
                 '       443:\r\n' \
                 '       443:     find library=libaec-2147abcd.so.0.0.4 [0]; ' \
                 'searching\r\n' \
                 '       443:      search ' \
                 'path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                '
                 '(RPATH from file ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n'
                 '       443:       trying ' \
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n' \
                 '       443:\r\n' \
                 '       443:     find library=libz-a147dcb0.so.1.2.3 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/h5py/.libs/.                ' \
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0)\r\n' \
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n' \
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3\r\n' \
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1\r\n' \
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0\r\n'
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1\r\n'
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       452:     find library=libc.so.6 [0]; searching\r\n'
                 '       452:      search ' \
                 'path=/usr/local/nvidia/lib/tls/x86_64:/usr/local/nvidia/lib/tls:/usr/local/nvidia/lib/x86_64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64/tls/x86_64:/usr/local/nvidia/lib64/tls:/usr/local/nvidia/lib64/x86_64:/usr/local/nvidia/lib64          ' \
                 '(LD_LIBRARY_PATH)\r\n' \
                 '       452:       trying '
                 'file=/usr/local/nvidia/lib/tls/x86_64/libc.so.6\r\n'
                 '       452:       trying ' \
                 'file=/usr/local/nvidia/lib/tls/libc.so.6\r\n'
                 '       452:       trying '
                 'file=/usr/local/nvidia/lib/x86_64/libc.so.6\r\n' \
                 '       452:       trying file=/usr/local/nvidia/lib/libc.so.6\r\n'
                 '       452:       trying '
                 'file=/usr/local/nvidia/lib64/tls/x86_64/libc.so.6\r\n'
                 '       452:       trying '
                 'file=/usr/local/nvidia/lib64/tls/libc.so.6\r\n'
                 '       452:       trying ' \
                 'file=/usr/local/nvidia/lib64/x86_64/libc.so.6\r\n'
                 '       452:       trying file=/usr/local/nvidia/lib64/libc.so.6\r\n'
                 '       452:      search cache=/etc/ld.so.cache\r\n'
                 '       452:       trying file=/lib/x86_64-linux-gnu/libc.so.6\r\n'
                 '       452:\r\n'
                 '       452:\r\n'
                 '       452:     calling init: /lib/x86_64-linux-gnu/libc.so.6\r\n' \
                 '       452:\r\n' \
                 '       452:\r\n' \
                 '       452:     initialize program: /bin/sh\r\n'
                 '       452:\r\n' \
                 '       452:\r\n'
                 '       452:     transferring control: /bin/sh\r\n'
                 '       452:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n' \
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n' \
                 '       443:\r\n' \
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so\r\n' \
                 '       443:\r\n' \
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: ' \
                 '/usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:     find library=libjpeg-3b10b538.so.9.3.0 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs            '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/x86_64/libjpeg-3b10b538.so.9.3.0\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/tls/libjpeg-3b10b538.so.9.3.0\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/x86_64/libjpeg-3b10b538.so.9.3.0\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n'
                 '       443:\r\n'
                 '       443:     find library=libopenjp2-b3d7668a.so.2.3.1 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n'
                 '       443:\r\n'
                 '       443:     find library=libtiff-8267adfe.so.5.4.0 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/PIL/.libs           '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n'
                 '       443:\r\n'
                 '       443:     find library=liblzma-6cd627ed.so.5.2.4 [0]; '
                 'searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls:/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64:/usr/local/lib/python3.5/dist-packages/PIL/.libs/.            '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/x86_64/liblzma-6cd627ed.so.5.2.4\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./tls/liblzma-6cd627ed.so.5.2.4\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./x86_64/liblzma-6cd627ed.so.5.2.4\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:     find library=libopenblasp-r0-2ecf47d5.3.7.dev.so '
                 '[0]; searching\r\n'
                 '       443:      search '
                 'path=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64:/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs            '
                 '(RPATH from file '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so)\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/tls/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/x86_64/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n'
                 '       443:       trying '
                 'file=/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling init: '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /usr/local/bin/python [0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_opcode.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_ctypes.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_umath.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/_multiarray_tests.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/linalg/lapack_lite.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/linalg/_umath_linalg.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libopenblasp-r0-34a18dc3.3.7.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_bz2.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_lzma.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_decimal.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/fft/_pocketfft_internal.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/mtrand.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/common.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/bounded_integers.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/mt19937.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/bit_generator.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_hashlib.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/philox.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/pcg64.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/sfc64.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/random/generator.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/google/protobuf/internal/_api_implementation.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/google/protobuf/pyext/_message.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_csv.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/termios.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/fast_tensor_util.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/wrapt/_wrappers.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_json.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/python3.5/lib-dynload/_ssl.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libssl.so.1.0.0 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/lib/x86_64-linux-gnu/libcrypto.so.1.0.0 [0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/_errors.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/defs.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/_objects.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/_conv.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5r.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5t.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/utils.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5z.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5a.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5s.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5p.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5ac.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/_proxy.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5d.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5ds.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5f.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5g.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5i.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5fd.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5pl.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5o.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/h5l.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/libhdf5-49599f4e.so.103.0.0 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/_lib/_ccallback_c.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/_sparsetools.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/_csparsetools.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_shortest_path.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_tools.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_traversal.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/sparse/csgraph/_reordering.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/_imaging.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/libopenjp2-b3d7668a.so.2.3.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/libtiff-8267adfe.so.5.4.0 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/libjpeg-3b10b538.so.9.3.0 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/PIL/.libs/./liblzma-6cd627ed.so.5.2.4 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/ndimage/_nd_image.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/ndimage/_ni_label.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_fblas.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_flapack.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_flinalg.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_solve_toeplitz.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/_decomp_update.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_blas.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/cython_lapack.cpython-35m-x86_64-linux-gnu.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/scipy/linalg/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/lite/experimental/microfrontend/python/ops/_audio_microfrontend_op.so '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/../libtensorflow_framework.so.2 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: '
                 '/usr/lib/x86_64-linux-gnu/libstdc++.so.6 [0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libgcc_s.so.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libm.so.6 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '       443:\r\n'
                 '       443:     calling fini: /lib/x86_64-linux-gnu/libpthread.so.0 '
                 '[0]\r\n'
                 '       443:\r\n'
                 '\r\n'
                 '```',
         'created_at': '2019-12-1'},
        {'body': '**System information**\r\n'
                 '- Have I written custom code (as opposed to using a stock example '
                 'script provided in TensorFlow):\r\n'
                 '- OS Platform and Distribution: Linux Ubuntu 16.04\r\n'
                 '- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the '
                 'issue happens on mobile device:\r\n'
                 '- TensorFlow installed from (source or binary):\r\n'
                 '- TensorFlow version (use command below): 2.0\r\n'
                 '- Python version: 3.6.8\r\n'
                 '- Bazel version (if compiling from source):\r\n'
                 '- GCC/Compiler version (if compiling from source):\r\n'
                 '- CUDA/cuDNN version:\r\n'
                 '- GPU model and memory:\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the current behavior**\r\n'
                 '\r\n'
                 '<img width="368" alt="屏幕快照 2019-12-13 下午6 06 20" '
                 'src="https://user-images.githubusercontent.com/22017000/70792077-6c453000-1dd3-11ea-8489-5b6131950515.png">\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the expected behavior**\r\n'
                 '\r\n'
                 'The tf.data.Dataset instance should be freed in every step.\r\n'
                 '\r\n'
                 '**Code to reproduce the issue**\r\n'
                 '\r\n'
                 '```python\r\n'
                 'import tensorflow as tf\r\n'
                 'import os\r\n'
                 'import numpy as np\r\n'
                 'import psutil\r\n'
                 '\r\n'
                 'def _generator():\r\n'
                 '    for i in range(100):\r\n'
                 '        yield "1,2,3,4,5,6,7,8"\r\n'
                 '\r\n'
                 'def _py_parse_data(record):\r\n'
                 '    record = record.numpy()\r\n'
                 '    record = bytes.decode(record)\r\n'
                 '    rl = record.split(",")\r\n'
                 '    rl = [str(int(r) + 1) for r in rl]\r\n'
                 '    return [",".join(rl)]\r\n'
                 '\r\n'
                 'def parse_data(record, shape=10):\r\n'
                 '    sparse_data = tf.strings.split([record], sep=",")\r\n'
                 '    sparse_data = tf.strings.to_number(sparse_data[0], tf.int64)\r\n'
                 '    ids_num = tf.cast(tf.size(sparse_data), tf.int64)\r\n'
                 '    indices = tf.range(0, ids_num, dtype=tf.int64)\r\n'
                 '    indices = tf.reshape(indices, shape=(-1, 1))\r\n'
                 '    sparse_data = tf.sparse.SparseTensor(\r\n'
                 '                indices, sparse_data, dense_shape=(shape,)\r\n'
                 '    )\r\n'
                 '    return sparse_data\r\n'
                 '\r\n'
                 'process = psutil.Process(os.getpid())\r\n'
                 '\r\n'
                 'step = 0\r\n'
                 'while (step < 10000):\r\n'
                 '    t = tf.data.Dataset.from_generator(_generator, '
                 'output_types=tf.string)\r\n'
                 '    t = t.map(lambda record: tf.py_function(_py_parse_data, '
                 '[record], [tf.string]))\r\n'
                 '    t = t.map(parse_data)\r\n'
                 '    for d in t:\r\n'
                 '        a = 1\r\n'
                 '    if step % 10 == 0:\r\n'
                 '        print("Memory : ", process.memory_info().rss)\r\n'
                 '    step += 1\r\n'
                 '```\r\n'
                 '\r\n',
         'created_at': '2019-12-1'},
        {'body': '<em>Please make sure that this is a feature request. As per our '
                 '[GitHub '
                 'Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), '
                 'we only address code/doc bugs, performance issues, feature requests '
                 'and build/installation issues on GitHub. '
                 'tag:feature_template</em>\r\n'
                 '\r\n'
                 '\r\n'
                 '**System information**\r\n'
                 '- TensorFlow version (you are using): 2.0\r\n'
                 '- Are you willing to contribute it (Yes/No): Yes\r\n'
                 '\r\n'
                 '\r\n'
                 '\r\n'
                 '**Describe the feature and the current behavior/state.**\r\n'
                 'Currently if you have a dataset with a large shuffle buffer, it '
                 "doesn't start trying to populate that buffer until the first "
                 'request to retrieve data from the data set. This is a lost '
                 'opportunity for parallelism during training script startup - the '
                 'shuffle buffer could be filling in the background while the model '
                 'is compiling.  \r\n'
                 'Since calling next is blocking, its not possible to trigger this '
                 'overlap to happen. \r\n'
                 '\r\n'
                 '**Will this change the current api? How?** Maybe - could add a '
                 'start_prefetching() method.  The other idea I had was to make it '
                 'implicit in calling __iter__ - but that is possibly more useful for '
                 'my specific use case than more generally.\r\n'
                 '\r\n'
                 '**Who will benefit with this feature?**\r\n'
                 'Anyone who cares about the startup time of their training script '
                 'and uses a large shuffle buffer.\r\n'
                 '\r\n'
                 '**Any Other info.**\r\n',
         'created_at': '2019-12-1'},
        {'body': 'Thank you for submitting a TensorFlow documentation issue. Per our '
                 'GitHub\r\n'
                 'policy, we only address code/doc bugs, performance issues, feature '
                 'requests, and\r\n'
                 'build/installation issues on GitHub.\r\n'
                 '\r\n'
                 'The TensorFlow docs are open source! To get involved, read the '
                 'documentation\r\n'
                 'contributor guide: '
                 'https://www.tensorflow.org/community/contribute/docs\r\n'
                 '\r\n'
                 '## URL(s) with the issue:\r\n'
                 '\r\n'
                 'Please provide a link to the documentation entry, for example:\r\n'
                 'https://www.tensorflow.org/api_docs/python/tf/audio\r\n'
                 '\r\n'
                 '## Description of issue (what needs changing):\r\n'
                 'Currently, there are no usage examples for tf.audio APIs , which '
                 'makes it difficult for new users to implement the same.\r\n'
                 '\r\n'
                 '### Clear description\r\n'
                 '\r\n'
                 'For example, why should someone use this method? How is it '
                 'useful?\r\n'
                 '**Audio is an area not really explored in machine learning to '
                 'extent image and text has. While TensorFlow does provide a good '
                 'amount of documentation for the general Args and Returns of the '
                 'various functions under tf.audio, since most new users will have '
                 'very little experience with audio as compared to tf.image**\n'
                 '\r\n'
                 '### Correct links\r\n'
                 '\r\n'
                 'Is the link to the source code correct?\r\n'
                 '**Yes**\r\n'
                 '\r\n'
                 '### Parameters defined\r\n'
                 '\r\n'
                 'Are all parameters defined and formatted correctly?\r\n'
                 '**Yes**\r\n'
                 '\r\n'
                 '### Returns defined\r\n'
                 '\r\n'
                 'Are return values defined?\r\n'
                 '**Yes**\r\n'
                 '\r\n'
                 '### Raises listed and defined\r\n'
                 '\r\n'
                 'Are the errors defined? For example,\r\n'
                 'https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n'
                 '**No**\r\n'
                 '\r\n'
                 '### Usage example\r\n'
                 '\r\n'
                 'Is there a usage example?\r\n'
                 '**No**\r\n'
                 '\r\n'
                 'See the API guide: '
                 'https://www.tensorflow.org/community/contribute/docs_ref\r\n'
                 'on how to write testable usage examples.\r\n'
                 '\r\n'
                 '### Request visuals, if applicable\r\n'
                 '\r\n'
                 'Are there currently visuals? If not, will it clarify the '
                 'content?\r\n'
                 '**Formatted code blocks are present, which are satisfactory.**\r\n'
                 '\r\n'
                 '### Submit a pull request?\r\n'
                 '\r\n'
                 'Are you planning to also submit a pull request to fix the issue? '
                 'See the docs\r\n'
                 'contributor guide: '
                 'https://www.tensorflow.org/community/contribute/docs,\r\n'
                 'docs API guide: '
                 'https://www.tensorflow.org/community/contribute/docs_ref and the\r\n'
                 'docs style guide: '
                 'https://www.tensorflow.org/community/contribute/docs_style\r\n'
                 '**Yes, I think I can provide a detailed usage example.**\r\n'
                 '  ',
         'created_at': '2019-12-1'},
        {'body': "We've had feedback from multiple developers that it's hard to "
                 'figure out how to calculate the right  int8 values for quantized '
                 'inputs, and understand what int8 values mean as outputs.\r\n'
                 '\r\n'
                 'For example, when feeding an image to uint8 quantized inputs, the '
                 'values can be left as in their source 0 to 255 range. For int8 '
                 'inputs, the developer will typically need to subtract 128 from each '
                 'value, but this knowledge (and how the offset value is calculated) '
                 'is not documented. In the same way, users will need to map the -128 '
                 'to 127 output values to the actual real number range of their '
                 'outputs, but this process is unclear.\r\n'
                 '\r\n'
                 'Tagging the @tensorflow/micro team.',
         'created_at': '2019-12-1'},
        {'body': 'TensorFlow Lite for Microcontrollers has a MAXPOOL operation, but '
                 'it only supports float and uint8 execution, not int8.',
         'created_at': '2019-12-1'}]
    a = match(issue, time)
    pprint.pprint(a)
